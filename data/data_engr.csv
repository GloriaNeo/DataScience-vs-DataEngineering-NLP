title,id,author,created_utc,distinguished,edited,is_self,link_flair_text,num_comments,saved,score,selftext,stickied,upvote_ratio,url,subreddit
Monthly General Discussion - Aug 2023,15fgn9y,AutoModerator,1690905649.0,,False,True,Discussion,2,False,3,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

Join our [newsletter](https://dataengineeringcommunity.substack.com/) for regular data engineering community updates, inspiration, and insights.",True,1.0,https://www.reddit.com/r/dataengineering/comments/15fgn9y/monthly_general_discussion_aug_2023/,dataengineering
Quarterly Salary Discussion - Jun 2023,13xldpd,AutoModerator,1685635229.0,,False,True,Career,213,False,83,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",True,1.0,https://www.reddit.com/r/dataengineering/comments/13xldpd/quarterly_salary_discussion_jun_2023/,dataengineering
What replaced cubes?,15gnctu,leaky_shrew,1691018421.0,,False,True,Discussion,31,False,32,"I’m fairly old school with lots of on prem and ssis/ssas experience. Been doing a ton more with cloud now but more on the E/L and infra and haven’t gotten around to dbt yet.

But I’ve wondered what has replaced the idea of a cube or enterprise star scheme with multiple facts to power reports and dashboards from a single source? 

Additionally is the idea of self service via a cube style interface dead?

Is it mostly dbt now and a sprawl of models users pick and choose from?

I’ve done a lot with power BI, but even that seems to focus on creating mini data models per report, so wondering how the reporting/dash data sources are scaled now.

Apologies if it seems like I’m dense, just used to a semantic layer that creates common metrics and dims for people so they’re all looking at the same things.",False,0.95,https://www.reddit.com/r/dataengineering/comments/15gnctu/what_replaced_cubes/,dataengineering
Is traditional data modeling dead?,15gf97e,New-Ship-5404,1690997721.0,,False,True,Discussion,46,False,66,"As someone who has worked in the data field for nearly 20 years, I've noticed a shift in priorities when it comes to data modeling. In the early 2000s and 2010s, data modeling was of the utmost importance. However, with the introduction of Hadoop and big data, it seems that data and BI engineers no longer prioritize it. I'm curious about whether this is truly necessary in today's cloud-based world, where storage and computing are separate and we have various query processing engines based on different algorithms. I would love to hear your thoughts and feedback on this topic.",False,0.9,https://www.reddit.com/r/dataengineering/comments/15gf97e/is_traditional_data_modeling_dead/,dataengineering
Lots of people seem to hate data engineering. Is it really that bad?,15gizw0,InevitableTraining69,1691006182.0,,False,True,Discussion,37,False,30,"There are a lot of engineering positions available today and plenty of departments and functions to expand into. One of the ones I see complained about the most is data engineering. Very similar to system or IT engineering, But seems to be very complained about and frowned upon overall. Everyone has some sort of cool background experience into engineering and IT whether it be infrastructure, being a DBA, analytics, machine learning... But when you hear about data engineering, immediate change in attitude. 


Recently, I interviewed for a position in IT engineering, and it was a lot of software as a service applications that they are supporting, using API and Python scripts to retrieve sets of data, supporting different applications that a business might use, managing compute resources in Google BigQuery and other apps. When I mentioned that I would love to be a data engineer in the future, they apologize and said I'm sorry for you. Thought it was kind of funny


Curious what might cause the attitude that data engineering overall is boring, or it sucks. Is it because of the fact that you're working with databases most days, or is there something boring about piping data into data lakes, data warehouses, using Python for that?",False,0.81,https://www.reddit.com/r/dataengineering/comments/15gizw0/lots_of_people_seem_to_hate_data_engineering_is/,dataengineering
"After all has been said & done, I'm looking for a new career",15ggaet,kerkgx,1691000008.0,,1691000433.0,True,Career,11,False,16,"Lead data team here, I've been working as software engineer/data analyst/scientist/engineer/ML/infra for as long as I can remember, did my Masters in ML and did little bit of research back then, and right now I feel like I don't really want to work in tech company anymore since it's not profitable/barely alive. The investors hardly pour any money especially to Southeast Asia region as well, it's getting harder day by day.

And I'm kinda tired with all these uncertainties, I've witnessed/experienced tears, layoffs, harsh treatment (because of frustration, yes I know very well making profitable & sustainable business is extremely difficult), ridiculous work load which lead to mental health issue (I do have ADHD and it has been worsening the situation) and very unhealthy life, etc. Yeah, it was accumulated since 2015.

So here's my question. For those of you guys who quitted job from tech company what were your options back then and why you pursued that career? What's your story? I'm interested to hear if any of you have gone to corporate and never go back to tech.

I'm gonna be little bit specific here, since perhaps our similar background can help, any feedback from my brothers and sisters age 30-40, married, especially from SEA countries will be appreciated.",False,0.9,https://www.reddit.com/r/dataengineering/comments/15ggaet/after_all_has_been_said_done_im_looking_for_a_new/,dataengineering
Would you do UPDATE/INSERT in Azure Data Factory or in database?,15gxnwv,ka_eb,1691049437.0,,False,True,Discussion,2,False,1,"Hi,

This is just a general question and I am sure there is enough pros and cons for both but what is your preferred solution?

I only use ADF to get the data from the source to the staging/destination with minimal transformation, maybe some filtration if needed. Then I do the target table update/insert using a SP.

Recently I encountered a solution where they use ADF dataflows for updating the table. They get the new data and compare it to the existing and then they save the results to the target table.

Which one do you prefer and why?

Thanks.

K.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15gxnwv/would_you_do_updateinsert_in_azure_data_factory/,dataengineering
How should I be planning my career when the job market is so bad?,15g63rd,level_126_programmer,1690974814.0,,1690975082.0,True,Discussion,36,False,40,"I live in a big city in the US, and I currently work at a very respected tech company for more than 2 and a half years.  Every 2 months since last summer, I check the job postings for data engineers in my city and I see that there are very few jobs in my city.  I'm honestly surprised to see that my LinkedIn also is not getting the same amount of recruiters messaging me as I did 2 years ago.



How are people looking for jobs, are they still using LinkedIn or just waiting at their current job for the economy to improve in a year or two?





Is anyone considered going into different areas of engineering, such as devops or software engineering?  Also, for the purpose of career progression, have any engineers at well known tech companies considered going into management at a non-tech company either at the manager or director level if the salary is good?",False,0.84,https://www.reddit.com/r/dataengineering/comments/15g63rd/how_should_i_be_planning_my_career_when_the_job/,dataengineering
This week I released the latest publication of data news (news from data engineering),15gwt30,AmphibianInfamous574,1691046468.0,,False,False,Blog,0,False,1,,False,1.0,https://patrikbraborec.substack.com/p/data-news-38,dataengineering
Is there a point in applying for DE positions as a SQL/Python DA with 2 YoE in the current market?,15gk10n,neheughk,1691008543.0,,False,True,Career,7,False,3,"I have barely any cloud or apache kafka/streaming experience which seems to be a key pre requisite these days. I know historically many DAs transitioned to DE, but given how difficult the market now is, will I be likely to get any callbacks with my limited skillset? UK based btw.",False,0.67,https://www.reddit.com/r/dataengineering/comments/15gk10n/is_there_a_point_in_applying_for_de_positions_as/,dataengineering
Do I need to learn Cloud skills before moving to new jobs?,15g783s,rbarath,1690978144.0,,False,True,Career,6,False,9,"I've been working in the Banking sector for almost 6 years as Data Engineer, I'm planning to apply for a senior job position outside the Banking sector. 

My primary skills are SAS Suite, Teradata SQL, Unix, and Python. And mostly I work with batch data and sometimes with APIs but on a smaller scale.

I've been looking at the jobs posted on LinkedIn and I can see that cloud skills became a major requirement for most of the jobs and my current skill sets do not match up in other sectors apart from Banking and a few Health Care industries. 

I have started to learn cloud data engineering (Azure Data Engineer, Google's  Professional Data Engineer ),  but I'm getting lost on all the different providers and their study materials. Should I continue to learn cloud skills before searching for a new job or Can I start to apply for the jobs and learn the skills afterwards? ",False,0.91,https://www.reddit.com/r/dataengineering/comments/15g783s/do_i_need_to_learn_cloud_skills_before_moving_to/,dataengineering
How to combine 100 JSON files with 100k rows each?,15fyvw4,workthrowaway12wk,1690950926.0,,False,True,Help,66,False,35,"I have 100 json files with 100k rows each that we need to combine in one single file for ingestion into a custom vendor process.


Although this is an ad-hoc task, I was wondering how do I make this performant if we had to do it daily. Certainly, if I use pandas, I'd be disappointed. 

Total data size is 15gb.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fyvw4/how_to_combine_100_json_files_with_100k_rows_each/,dataengineering
Questions about complex database schema,15gaso9,brandboat,1690987523.0,,False,True,Discussion,9,False,4,"1. If there are a large number of tables, do data engineer/analysts spend a lot of time figuring out the relationships between tables every time when writing SQL queries?
2. Besides asking people directly, how does everyone figure out the relationships between tables? Are there any tool/solution that can help us clarify complicated database schema more quickly?
3. If there is a tool that could predefine the relationships between tables, and in SQL we could simply use ""."" to denote the relationship between tables, such as using `foo.bar.quux` to replace `foo JOIN bar ON foo.bar_id = bar.bar_id JOIN quux ON bar.quux_id = quux.quux_id`, would you find writing to be more intuitive, or say it helps understand the whole database schema?",False,0.83,https://www.reddit.com/r/dataengineering/comments/15gaso9/questions_about_complex_database_schema/,dataengineering
Send data to Salesforce from BigQuery,15gkxb6,bobasucks,1691012687.0,,False,True,Help,6,False,0,"New data engineer here. Our analytics / reporting views are on BigQuery and there is a need to upsert this data daily into Salesforce objects. Our business users use Salesforce. 
Any inputs on what the best approach could be. Volume is in 10-15GB range.",False,0.5,https://www.reddit.com/r/dataengineering/comments/15gkxb6/send_data_to_salesforce_from_bigquery/,dataengineering
Help on migrating azure synapse pipeline from one tenant to another,15giz74,Extra_Blacksmith_567,1691006137.0,,False,True,Help,2,False,1,"Hello everyone,
We have the requirement to migrate the azure synapse pipeline having 100 + pipelines from one tenant to another.
Can anyone help in providing me the automated way to achieve it ?
Any help is highly appreciated.
TIA",False,0.67,https://www.reddit.com/r/dataengineering/comments/15giz74/help_on_migrating_azure_synapse_pipeline_from_one/,dataengineering
Converting PowerBI (M) to SQL,15gd0ao,srevolve,1690992588.0,,False,True,Help,3,False,2,"hey hey!

Sorry if this is a dumb question but we have a team of analysts building datamarts in PowerBI. We need to rebuild those tables in SQL in a separate location. For me trying to figure out what the analysts are doing, getting all the data/business context is super time consuming since I have a lot of other responsabilities. Is that a sign that i'm a bad dataengineer (not sure if this should be an easier task than it feels like?) If not then i was wondering if someone else has had this situation and whether they found a tool to convert PowerBI transformations (written in M afaik?)  into SQL.",False,0.75,https://www.reddit.com/r/dataengineering/comments/15gd0ao/converting_powerbi_m_to_sql/,dataengineering
Fancy dashboards with volatile data pipelines!,15f9uab,growth_man,1690889349.0,,False,False,Meme,10,False,287,,False,0.98,https://i.redd.it/diy1dfnohhfb1.jpg,dataengineering
data disasters and mishaps....,15gfyjv,Crafty_Combination54,1690999288.0,,False,True,Discussion,4,False,1,"Data disasters and mishaps …

Yup, they happen to the best of us. 

Most of us have experienced something in this realm. Data loss. Flawed interpretation. 

Maybe it was a PEBCAK (problem exists between chair and keyboard), maybe it was a piece of tech or code that took a shit.

Hopefully there was a fix, rollback, backup, or something else to minimize the impact. 

Ya, when these things happen it sucks, but as the gurus always say, these are great teaching moments. Or at least funny stories we can all share.

What’s your “favourite” story? - I’ll start in the comments.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15gfyjv/data_disasters_and_mishaps/,dataengineering
SWE here. Accepted to masters but having issues on what to expect.,15geinq,hairystripper,1690996021.0,,False,True,Discussion,0,False,1,"Hi all,

I have been working under a weird title, research engineer, just over 3 years in an embedded network company. I mostly looking out for ways to improve wifi quality and develop end-to-end solutions including parts on embedded devices and server side. Last year I tried something somehow data engineering(?) related where I collect bunch of data from devices and feed them into a ML model. I always had an interest in big data management and patern discovery and data related projects overall.

Friday I accepted to a masters program which is just CS masters. Professor which accepted me works on distributed data management and pattern discovery. On time I wander here I did not saw any post related to these topics. Also I saw many posts/comments disscussing ""DE or DS"" but  I failed to come up with definitive differences. What are the differences between data engineering and data science ? Are people working on those roles work interchangeably or they just never touch each others work ? My confusion might be because where I work, I develop both embedded and  cloud code as well as deploying them to various places(aws, heroku, k8s in my own homelab) as a result I lost my sense of job titles and their roles. What should I expect from my masters ?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15geinq/swe_here_accepted_to_masters_but_having_issues_on/,dataengineering
Introducing the dbt adapter for Synapse Data Warehouse in Microsoft Fabric | Microsoft Fabric Blog | Microsoft Fabric,15fwrgn,theporterhaus,1690944608.0,,False,False,Blog,3,False,13,,False,0.93,https://blog.fabric.microsoft.com/en-US/blog/introducing-the-dbt-adapter-for-synapse-data-warehouse-in-microsoft-fabric/,dataengineering
AI/ML and Big Data Software Agencies Repository Submissions Request.,15gd7kx,freeway334,1690993054.0,,False,False,Help,0,False,1,,False,1.0,https://docs.google.com/forms/d/e/1FAIpQLSdH5dj74TRGNqM3-O7Ym0r_Rp5FVgj2ckj7VCM6bAKpLcds4w/viewform,dataengineering
Optimal Database for Querying 2 Billion Rows of Social Media Data in Under 1 Second?,15g6wp4,DataShack,1690977253.0,,False,True,Discussion,21,False,2,"I need some advice on selecting the best database to handle 2 billion public profiles from TikTok in my dashboard [app.seeksocial.io](https://app.seeksocial.io/). Currently, I'm using elastic search, but the queries are slow, taking 2-3 seconds each. Moreover, about 70% of the search queries don't involve text search but rather focus on follower count, likes count, country, etc.

I'm looking for suggestions on a more efficient database that can handle these queries lightning fast (in less than 1 second). Any recommendations or insights are highly appreciated! Thank you!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15g6wp4/optimal_database_for_querying_2_billion_rows_of/,dataengineering
Honest admittance,15flpsj,Jamese0,1690916981.0,,False,True,Help,57,False,42,"I am a very experienced DE, I have built very complex data models for huge commercial entities, and nobody has been able to explain to me what a webhook is in non-technical terms. I work with students but I am still in the dark.",False,0.92,https://www.reddit.com/r/dataengineering/comments/15flpsj/honest_admittance/,dataengineering
Parallel write to Iceberg Table using Athena,15gaso1,mosquitsch,1690987522.0,,False,True,Help,2,False,1,"Hi,

I have created a job which writes one partition to an iceberg table using an Athena query. This works fine when I run it standalone. I have configured it in Dagster to be a partitioned asset, so that Dagster spools up 10 parallel runs (10 is our configuration for Dagsters concurrency).

Unfortunately, many of these jobs fail with the following error:

    pyathena.error.OperationalError: ICEBERG_COMMIT_ERROR: Failed to commit Iceberg update to table:

It looks like Athena does not manage parallel write to an iceberg table very well. Is there anything I can do about it?

The SQL statement is :

    merge into ... when not matched then insert ...

I only have the not matched insert branch. No matched , delete or update branches.

Cheers,

Matt",False,1.0,https://www.reddit.com/r/dataengineering/comments/15gaso1/parallel_write_to_iceberg_table_using_athena/,dataengineering
Argo Workflows/Events for Data Pipelines?,15g8ish,mbsquad24,1690981738.0,,False,True,Discussion,1,False,1,"At a small company building out the data platform. Our SWE group wants to implement a k8s devops environment since their stuff is all k8s, and that includes Argo.

Right now our pipelines are a few cron jobs, bash scripts triggering dbt, and airbyte syncs. It’s going to start to get a bit more complicated soon, and since our architect liaison (in his infinite f*cking wisdom) is on week 6 of attempting to PoC Prefect (IYKYK), if I can leverage Argo I can relinquish responsibility for the ops platform. 

So my question is, have any of you built any meaningful pipelines using Argo Workflows or Argo Events before? It looks fairly straightforward so I’m looking for gotchas.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15g8ish/argo_workflowsevents_for_data_pipelines/,dataengineering
date engineer seeking career advice,15fuadn,uwrwilke,1690937658.0,,False,True,Career,10,False,9,"i’ve been in the tech industry since 2010. throughout that time, i’ve been a GIS Analyst, Support Engineer, Manager, Solutions Architect and now the last 6 years as a Data Engineer. 

i’m skilled in nearly everything that a data engineer does, but the last 3-4 years ive noticed that DE roles now require python experience. i just haven’t had to use ot. and if i did, i just borrowed snippet code and modified. 

this lack of python experience appears to be the bane of my existence on being able to be marketable. it makes me feel trapped on my current role. ive taken boot camps for python in the past, but it didn’t stick. life and or work projects take priority. 

should i just get into management and forgo this gap in skills to move on?",False,0.8,https://www.reddit.com/r/dataengineering/comments/15fuadn/date_engineer_seeking_career_advice/,dataengineering
Does every big-ish company need data engineers?,15foo1o,Sacred_Tomato,1690923649.0,,False,True,Discussion,9,False,19,"I work for a large beverage corp. One you've likely seen on commercials in between NBA and NFL games. 

Obviously we work with a large...large amount of sales data and drink cases  volume data. But we don't actually have a team of data engineers in our company. 

We do have two companies that we pay to host Data services for us but wouldn't it be beneficial to have an internal data engineering team? 

This is a non-tech company btw. Our team has pitched this idea to some highers ups only to be met with ""why? Jobs getting done now isn't it?""",False,0.95,https://www.reddit.com/r/dataengineering/comments/15foo1o/does_every_bigish_company_need_data_engineers/,dataengineering
Throwback: HQ Trivia Server Crashes - HBO's Glitch,15g765x,ForOhForUserNotFound,1690977981.0,,False,True,Discussion,3,False,1,"Watching the new documentary on HBO about HQ made me curious: what exactly happened on the backend when the game would go down?

Surely some nerd out there has profiled the issues in detail, just haven't been able to find a good blog/article yet.

Thanks fellow nerds",False,1.0,https://www.reddit.com/r/dataengineering/comments/15g765x/throwback_hq_trivia_server_crashes_hbos_glitch/,dataengineering
Job Demand in the time of Gen AI?,15g6t8n,Solid-Exchange-8447,1690976960.0,,False,True,Career,8,False,0,"I'd like to know what your take of DE Demand in the time of Gen AI.

I'm taking degree to become a Big DE in 2 years' time. But I'm concerned about the future demand of this job. 

What're you thinking",False,0.5,https://www.reddit.com/r/dataengineering/comments/15g6t8n/job_demand_in_the_time_of_gen_ai/,dataengineering
Lakehouse platform available for cloud and on-premise,15g6s9a,IOMETE-,1690976870.0,,False,True,Blog,3,False,2,"Would like to share new updates on the IOMETE lakehouse platform!

Scala Notebook

We are thrilled to announce an exciting update for Jupyter Notebook users! With our latest integration, connecting to [IOMETE's Jupyter Gateway](https://iomete.com/docs/starting-with-notebook) enables seamless data exploration and analysis from your local environment. Harness the power of Jupyter Notebook as you directly access and analyze data stored in IOMETE's data lake, making your data exploration process more agile and efficient than ever before. Unlock the full potential of your data resources and elevate your data-driven decision-making with this groundbreaking collaboration. Upgrade your data exploration experience today with Jupyter Notebook and IOMETE's integrated solution.

## Added sample SQL worksheets

Introducing the latest update Sample SQL Worksheets that are designed to elevate your data exploration experience to unprecedented heights.

With these pre-built SQL worksheets, you can now jump-start your data analysis journey effortlessly. Whether you are a seasoned SQL pro or just starting with data exploration, our sample worksheets offer valuable templates that cater to a wide range of use cases.

## IOMETE Data Lakehouse is Now Available for Google Cloud Platform Users! 🌟

We are thrilled to announce a significant milestone for IOMETE, your go-to data solution! As part of our commitment to expanding accessibility and empowering users across diverse platforms, we are excited to introduce the availability of IOMETE Data Lakehouse for Google Cloud Platform (GCP) users. You can easily [set up your clusters](https://iomete.com/docs/guides/deployment/gcp/install?utm_source=reddit&utm_medium=social&utm_campaign=gcpinstall) in any of the available regions on GCP.

## IOMETE Data Lakehouse is Now Available for Microsoft Azure Users! 🌟

We are excited to expand the reach of IOMETE Data Lakehouse to Microsoft Azure users, empowering them to harness the full potential of their data and drive innovation. Whether you are a data scientist, analyst, or business professional, IOMETE's availability on Azure opens up a world of possibilities for your data management needs.

Take advantage of this powerful combination today! To start with IOMETE Data Lakehouse on Microsoft Azure, visit for [detailed setup instructions](https://iomete.com/docs/guides/deployment/azure/install?utm_source=reddit&utm_medium=social&utm_campaign=azureinstall) and explore a new era of data-driven possibilities.

## Spark upgrade 3.3.3

As part of the IOMETE platform, Apache Spark is used for large-scale data processing. Spark is fast and easy to use. It can handle ETL processes, analytics, machine learning, and more. You can work faster, more quickly, and more efficiently with Apache Spark 3.3.3, a cutting-edge advancement.

As always, our team is dedicated to delivering cutting-edge features and integrations to enhance your data journey. We welcome your [feedback and suggestions](https://github.com/iomete/roadmap/discussions) as we evolve IOMETE's capabilities.

Thank you for being part of the IOMETE family. Stay tuned for more exciting updates in the future as we work together to transform the way you interact with data!

Happy data exploring! 🚀",False,0.67,https://www.reddit.com/r/dataengineering/comments/15g6s9a/lakehouse_platform_available_for_cloud_and/,dataengineering
"I'm an intern, I need help regarding pipelines",15fxc6k,YouKnowILoveMyself,1690946261.0,,False,True,Help,9,False,5,"Hello everyone, 

I'm currently given a task to design backend. The problem is to take PDFs convert them to excel then parse the excel files and save the data to a database server. They don't want cloud solutions. Their entire ecosystem is in windows. I tried Airflow but don't understand how to get it up and running in windows.  I made the program(mix of python and C#) but I need it to run automatically and like a service . I have no idea how to make it runnable on a different environment and how to show a demo. Can anyone help me out?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fxc6k/im_an_intern_i_need_help_regarding_pipelines/,dataengineering
Iceberg Integration with Databend,15fwpq7,PsiACE,1690944463.0,,False,False,Blog,0,False,4,,False,1.0,https://databend.rs/blog/2023-08-01-iceberg-integration,dataengineering
Need Help In Setting Up Self-Hosted Integration Runtime in ADF on MacBook.,15g50xb,Vishesh_Sharma_,1690971420.0,,False,True,Help,2,False,1,"Hello, I am facing some issues regarding the set-up of self-hosted Integration runtime in the Azure data factory in my MacBook, Most of my peers have a Windows PC so I am struggling to find a solution.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15g50xb/need_help_in_setting_up_selfhosted_integration/,dataengineering
What are the approaches to ingest large db,15g4mit,Both_Antelope_9872,1690970094.0,,False,True,Discussion,2,False,1,"If you have a really huge table in rdbms.
How do you ingest the whole snapshot of the table into parquet or other format.

It is not so clear to me how to do it in batch with an atomic (capture only the state of the table at the ingestion time) and whether it is possible to distributed the task across multiple workers to speed up.",False,0.99,https://www.reddit.com/r/dataengineering/comments/15g4mit/what_are_the_approaches_to_ingest_large_db/,dataengineering
How is your company handling pipelines for non-prod data sources?,15fp9m6,0_to_1,1690924984.0,,False,True,Discussion,14,False,8,"A lot of projects, guides, even literature mostly covers production centric pipelines & architectures. I'm curious how you or your company is handling situations where you not only have a production app but also a QA / UAT environment and potentially even a stable development environment. 

To better explain: 

| Environment | Name        | Purpose                                       | Example Source URL                            | Example Source System    |
|-------------|-------------|-----------------------------------------------|---------------------------------------|--------------------------|
| Production  | prod  | Live environment for real-world data usage   | https://app.domain.com/   | the ""live"" app                |
| UAT         | uat         | User Acceptance Testing before production    | https://app-uat.domain.com/           | the ""uat"" app            |
| Develop     | develop     | Development and testing of new features      | https://app-dev.domain.com/      | the ""dev"" app    |


Do you treat those all as separate pipelines? Is it pure configuration? 
What about end results? Do you have a separate ""reporting"" / ""analytics"" exposure per environment?  

Thanks.",False,0.91,https://www.reddit.com/r/dataengineering/comments/15fp9m6/how_is_your_company_handling_pipelines_for/,dataengineering
Should I switch from Data Science to Data Engineering?,15g1rc5,hashirbhatti,1690960349.0,,False,True,Career,9,False,1,"Hello all, I am an undergraduate student pursuing a bachelor's degree in computer science (halfway through it). I have been learning Data Science and Machine Learning for two years now and have also worked on many portfolio projects. Lately, I am getting concerned about the job market in data science and am considering switching my career to Data Engineering. I am here to seek advice from experienced professionals who have been in the industry for some time on whether I should change my career to Data Engineering or stay focused on Data Science.

The transition to Data Engineering might not be too challenging for me, as I am already familiar with data concepts and techniques. If I start learning Data Engineering now, by the time I graduate, I will have the necessary skills to pursue a career in this field.

I would truly appreciate any insights, suggestions, or experiences you could share with me. Thank you in advance!",False,0.6,https://www.reddit.com/r/dataengineering/comments/15g1rc5/should_i_switch_from_data_science_to_data/,dataengineering
"Are my external data engineers incompetent, or exploiting us?",15f83gv,UNSTOPPABLE-CUM,1690884141.0,,False,True,Help,29,False,36,"Hey all!

I'm in a bit of a pickle. I've recently started a new job in a fortune 500 company. My role is primarily as a business analyst / marketing analyst. My team consists of around 7 analysts, whom come from domain specific backgrounds with zero or limited programming experience - they work with Excel.

One of our tasks is updating customer segments from sales reps: they hand in an excel spreadsheet, we see the corrections, and update them.

The update process, so far, is all done by using some old SQL queries, pulling data into multiple spreadsheets, doing a bunch of VLOOKUPS, then pulling historical action data out of our DWH, comparing and see if historic changes were met.

Now, in order to make the changes, we email a list to our data rep (xlsx), and they then load these changes.

I'm trying to understand: why do we, as analysts with zero knowledge of ETL processes, limited knowledge on DBs, work on manual joins and analysing historical changes ourselves? I totally understand the use of Excel, and automation takes time, but there is so much room for human error here.

The majority of my filtering / vlookup / categorising tasks could be cleaned up with some regex and automation server side + additional data validation checks. We consistently have errors due to the human condition: Tuesdays suck, we forgot to join certain IDs etc.

I would like to approach our external partner and ask why this is the case, but maybe this is a little too political?

so the question is: **as data engineers, is our external partner not delivering, or why might they want a workflow like this?**

Thank you!",False,0.92,https://www.reddit.com/r/dataengineering/comments/15f83gv/are_my_external_data_engineers_incompetent_or/,dataengineering
How does backfilling work if the source db occasionally deletes old data?,15fmgtz,Weekly_Dimension_332,1690918686.0,,False,True,Help,13,False,5,"Probably a dumb question.

Hypothetically, say your transformation logic changes, and you want to run a backfill. 

But say the source database has deleted most of its old data, how is this generally handled?

I get a solution might be to create a “raw data area” where you basically ingest data into this area everyday (as an example), and then going forward, you treat this as the source data, rather than relying on the source db.

If this was the case, would you only run the transformation part of your pipeline, and if so, how would this work in something like ADF? 

Would it make sense to create an extraction pipeline and a transformation pipeline, and make sure the team only ever run backfills on the transformation pipeline?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fmgtz/how_does_backfilling_work_if_the_source_db/,dataengineering
Side Project Advice Needed For a Data Noob,15fqg3f,dhwi1ue9dj,1690927726.0,,1690929533.0,True,Help,3,False,2,"Hello,

I am a university student looking to go into data engineering/science in the future. I have a side project in mind with the following data flow:

\- Real-time data input from an API

\- There exists a couple of data tables (< 10 GB), which the data ""goes through"" to calculate relatively simple statistical stuff

\- Each instance of the data & statistical results gets posted on a website

\- I want this to be automated even when my computer is off

Any advice on which tools to use that are free or cheap? I looked into Azure or AWS cloud computing, but they seem to be costly when I go over a couple of GB of storage. I'm not even sure if I can integrate that to a website. I want to use Python + I have experience in building simple websites but nothing like this before.

If you have any other advice or helpful comments I would appreciate it.

Thank you!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fqg3f/side_project_advice_needed_for_a_data_noob/,dataengineering
Question About Naming Conventions for ADLS Gen2 Hierarchcal Namespace,15fq4yq,who-wut,1690927001.0,,False,True,Help,5,False,2,"When naming folders for ADLS gen2, is there a signficance to the convention of naming a folder '***year=2022***' versus simply, '***2022***'? Is it to make the folders less ambiguous, for compatibilty, performance increases, or any other reasons not mentioned?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fq4yq/question_about_naming_conventions_for_adls_gen2/,dataengineering
Best way to call Synapse notebook from another notebook?,15funyx,mr_electric_wizard,1690938671.0,,False,True,Help,2,False,1,"I have lots of synapse notebooks that I have been calling with other “master notebooks” using the magic %run command.  
I recently found the mssparkutils.notebook.run() method of invoking a notebook.
I like the latter method because you can format the parameters section on multiple lines, however there seems to be issues calling notebooks this way.
I had one complete with an error today but the cell with the .run command completed successfully.
If error trapping doesn’t work then it’s worthless to me.
How do you call notebooks from other notebooks?  Which is the most ideal way?
I’d love to hear your insights.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15funyx/best_way_to_call_synapse_notebook_from_another/,dataengineering
Ambarish Donga DatabricKS Solution Architedct interview Tips,15fs76t,jonyeanu,1690932105.0,,False,False,Blog,0,False,0,,False,0.5,https://www.youtube.com/watch?v=eLCcNRT5BlA,dataengineering
Upcoming Interview Questions,15frry0,danielwilu2525,1690931024.0,,False,True,Interview,2,False,0,"Hi everyone! I have a final round interview coming up for a Technology internship for next summer. One of the possible specializations is Data Engineering, which is what most interests me. I have a lot ML / AI background, but have yet to use the tech stack that comes with the typical work of a DE (SQL, BigQuery, Airflow, etc.). 

I was wondering if there's a short-term project ( < 1 week completion) that could give me experience in these technologies so I can confidently talk about them during the interview? Additionally, any resources in learning them would be great. ",False,0.33,https://www.reddit.com/r/dataengineering/comments/15frry0/upcoming_interview_questions/,dataengineering
Can anyone elaborate me about the 'DevOps' side of DE or how should I learn them to fit for an interview or working as a data engineer,15f127h,GeForceKawaiiyo,1690860937.0,,False,True,Help,9,False,36,"Been working at a large domestic company before as a DE, but used mostly inner self-made tooling to write SQL/Spark code to do ETL. No so-called 'deployment' or CI&CD concept. Most of the time It actually is just filling out a form with parameters and configuration and you submit the code and workflow. Most DE just focus on the business logic and a little bit of parameters tuning. 

&#x200B;

So when I first realized that many DE jobs on the job market required experience like DevOps, gitlab, CI&CD, Jenkins and Agile, I got painfully nervous. I think these are really important and wish I have these knowledges because I can be more prepared for the work in the future. However I don't really possess the 'Software developer' side knowledge since I came out as a data-guy from the beginning. 

Can anyone recommend some materials or share some learning experience about these 'DevOps' side tooling/concepts/learning paths? Much appreciated.

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/15f127h/can_anyone_elaborate_me_about_the_devops_side_of/,dataengineering
McKinsey - Data Engineer - University Students Role,15fhe6c,WaitPsychological532,1690907334.0,,False,True,Interview,2,False,3,"Hey guys I got a invitation for an Online Technical Interview for McKinsey - Data Engineer - University Students Role. I have been getting rejected to most of the places I apply at, so I was kind of confused that McKinsey send a  interview. I was wondering, does everyone who applies get the Online Technical Interview or is it more selective? I also not the best at taking  Online Technical Interviews so I was wondering if anyone has any tips for that.",False,0.8,https://www.reddit.com/r/dataengineering/comments/15fhe6c/mckinsey_data_engineer_university_students_role/,dataengineering
DuckDB vs. MotherDuck — should you switch to the cloud version?,15fow15,tchiotludo,1690924136.0,,False,False,Blog,1,False,1,,False,0.67,https://kestra.io/blogs/2023-07-28-duckdb-vs-motherduck,dataengineering
Not confident with the Quality of Data.,15f446n,burningburnerbern,1690870749.0,,False,True,Discussion,8,False,13,"Feeling nervous about a prod release for a data model that I've been working on. Been doing extensive QA on it but I feel like it's just never enough. Messy data always finds its way even with all the nonstop transformations that I've put into place. I'm always worried about some bad join that happens in the pipeline that just screws up the data. I always feel really uneasy about people using my data to make key decisions. Now again so far the numbers look good, but theres just gonna be that stupid something that pops up. I can't help but want to be like ""wait hold up! let me compare those numbers against the raw data!"" 

Anyone ever have this feel? ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15f446n/not_confident_with_the_quality_of_data/,dataengineering
Advice on improving the data architecture at a company I just joined?,15etn2w,King_Spike,1690841198.0,,False,True,Help,22,False,38,"A few weeks ago I started a new job at a small company, and due to several people leaving, I'm about to become a data team of one. Before you tell me to leave immediately, I took the job because I'm going back to school soon and the work life balance afforded at this company should allow me to balance studying with a full time job. If it doesn't, I'll quit because school is the higher priority for me at the moment. And I was fully prepared to work part-time before I had this job offer.

But that being said, I would like to give an honest attempt at improving the state of the company's data pipelines. Currently there are a ton of pipelines all running via airflow, and the airflow site itself is consistently going down, dags will fail and have to be manually re-run, some dags seems to run in an order such that a downstream table will build prior to one it relies on, resulting in missing data in some columns.

I'm coming from a very large company that builds data analytics software, so I'm used to using tools built in-house and within one cohesive ecosystem. I realize this post is fairly vague, but I suppose if you could give a pointer or two to someone entering the deep end, I'd really appreciate that!",False,0.9,https://www.reddit.com/r/dataengineering/comments/15etn2w/advice_on_improving_the_data_architecture_at_a/,dataengineering
Is HDFS + Spark a good tech stack in today's world ?,15f3uev,GreekYogurtt,1690869813.0,,False,True,Discussion,20,False,9,"As a DE, I'm trying to build a data platform for our data science team. Our main use cases are : Data lake, and letting data scientists run jupyter notebooks to read big data files.   


These days, everyone seems to be on cloud. We prefer on-prem as we might have a lot of read actions on the data, which can be costlier.   
Also, everyone seems to be talking about the maintenance effort in HDFS. Is it not worth the performance improvement we might get with data co-locality ?    


  
",False,0.85,https://www.reddit.com/r/dataengineering/comments/15f3uev/is_hdfs_spark_a_good_tech_stack_in_todays_world/,dataengineering
Cloud services to run a Python function in the cloud seem suboptimal,15fbu8g,mrocklin,1690894559.0,,False,True,Blog,0,False,2,"I help lots of people use Python in cloud (I work on Dask, an OSS library for parallel computing).  Some people don't need the complexity of Dask (it can do a lot) they just need to run some simple processing function 1000 times on data in the cloud.  I tell them that Dask is overkill.

But working with users, it seems like cloud solutions to this don't provide a great UX.  Why not?  Surely this can't be that hard.

I wrote a small blog post about this here: [https://medium.com/coiled-hq/easy-heavyweight-serverless-functions-1983288c9ebc](https://medium.com/coiled-hq/easy-heavyweight-serverless-functions-1983288c9ebc) .  It (shamelessly) advertises and thing we built on top of Dask + Coiled to do make this more palatable for non-cloud-conversant Python folks.  I'd welcome feedback/critique.  This was kind of a slapdash effort, but seems ok?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fbu8g/cloud_services_to_run_a_python_function_in_the/,dataengineering
Auto-Synchronization of an Entire MySQL Database for Data Analysis,15fbffy,ApacheDoris,1690893548.0,,False,True,Blog,1,False,2,"Flink-Doris-Connector 1.4.0 allows users to ingest a whole database (**MySQL** or **Oracle**) that contains thousands of tables into Apache Doris, a real-time analytic database, **in one step**.

With built-in Flink CDC, the Connector can directly synchronize the table schema and data from the upstream source to Apache Doris, which means users no long have to write a DataStream program or pre-create mapping tables in Doris. 

When a Flink job starts, the Connector automatically checks for data equivalence between the source database and Apache Doris. If the data source contains tables which do not exist in Doris, the Connector will automatically create those same tables in Doris, and utilizes the side outputs of Flink to facilitate the ingestion of multiple tables at once; if there is a schema change in the source, it will automatically obtain the DDL statement and make the same schema change in Doris.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fbffy/autosynchronization_of_an_entire_mysql_database/,dataengineering
Question about Operational Reporting,15fb0wr,Pure-Insanity-1976,1690892489.0,,False,True,Discussion,3,False,2,"I currently work as a data engineer on a team responsible for creating data structures for strategic analysis (such as the data warehouse), but our team is also responsible for tactical, operational reporting (i.e. a lot of one-off reports directly against transactional data) as well as integrations to provide data to third party applications. I am curious as to how widespread it is to mix these two functions on a single team? Also, for those teams who do cover both strategic and tactical reporting, do you have dedicated report writers, or is this duty handled by your data engineers and BI developers? Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15fb0wr/question_about_operational_reporting/,dataengineering
Bridging Financial Data Streams: A Look at ASOF Join in Pathway,15fdzts,arimbr,1690899667.0,,False,False,Blog,0,False,1,,False,1.0,https://pathway.com/developers/tutorials/finance_ts_asof_join/,dataengineering
What are the best programs in top colleges to get a master's in Data Engineering?,15fchuj,ajeenkkya,1690896156.0,,False,True,Career,2,False,0,"I did my bachelor's in India and have a 3 year work experience in a consulting firm where I developed interest in data engineering. I feel I have a high level knowledge in this field and I want to learn more (I know online courses are best to get me up to speed) but i am looking for employment opportunities abroad as well.

Please suggest and thanks in advance.",False,0.5,https://www.reddit.com/r/dataengineering/comments/15fchuj/what_are_the_best_programs_in_top_colleges_to_get/,dataengineering
Hiring Managers: Which Education Path Would You Prefer?,15fbsog,anewguy03,1690894451.0,,False,True,Interview,11,False,0,"All else being equal, which education path is better to you in a candidate (or does it not matter?)?

Business degree -> MBA -> Bachelor’s Comp Sci

Or

Business degree -> MBA -> Master’s Comp Sci

Thanks!",False,0.5,https://www.reddit.com/r/dataengineering/comments/15fbsog/hiring_managers_which_education_path_would_you/,dataengineering
y'all keep asking for book suggestions,15fghp0,keeney_arcadia,1690905302.0,,False,False,Meme,5,False,0,,False,0.38,https://i.redd.it/7vhf9adltifb1.jpg,dataengineering
A simple way to estimate memory consumption of PySpark DataFrame,15ehmrq,Hefty-Consequence443,1690813460.0,,False,True,Blog,4,False,36,"A simple way to estimate the memory consumption of PySpark DataFrames by programmatically accessing the optimised plan information:

[https://medium.com/@miguel.otero.pedrido.1993/dataframe-memory-consumption-8687354263e2](https://medium.com/@miguel.otero.pedrido.1993/dataframe-memory-consumption-8687354263e2)

&#x200B;",False,0.92,https://www.reddit.com/r/dataengineering/comments/15ehmrq/a_simple_way_to_estimate_memory_consumption_of/,dataengineering
Moving to an Architecture role?,15elp9j,BramosR,1690822913.0,,False,True,Career,19,False,16,"Hey everyone!

I’m a senior data engineer with almost 7 years of experience, and now I got to the point where I was trying to look for the next step in my career and I just… couldn’t see anything.

So my question is, would an architecture role be the next step? Has anyone here moved from data engineering to a data architecture or even a solutions architect role?

Thanks!",False,0.88,https://www.reddit.com/r/dataengineering/comments/15elp9j/moving_to_an_architecture_role/,dataengineering
"CS student here, wanna know if Data Engineering would be a good path.",15evmot,SnooPineapples7791,1690846132.0,,False,True,Career,13,False,6,"I am starting my 2nd semester at a CS course and i have been pondering about my future paths and career and i liked the ideia of Data Engineering.

On my first semester i used Python for some NLP/LLM projects and really liked it, usually more applied stuff using established frameworks, i also got some nice stuff with professors lined up about this topic (ML/AI/NLP)

I also heard some quite interesting Data Engineering projects (pipelines, cloud hosting and Pyspark to creating continous data flows) and liked them.

So then i suppose the closest things to my interests would be a Machine Learning Engineer, but then i heard MLE jobs are not only fewer than DE but also usually tougher on academic requeriments (big emphasis on having a masters), then i learned of DE which has more job openings, usually less requiring of academic titles so i guess that more fitting.

Also many DE jobs ask for ML experience so all my interests and projects could go towards building a CV of a DE with knowledge of ML 

I mainly want to ask you guys hows the job market for Data engineering? one of the reasons i dont want to get into Data Science is not only preferring a more engineer and software role but also the fact that it seens to be a quite saturated market, is DE less saturated than DS? what do you guys think?

&#x200B;",False,0.71,https://www.reddit.com/r/dataengineering/comments/15evmot/cs_student_here_wanna_know_if_data_engineering/,dataengineering
"What schema type of my default, boring, old-school database using?",15f1rs9,atlwellwell,1690863083.0,,False,True,Discussion,3,False,2,"tldr: what database schema type have i been using for 30+ years?

I'm an old and have only tangentially been involved with data warehouses and most/all of my 'data engineering' work has been as some type of app developer or etl consultant or sales engineer or something. I either hooked into (via sql, api, etc.) or created hundreds of databases -- small, medium, large, stupid, etc.

I never thought of star vs. snowflake vs. data vault vs. whatever, and I don't suspect any of those databases did either - unless they were and I was just unaware of these schema types/design patterns/whatever. 

The only exception would be if I hooked into something that was specifically called a 'data warehouse' and had someone or a team that could talk about its 'facts' and 'dimensions' and 'slowly changing dimensions' and such - and I pretty much never worked with data warehouses except to prove that I could get connectivity, query it, etc.

I've given up on trying to figure out what various schema types supposedly are - star, snowflake, data vault, etc., but I am still curious -- what schema type were these hundreds of databases that I either created or worked with? 

When I think of a database, I'm thinking of ER diagrams and logical and physical models, some other design decisions and artifacts and things like tables and views and keys and PKs and FKs and constraints and procs and funcs and whatever, but there was never really any 'star' nor 'snowflake', but I guess I \_could\_ have some tables which seemed to be \_more important\_ than some of the others, more central to whatever your business was -- like 'Product' in an e-commerce store. It might have some 'leaves' (?) radiating outwards from it -- relationships to other tables -- but that was true of most tables in the schema except for the dumbest key/value lookup tables.

so, what schema was I using? or the umpteen data modelers that created the databases that i used? what is the schema type of a standard line of business database? is it 'star' by definition? or is it 'none', or 'none - just normalized - the default'?

is there an analogous concept somewhere else in IT?

thanks!",False,0.75,https://www.reddit.com/r/dataengineering/comments/15f1rs9/what_schema_type_of_my_default_boring_oldschool/,dataengineering
"My God, It's Full of Data Pipelines",15ev2w6,botswana99,1690844736.0,,False,True,Blog,0,False,5,"**The Ten Standard Tools To Develop Data Pipelines In Microsoft Azure.**

**Is it overkill? The paradox of choice? Or the right tool for the right job? We discuss.**

[https://datakitchen.io/the-ten-standard-tools-to-develop-data-pipelines-in-microsoft-azure/](https://datakitchen.io/the-ten-standard-tools-to-develop-data-pipelines-in-microsoft-azure/)

[My God, its Full of Azure Data Pipelines](https://preview.redd.it/wl4wzsnqtdfb1.png?width=736&format=png&auto=webp&s=f302def44295437f5294b21a92a0d5c21554edc5)",False,0.73,https://www.reddit.com/r/dataengineering/comments/15ev2w6/my_god_its_full_of_data_pipelines/,dataengineering
Seattle Data Guy - What is Apache Iceberg?,15fd3i8,AMDataLake,1690897603.0,,False,False,Blog,2,False,0,,False,0.3,https://www.youtube.com/live/SzTOhiyiZKE?feature=share,dataengineering
Change Data Capture Is Still an Anti-pattern. And You Still Should Use It.,15eimaj,sap1enz,1690815715.0,,False,False,Blog,14,False,22,,False,0.92,https://streamingdata.substack.com/p/change-data-capture-is-still-an-anti,dataengineering
Column level data encryption,15evj9w,Foodwithfloyd,1690845894.0,,False,True,Discussion,3,False,4,"I've been pulling my hair out trying to figure out how to implement a column level encryption similar to what's available on spark. My stack is essentially s3 parquet files with a trino layer that pipes to postgres. On a different life I would use a spark udf but I don't want to pull in spark just for encryption. 

Been doing a fair bit of googling, am I crazy or is this just not a thing you can do in trino or postgres?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15evj9w/column_level_data_encryption/,dataengineering
"Interview prep for DE , SWE Eng; Data, Data Infra , Data Platform, Analytics Engineer roles.",15ev7ew,citizenofacceptance2,1690845062.0,,False,True,Interview,0,False,5,"Hi all, I was recently laid off and have about 10 years experience (USA)  so have done tasks that could fall under any of the titles above. Most interview will most like be DE roles or SWE;DE. That being said when I have interviewed in the past and what I am seeing again is a lot of variety in what these roles get interviewed for and recruiters prepping you  not really even being as discrete. Here are the 3 flavors I’ve seen. 

1. Bigger tech; gives you random leet code that  could have nothing related to DE they just are washing you through the ritual of checking algo/data structure knowledge. 

2. Seems like “HCOL well funded tier 2” start ups will mimic the above (and recruiters will always say prepare leetcode style ) but will sometimes deliver a bespoke question closer to what a swe would think a de does ie process log data data from a server and return  different aggregates of that data but only using the stand library of a dynamic programming language and return it in a dictionary(ie python without pandas ). Which seems like always hard to align on what the are really trying to test for other than if you recall all the languages out of the box functions really well. 

3. Usually small start ups; take home project to write a pipeline. 

**curious if we feel the same ** 


I just get frustrated spending hours on leetcode and Big O to only find my interview went south because I have not made an aggregation for awhile off a python dictionary or forgot the regex syntax or some nuance of .split(). 

Additionally while trying to brush up on traditional CS concepts and also applying / interviewing it is hard to not have some atrophy on SQL and day to day python  coding ; I know I can practice and make a project but it’s hard to determine how much to invest vs CS fundamentals, language of choice nuances, hard transformations , one off dynamic programming questions , leetcode , general architecture and other tech needed to be known as a DE. 

I know it’s all apart of the job and I have experience doing it all just feeling like it’s hard the future into read an interview process. 

Maybe discussion here on how to best prepare would help us all ? My current approach to getting a job is below and I am still crafting as I was laid off two weeks ago and trying my best to get a job offer in 10 weeks (I know , ambitious but also last employer give little severance to us). I’ve come from a place where some days i wrote Python, filled in terraform yaml files , spun infra in cloud, model data, debug airflow ect. So my skill repetition was not narrow. 

I wanna optimize how to prepare and succeed in this job market and it seems like most interviews say they care about skills in “cracking the coding interview” , but doesn’t seem to be spot on for people with our background ie data engineering. Here is my schedule below , open to feedback. 

M-f (unless interviews mixed in or having me do this on the weekend)
1) morning: (up to 4 hours ) doing algo/data structure training. Ie hand writing pre_order_traversal template. Hoping this also reinforces python syntax as well. 
2) afternoon: . As much as I can apply to jobs via LinkedIn and take interview. 
3) evening — 1hr; lightly refresh on python syntax , sql & genera DE architecture (this work is pretty unstructored — weather it be w3schools, terraform docs , k8s medium article). Try to relax and change mind space.",False,0.86,https://www.reddit.com/r/dataengineering/comments/15ev7ew/interview_prep_for_de_swe_eng_data_data_infra/,dataengineering
Modern Data Platforms: PostgreSQL,15f3qnc,throwaway_112801,1690869492.0,,False,True,Discussion,1,False,1,"Is it a correct assumption that most modern data Platforms such as Big Query, Data Bricks, and Snowflake are based on the POSTGRESQL dialect? Thanks.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15f3qnc/modern_data_platforms_postgresql/,dataengineering
Accelerators for Data Engineering,15f3114,cida1205,1690867119.0,,False,True,Discussion,1,False,1,"Looking forward to suggestions on the accelerators that drive delivery in data engineering space, what could be the starting point. I am planning to templatize some of the IAC and consider that as a service. Data as a service is a huge umbrella, wanted to understand if someone took the route and what were the key things that were kept as milestones",False,0.6,https://www.reddit.com/r/dataengineering/comments/15f3114/accelerators_for_data_engineering/,dataengineering
Accounting software data transfer to external server.,15f2pay,AlexJamesAce,1690866048.0,,False,True,Help,1,False,1,"Hello Everyone

I'm a new data analyst at a small engineering firm and I need some guidance in setting up an external database to easily analyze data. 

I'm very new to this profession (having transitioned from a pure mechanical background) so I apologize for any mistakes that I make. 

Our company uses QuickBooks Desktop enterprise for it's accounting and for my analysis I usually export reports in csv format and then plug those into MySql for further analysis. Although this method fits my need for now, I would like to setup a link to QuickBooks directly to load it's data into an external database so I can analyse perform queries and analysis without having to export multiple reports frequently. 

I'm looking up some solutions and here's what I've found so far. 

1. Setting up an QODBC driver with MS Access(?) To load data into it and perform queries. I'm not familiar with MS Access so any guidance on this will help immensely.

2. There are are third party tools that also provide integration, but most of them link to the online version or are paid. 

3. Custom API integrations. I'm not a programmer (still very early into my programming journey) so I wouldn't know where to start. 

I understand QuickBooks also has SDK (literally came to know about this when researching), but due to my lack of programming knowledge, I don't know if I can utilize this or not. 

Any help or guidance will help me immensely and truly appreciated. Thank you.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15f2pay/accounting_software_data_transfer_to_external/,dataengineering
"Aside from additional storage costs, what are some arguments for and against promoting raw tables (bronze layer) to silver and then gold but without transforming the data throughout this process?",15eqpc0,azazazazaz3,1690834396.0,,1690837717.0,True,Discussion,7,False,3,"Essentially the gold-layer data will be the exact same as bronze.

Backstory:

Dealing with a neglected DB system that, over time, has seen several reports being generated from the bronze-layer, which is a db consisting of raw, historic data. The system contains the usual silver/gold layers, where the **majority of reports are being generated from gold after the underlying data is transformed in silver**. I'm overhauling this and designing a new system, and will most likely keep the raw/transform/report 3-layer structure.

I'm thinking that the best approach here is to take the tables that are being used for these Bronze-facing reports and promoting them to Silver and then Gold without transforming them (however, I'll add steps to validate the data and minimize any issues with fidelity).

What benefits and caveats, aside from additional storage cost, can you guys think of with having a mirror image of the respective bronze tables as a silver and gold layer?",False,0.72,https://www.reddit.com/r/dataengineering/comments/15eqpc0/aside_from_additional_storage_costs_what_are_some/,dataengineering
What topic would you choose in a masters?,15esyxb,abbadb,1690839623.0,,False,True,Help,6,False,2,"This is my first topic here, and my motivation comes from the situation I am facing. I have been studying formally since I 5\~6yo, went all up to school, high school and here in Brazil, we have no college; it is a direct university, last year I graduated with a bachelor's in Information Systems, and I am thinking about doing a master.

I have been working with data since 2020; I started as a data analyst and became a data scientist, and right now, I am in my second year working as a data engineer. During all these processes, I have worked on different kinds of companies, from 30-man startups to multinational consulting. 

I am enjoying working with data engineering, and I want to go deep in my knowledge; some must say that master's doesn't matter much to this, but I am feeling a bit off not studying formally after that many years. I have two options and would like to know what you think may be better to get specialized for the present and future. 

The first would be to work with a younger professor, known to be a cool guy; he is not that famous but has a Ph.D. from Pisa University (Italy) and has researched streaming lines with C++ etc. Streaming is exciting, but I have never worked with it since it is a niche area with which only a few companies work. 

The second professor is more famous, has a PH.D. from Georgia Tech and has a bit more projects with companies; his researches are more focused on data architectures, delta lakes, data lakes, data warehouses etc. 

I would prefer the second one maybe, but I would like to hear from experienced people before making a choice.  


I feel that I would like to do more research in theoretical content, maybe something like a study about cloud architectures for big companies using more modern tools for streaming instead of coding something specific if I chose the first professor or with the second one something in this architecture side too but on this data lake/delta lakes etc. point of view.",False,0.6,https://www.reddit.com/r/dataengineering/comments/15esyxb/what_topic_would_you_choose_in_a_masters/,dataengineering
Is this feasible?,15ekc89,MiserableCharity7222,1690819717.0,,False,True,Career,6,False,8,"I'm currently in my last (hopefully) year of grad school and I'm also working full time. I have goals of working in data engineering, but currently I work in a role that isn't in the general field of data analytics, nor data engineering, however, I've done multiple analytics projects at work, as well have made some automation pipelines, but my day to day work doesn't really involve data engineering work. I've been advised by mentors to find work that would allow me to use sql or python on a daily basis so I wouldn't have to worry about doing side projects outside of work and school. I just want to be in a good position come graduation time so I can compete for data engineering roles, and that's only possible if I keep my skills sharp (SQL, Python, etc) Any advice would be greatly appreciated.

Edit: The only program I use at work is a no-code program, but I'm hesitant to dive into using it because I don't want my coding skills to atrophy. ",False,0.73,https://www.reddit.com/r/dataengineering/comments/15ekc89/is_this_feasible/,dataengineering
DE team experience,15eg3n3,iupuiclubs,1690809757.0,,False,True,Career,6,False,11,"Hey all,

I'm looking for a sanity check on an experience I'm having with a team lead/jira sprint lead. He seems brilliant but I'm looking for advice on what to do with a potentially unstable jira team lead.

I've been with this company 6+ months. Agile/jira/sprints. A lot of looker based tickets, and some python. I enjoy the looker, and its interesting catching up on all the docs and looking through codebase.

Team felt great on joining. I noticed our team lead seems to be very ""passionate"". The other 6 on the team are pretty calm and stable so it feels to even out. Something seemed to shift last month. 

I met to go over a python 5 line functional commit, and he asked me on the spot if I wanted to refactor this into something better architecturally. I was worried about finishing the ticket, but he seemed confident so I said ""if you think it's possible"". He said it was easy and we started pair programming this (my in retrospect opinion) monstrosity. 

What I'm worried about is little things I'm observing:
* Asked me to remove a variable and implement DRY code in 6 places. Repeating the call/not storing the data. Major wtf question for me.
* Given two tickets, one using a new field created by the previous. I asked if these were linear tickets (T1 creates a full timestamp from two elements, T2 uses T1.timestamp). I was told no with an air of ""why would you think that"".
* Left an interdepartmental meeting abruptly during a discussion because another department was trying to explain something didn't want to hear. (I've never heard of just exiting a meeting abruptly)
* Reprimanded for using specific markdown in ticket communication, that i found referenced in our team written docs after.
* Every sprint is a nebulous push for higher points. Last sprint we did 50% more points than projected and there wasn't a satisfaction relayed in retro.

I get along great with the other members on the team. I come from a finance/IT background, so I'm wondering for guidance on what are norms in jira style work.",False,0.93,https://www.reddit.com/r/dataengineering/comments/15eg3n3/de_team_experience/,dataengineering
Life balance?,15ez9mo,Alexshvd,1690855773.0,,False,True,Discussion,10,False,1,"Hi there! I need your advice. I'm currently working as a QA Engineer, but I'm considering moving to Data Engineering due to higher salary and future prospects. The training is quite challenging, especially the part with Python, but I'm in progress.

I would like to ask how are you doing with life balance at work? As a QA, I have fixed hours and no one bothers me before or after work. I have an hour for lunch, and also, if there is no particular load, I can do my own business during working hours.

Would be awesome if you could tell me about your work experience. How much stress, fatigue, and other negative factors? It will also be interesting to read the positive aspects that can serve as a motivator for me. Thanks everyone in advance!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ez9mo/life_balance/,dataengineering
Spark-Submit on K8?,15ercry,,1690835883.0,,False,True,Help,3,False,2,"How do you run spark-submit on a K8 cluster? It’s my understanding that you can exec into any pod and run spark-submit from there.

Would I just need to create a pod that terminates after spark-submit is run? Or is there a better way to do this? 

 The goal would be to trigger this from airflow by posting the pod YAML file using kubectl.

Is there a better way to go about this?",False,0.75,https://www.reddit.com/r/dataengineering/comments/15ercry/sparksubmit_on_k8/,dataengineering
Sink process taking way too long in Pyspark dataframe,15ekclt,imameeer,1690819741.0,,False,True,Help,4,False,3,"I'm totally new to spark and learning it continuously.  Currently working in a pipeline were I have already extracted needed data from a source file and did all the transformations and the final df looks fine to write and heres were I'm stuck now. When I try to write the output in s3 by using partitionBy based on two columns ""name"" and ""type"" and since there maybe multiple entries for same name and type combination while writing parquet if its present in different partition it writes a new file instead of appending.

To overcome this previously my teammates have tried doing coalesce(1) and the final df have over 2 million records and it was taking more time and it was never completed even after 2-3 hours.

So suggested using repartition(100, ""domain"") based on the column domain which will make sure all name and type for a single domain comes under same partition. But still it also took the same time and the sink process is not completing.

To cross check why its getting stuck.. once the final df is ready I checked the natural partitions and it was around 1060 something so I have again tried repartition with 500 and based on domain column and again takes the same time.

We are using AWS EMR with 1 master 14 cores to do this and tried increasing it to 20 as well but still stuck at the same point.

Since its in jupyter notebook in EMR.. the last job process which is doing the sink process finishes first 54 steps within 10 seconds ans afterwards its taking more than 45 mins for a single task and total 1000 something tasks were scheduled (55/1012).

Can you someone please help me out how to resolve this issue and write the file ? so that s3 has name/type/single parquet file for it which is used my a restapi to get data.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ekclt/sink_process_taking_way_too_long_in_pyspark/,dataengineering
need aws to azure replication,15emxyv,PrtScr1,1690825780.0,,False,True,Help,2,False,2,"We need to replicate / sync AWS Mysql db to Azure SQL db.  The DB is small size with nominal activity.

Is there a build in solution available in either platform? If not, what is the simplest solution we can design and implement using which tools?

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/15emxyv/need_aws_to_azure_replication/,dataengineering
From Half A Day To Half An Hour! Performance Tuning Snowpark For Identity Resolution On Snowflake,15ej2ut,sonalg,1690816796.0,,False,False,Blog,0,False,3,,False,1.0,https://www.learningfromdata.zingg.ai/p/performance-tuning-snowpark-for-identity,dataengineering
Batch processing for ML pipeline - questions in regards to data ingestion and data storage,15esawn,aloy_joz,1690838075.0,,False,True,Help,2,False,1,"I am working on a batch data pipeline for a ML application (large scale) as a first data engineering project. I am new to data engineering and therefore not familiar with most of the microservices and best practices and have a few issues understanding some concepts. (ChatGPT unfortunately also did not help much)

1. Data Ingestion Layer: In most sources that I have read so far Kafka is suggested for pulling data from the original source (a CSV in my case). When reading only about Kafka though, it is described as a tool mainly used for stream processing. Why should I use Kafka for batch processing then and is this really best practice? What are the alternatives?
2. Data Storage: I am also very indecisive when it comes to data storage for the batch processing pipeline. Most of the microservices seem to over-perform for my actual needs and I am wondering if there might be a lean solution. E.g. I have a dataset with structured data so MongoDB for example might be a bit excessive.

Further comments: so far, I have settled on using PySpark for Data Processing & Aggregation. Airflow & Docker for scheduling & orchestration

I have to decide on certain microservices to use and come up with a flow chart for the whole data pipeline. For advice and best practice tips as well as usefull git repos, I would be very grateful.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15esawn/batch_processing_for_ml_pipeline_questions_in/,dataengineering
Tool or service for querying and exposing database through API,15ehxh4,Montty1,1690814149.0,,False,True,Help,8,False,3,"I am looking for service or tool similiar to [Metabase](https://www.metabase.com/) or [Redash](https://redash.io/) that allows me to add data source - for example Postgres connection, and create raw SQL queries that can be shared or exposed through API. So instead of keeping raw SQL code somewhere, my other service would call this tool e.g. `http://microservice/query=1?param1=xx&page=2` and get the results from the DB. 
These calls are internal only and part of ETL processes, but of course authentication would be required.

The services that I mentioned are more or less focused only on visualizations, I do not really need that, but for example Metabase is almost perfect as it has API that can work with parametrized queries, but for sake of visualizations it has hard limit on the count of results that are returned.

Any other suggestions are welcome.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ehxh4/tool_or_service_for_querying_and_exposing/,dataengineering
Options to integrate DBT with GCP Secret Manager,15ej945,tmanipra,1690817193.0,,False,True,Discussion,7,False,2,"Hi All, I'm working on a Side project design to Hash Credit card number data with a Secret value from Secret manager. 

DBT to read Source BQ table, Get secret value & concatenate with PII Column which needs to be hashed with SHA256. 

I'm not able to integrate DBT with Secret Manager. Storing secret as Environment variable option cannot be used as SM option to be tried. 

I have options to include Cloud Function, Composer in my design. 

So I have below things in mind:

1. Composer DAG to access secret via Cloud function & pass as XCOM variable to DBT task. 
2. Composer DAG to get secret using Secret backend & pass as XCOM variable to DBT task. 

Also, Secrets should not be in readable format in Composer logs. 

Which one is feasible or please advise other alternatives? 

   

   ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ej945/options_to_integrate_dbt_with_gcp_secret_manager/,dataengineering
job search advice (~3 years of DE),15e5pab,Comprehensive_Ad8288,1690776645.0,,False,True,Career,8,False,13,"hey folks,

long time listener first time caller. i wanted to get some advice on how i might advance my career in the DE world. i'm a data warehouse developer at my current job and we use an on-premise oracle data warehouse and build our data pipelines with SSIS - they have hopes of migrating our DW solution to the cloud eventually, but i fear it will be several years from now and that i will be tragically left behind on the skill curve by then. i'm hoping to hop into a job that utilizes a more modern data solution, and of course, pays more. 

&#x200B;

so yeah, my experience includes:

\- building data pipelines in SSIS

\- advanced knowledge of SQL

\- adequate knowledge of python (mostly retrieving data via APIs)

\- a data science master's degree (for what it's worth, but i've since decided i prefer DE)

\- replicating some of my company's current pipelines in ADF and Spark with ADLS, as a sort of proof of concept for my manager

&#x200B;

i was thinking of maybe getting some certs in databricks or some equally popular DE technologies just to show some initiative, though i'm not sure any hiring managers would care. i'm not sure i could reasonably expect to get hired with the experience i have, though i'm very confident i could perform the duties of the role in a more modern DE environment.

what do ya think?",False,0.94,https://www.reddit.com/r/dataengineering/comments/15e5pab/job_search_advice_3_years_of_de/,dataengineering
"How to test the relationship between red dots & yellow dots??? [R package, archaeologist needs help lol]",15eek4s,enemies2l0vers,1690805744.0,,False,True,Help,8,False,2,"&#x200B;

[ Is there any way to statistically test \(In R\) the association between the red dots \(rock art\) and the yellow dots \(funerary monuments\)? I want to prove the red dots are not just randomly located in the landscape but always situated in relation to yellow dots. How do i test for that relationship objectivley? ](https://preview.redd.it/vg882vbglafb1.png?width=1724&format=png&auto=webp&s=d1b1cde262fe505260cf0f1b26d9fc4849f45c03)

 *I want to prove the red dots are not just randomly located in the landscape but always situated in relation to yellow dots.*

I already have the coordinates of all the points so could easily do this via distance measurements between each point.

I've been reading papers and there seem to be **a couple of options** (none of which are explained very well, and I'd love some more clarification on):

1. Use a Monte-Carlo simulation \[Vanacker et al. 2001\] to prove red dots are not randomly situated. Apparently Vanacker converted the distance measurements between points into categories eg. <10km, 10-15km, 15-20km etc. But they didn't include a very good method or any of their code in the paper so I can't tell exactly how that would work and am struggling to find other examples/resources
2. Point Pattern Analysis since I am dealing with 'environmental coviariates' = second-order properties? \[Kempf & Gunther 2023 say they: ""used spatstat package in R and function rhohat to calculate site intensity as a function of the pre-processed focal raster data to visualise the effect of attraction or repulsion given by a specific parameter...""\] Why do they need to convert point data to raster data for this analysis?
3. Multivariate Regression since this would allow me to include other variables like elevation, distance from water source, soil type etc

Or are all these ideas bad and should I try another way?

Thankyou so much for your help, feel free to point me elsewhere but this is the result of my googling so far :))))",False,0.67,https://www.reddit.com/r/dataengineering/comments/15eek4s/how_to_test_the_relationship_between_red_dots/,dataengineering
Real life examples of troubleshooting streaming data ingestion and use cases,15elzja,sanimesa,1690823583.0,,False,True,Career,0,False,1,"Wondering if experts here can share some real life streaming use cases they have built, and how troubleshooting is performed on these pipelines. 

Particularly the troubleshooting aspect of it - how do you ensure no data loss? Are there deduplication processes or manual reconciliation after restarting the jobs?

I am a data engineer on GCP but have not built any streaming jobs other than the tutorials on Dataflow and a few pub/sub use cases. Streaming experience appears to be highly critical in finding a job, but these are not cutting it. Any pointers will be deeply appreciated. 

Thanks.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15elzja/real_life_examples_of_troubleshooting_streaming/,dataengineering
Data Privacy and Compliance: Ethical Web Scraping with Python,15el2db,TheLostWanderer47,1690821405.0,,False,False,Blog,0,False,1,,False,1.0,https://python.plainenglish.io/data-privacy-and-compliance-ethical-web-scraping-with-bright-data-and-python-8cc63b1c3db4,dataengineering
Data Engineer interview experiences,15dutwi,Delicious_Attempt_99,1690747259.0,,False,True,Interview,34,False,38,"Greetings everyone,

I am a Data Engineer with approximately three to four years of experience in this domain. Currently, I am exploring job opportunities, particularly within product-based companies in Europe.

I would greatly appreciate it if you could share your recent interview experiences for Data Engineering roles ( any level ). I'm particularly interested in understanding the various stages and types of interviews you encountered during your job application process.

With few interviews which I gave, it looked something like below
1. Screening round - call with recruiters, briefing for what role is about
2. Hiring manager round - interview round with hiring manager, discussing depth about your previous experiences
3. Technical round or take home assignments - not much aware of this round, since I have just started interviewing and few are lined up in upcoming days
4. Designing data pipeline
5. Culture fit / Behavior round 
6. HR and release of offer after negotiations.

Thank you for your insights in advance.",False,0.93,https://www.reddit.com/r/dataengineering/comments/15dutwi/data_engineer_interview_experiences/,dataengineering
Course Recommendations,15eifq6,Pershanthen,1690815295.0,,False,True,Help,1,False,1,"Hey guys!

I'm looking to enhance my data skills and dive deeper into Microsoft's powerful suite of tools for Business Intelligence and data processing. Specifically, I'm interested in learning more about SSIS, SSAS, SSRS, and Power BI.

I'm reaching out for some course recommendations do you guys perhaps have any? ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15eifq6/course_recommendations/,dataengineering
Anyone gone through Booking.com Data Engineering interview process,15eifkv,,1690815286.0,,1690815935.0,True,Interview,0,False,1,"I wanted to know the process of the interview. And mainly what to expect during the Code pair round.

Where should I be concentrating on. Is it DS and Algo or mainly the Data Engineering Tech stack related topics.

Thanks.",False,0.67,https://www.reddit.com/r/dataengineering/comments/15eifkv/anyone_gone_through_bookingcom_data_engineering/,dataengineering
WRITING CLEAN JAVA CODE,15ehk9q,Syntax_Maestro_SE,1690813296.0,,False,True,Blog,12,False,0,[https://digma.ai/blog/clean-code-java/](https://digma.ai/blog/clean-code-java/),False,0.4,https://www.reddit.com/r/dataengineering/comments/15ehk9q/writing_clean_java_code/,dataengineering
Looking to get either a Data Engineer Masters or a Data Scientist Master: Tell me why you chose to go with your path!,15duqal,Apollo_3_14,1690747017.0,,False,True,Career,54,False,27,"I'm currently a data analyst with two years experience and have a bachelor's degree in Actuarial sciences. I have some competence in SQL, PowerBI, SSIS and have decades of experience in Excel and have a reasonable enough understanding of Linear Algebra, Calculus (1-4), Statistics, , SARIMAX Time Series analysis that I believe that I could pick up many of the ML basics pretty quick. That's my argument for a Data Science Master's Degree. On the other hand most of the problems I see with the data projects that I'm envlolved in stem from the business leaders not understanding the importance of having solid data quality and data pipelines. I believe as time goes on in the next decade or so that more companies will realize they actually need Data Engineers as a solid basis to build their application upon. This is my argument of getting a Data Engineer Master's. Can anyone help me with some direction, insite or constructive criticism of my arguments that I have to help me get off the fence and get an education. ",False,0.79,https://www.reddit.com/r/dataengineering/comments/15duqal/looking_to_get_either_a_data_engineer_masters_or/,dataengineering
"Preparing for an entry level ""systems engineering"" role. Can anyone give me some advice?",15eg1ap,InevitableTraining69,1690809600.0,,False,True,Career,1,False,1,"So I'm preparing to interview for an entry-level systems engineering role in the data engineering department. Supposedly, it's directly related to systems engineering, so compute resources in Google cloud, setting up confluence and resolving Jira tickets. My concern even though this is an entry-level position is that they are going to be asking a lot of questions about some technical stuff that I've only had brief, extremely limited exposure to. I've used confluence for a grand sum of about 20 hours total, Jira for like 50 hours total, and Even though I am very well versed in a lot of data engineering functions, my skill and systems engineering is kind of lacking because I never had exposure to that.

Here are some questions I have:

 


- How do I address questions that I don't know, or don't have experience with? I think I have shot myself in the foot in the past in interviews when faced with these kind of questions. I usually say something like ""well I don't really have experience with this directly, in all honesty, but I am definitely open to learning more about this and finding out more"". I'm not even sure how to answer these questions when I don't know the answer.  ""What's your experience with Bash scripting/confluence/JIRA?"" How do I answer this question with novice level experience and skill and not sound like an idiot? ""I've never done that "" It's definitely not the best way to answer this I bet.... How do I explain it? 



- How do I explain my experience with projects unrelated to work in a way that doesn't sound clowny? I've been studying Google cloud platform and taking training on it extensively through Coursera and on Google's cloud platform website to try and skill up, and go for a certification in infrastructure. To that effect, I have several practice projects where I develop APIs, set up GCP and arrange compute resources, set up IAM roles, etc. I just don't know how to explain it in a way that makes it sound like it's more than just a silly project",False,1.0,https://www.reddit.com/r/dataengineering/comments/15eg1ap/preparing_for_an_entry_level_systems_engineering/,dataengineering
Polyglot Apache Arrow: Java and Python Perspective,15ecqrj,AmphibianInfamous574,1690800517.0,,False,False,Blog,0,False,1,,False,1.0,https://medium.com/gooddata-developers/polyglot-apache-arrow-java-and-python-perspective-bf2ce020e27d,dataengineering
We created an open-source semantic search package on top of PostgreSQL,15e0zpu,philippemnoel,1690762555.0,,False,True,Open Source,4,False,5,"Hey everyone! A few months ago my friend and I were working on a sustainability software project and wanted to use semantic search/vector search to help improve search accuracy for materials in our PostgreSQL database.

  
We found it difficult to do well with standard vector databases and so we ended up making a [nice open-source package to layer semantic search on top of Postgres](https://github.com/getretake/retake) with just a few lines of code. It supports Python backends right now, always stays in sync with PostgreSQL via Kafka, doubles as a vector store, and can be deployed anywhere.

  
We wrote some [documentation](https://docs.getretake.com/quickstart) on it and are curious to see what people do with it! If you encounter any issues or have exciting ideas, feel free to [open an issue](https://github.com/getretake/retake/issues) or contribute alongside us to make it better! Any feedback is warmly appreciated",False,0.86,https://www.reddit.com/r/dataengineering/comments/15e0zpu/we_created_an_opensource_semantic_search_package/,dataengineering
How do you size clusters for dashboard use cases where where the BI tool generates multiple queries for each refresh. Is your refresh interval getting more frequent?,15e4ilk,brrdprrsn,1690772832.0,,False,True,Discussion,7,False,3,Thank you for any advice you can offer on this. Also - Is it common to have the BI tool query pre loaded / static extracts (eg. in Tableau)? Or is the live connection mode to the data warehouse more common?,False,1.0,https://www.reddit.com/r/dataengineering/comments/15e4ilk/how_do_you_size_clusters_for_dashboard_use_cases/,dataengineering
This explains A LOT,15d1z98,morpho4444,1690662377.0,,False,False,Meme,96,False,447,,False,0.98,https://i.redd.it/m5tcvsl7ryeb1.png,dataengineering
SWE Practices for Analytics without DBT?,15dw48y,VersatileGuru,1690750277.0,,False,True,Help,9,False,4,"I work for an analytical team, and the idea of supporting our BI and analysts with transforming data and making it reusable is really appealing because there's a lot of duplication and hokey practices.

The idea of being able to manage virtual mappings, queries and transformations as maintainable models like dbt is appealing, but most of our folks are using python and spark so it doesn't seem like they would be able to make much use out of dbt since it's all SQL based (unless I'm misunderstanding DBT and after you create a DBT view / model it can be callable from python or spark somehow?).

How should I consider managing the 'T' in ELT but having it version controlled, reusable and documented in a primarily notebooks driven spark and python environment?

 We have our own local conda repo, could I just materialize and save transforms for them to use as a python module they can just run without going through all the HTTP or container setup?

Basically the use case is when an analyst needs a bit more heavy transformations, deduplication or some other processing and  rather than just writing a notebook and giving it to them, how else can I make it something that can be reused and maintained?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15dw48y/swe_practices_for_analytics_without_dbt/,dataengineering
Creating Datalake Layers on Azure Synapse Analytics and Databricks,15do9zm,No_Sheepherder5177,1690730756.0,,False,True,Help,9,False,9,"Hi guys.

I'm looking for a resource to study, how can I develop all layers of a data lake like raw, bronze, silver, gold using Synapse and Databricks. Does anyone know a course or material on the internet?",False,0.92,https://www.reddit.com/r/dataengineering/comments/15do9zm/creating_datalake_layers_on_azure_synapse/,dataengineering
Choosing a field to specialize in,15e50yl,chenvili,1690774471.0,,False,True,Career,4,False,1,"I have just started my full time job in the same company after almost a year there as a part time (70%). We use DBT+Snowflake+Argo (transitioning to Flyte, it's pretty nice!) for our ETL. And the team is in charge of creating APIs to access the data. 
As a part time, my job mainly consisted of data modeling, being in charge of small end to end ETL projects, visualization (kida sick of Tableau already haha) and stepping up my SQL game. 
Now I'm  in a place where I need to know a little more where I want to specialize, and I'm not sure what other companies are doing. I have never used any of the services like Spark or Haddop because we don't have any use in those. 
I guess I'm not sure which field is more needed, and what skills should I learn to become a more worthy data engineer. 

Would appreciate any insights on everything I said. Thanks you guys!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15e50yl/choosing_a_field_to_specialize_in/,dataengineering
Need advice on how to manage data requests from Finance.,15ds6ai,biga410,1690740623.0,,False,True,Help,5,False,5,"Hi Everyone,

&#x200B;

I'm currently tasked with building out the first DW at my organization. I have been in conversation with Finance since the beginning and I finally go them to generate a list of their needs for the future. The concern I have is that I'm not sure how to discern some of the requests between finance and accounting. As I understand it, accounting is a whole other beast that generally not worth building out a data platform for and is best left for an accounting team to address, as it can be a very complicated and low ROI endeavor for a data eng team to embark on.

&#x200B;

My question specifically is where is the line here? Is it when numbers are used to pair against an SOW for audit reports? is it when you need to account for changes in orders (forfeited, transfers, etc)?

&#x200B;

Thank you!

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ds6ai/need_advice_on_how_to_manage_data_requests_from/,dataengineering
Reporting Tooling,15e3z9n,quincycs,1690771208.0,,False,True,Discussion,8,False,1,"Hi 👋

I want a tool that can help me deliver this kind of user experience below 👇.   Any ideas / suggestions?  The user here is a typical non-tech user.  

1. Pick an already built report or entity to start with.   ( this is prebuilt by a software engineer / data engineer )
2. Customize the report by including / excluding a fairly large set of columns.  For example, 50 columns. 
3. Customize the report by simple filter options on each column that you expect from excel.  Sort too. 
4. Save the customized report for easy access in future. 
5. Export to excel
6. Near real-time sync with source application database.  Eg> stream updates.  Making the user press some refresh button is fine though. 
7. Settings for performance/latency.  Eg> Some kind of Cache / TTL be nice. 

I have an existing system that was custom built for this, running off of elasticsearch.  It’s super slow, and we haven’t been successful in making it faster / easier to maintain.  

I’d like options where I can buy some tool that does most of the work for me.",False,0.67,https://www.reddit.com/r/dataengineering/comments/15e3z9n/reporting_tooling/,dataengineering
Job Search Boards-USA,15e3m9f,srujanmara,1690770089.0,,False,True,Interview,1,False,1,"Hi Everyone,
For the DE related job postings which all websites you refer and apply.
Do you recommend to apply from a company careers page or is it fine if we use the Job portals which post out the positions and apply from the Portal Websites?

I want to understand the community perspective about this?
I'm currently applying from the Company Careers Page and see no luck.

Please guide me as per your understanding. 
Thanks in Advance.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15e3m9f/job_search_boardsusa/,dataengineering
Take a rest or invest in learning new stuff,15dgfsb,DarthDatar-4058,1690706214.0,,False,True,Discussion,18,False,26,"Hi guys,

I recently started as a DE at a software scale-up (4 weeks). It's a lot of work, especially since I'm still getting accustomed to the data model and data flows that the company has put together over the years. I notice that I'm often very tired at the end of the day, having done lots of ETL stuff within a context that is new to me. 

In the weekends I feel like I need to rest, but I also have to urge (almost like an obligation) to learn new DE stuff. I spoke to me friends and family and they say that I already have a lot on my plate, because I'm still getting accustomed to a totally new DE stack. They say I need the weekend to process everything and clear my mind before I can start with the next week. 

So how do you guys do it? Do you take some time in the weekend to learn new DE-related stuff? Or do you have the same feeling that you really need to rest and take your mind off of DE?",False,0.97,https://www.reddit.com/r/dataengineering/comments/15dgfsb/take_a_rest_or_invest_in_learning_new_stuff/,dataengineering
"Just curious, Any of you guys have some end to end solution in terraform that can spin up your entire bi infra and can be resused when you move to different companies ?",15dirqg,Sea-Forever3053,1690714628.0,,False,True,Discussion,20,False,14,"I will be starting my job in a startup with creating my own data team and infra from starch, and I am looking to use all open source tools like, airflow, any warehouse, dbt, some injection like mage, bi front end like Redash. I will be the only person initially and looking if you any of have an easy way yo spin up these using terraform or something? And any suggestions for infra/clusters/cost and anything that you think can help me? Thank you in advance",False,1.0,https://www.reddit.com/r/dataengineering/comments/15dirqg/just_curious_any_of_you_guys_have_some_end_to_end/,dataengineering
How To Create Compound Efficiencies In Engineering,15dt6fx,pinpepnet,1690743150.0,,False,False,Blog,0,False,3,,False,1.0,https://devinterrupted.substack.com/p/how-to-create-compound-efficiencies,dataengineering
Non-proprietary data architecture,15e17ma,Adorable_Compote4418,1690763163.0,,False,True,Discussion,3,False,0,"I’m currently identifying on which platform I’ll build my analytics/ml platform and I’m looking for second opinion.

Requirements:

-Non-proprietary solutions. I don’t want anything that is exclusive to Azure/AWS/GCP. After seeing what Microsoft is doing with Intune I can’t allow this to happen (changing price, features and else on the fly)
-Easily scalable. From a few gb per customer to petabyte class customer.
-Real-time analytics , efficient and fast data ingestion, delivery
-Full support and optimization of Nvidia rapids library/Intel AMX
-Designed and optimized for kubernetes

So far it seem like only Apache solutions (sparks, kafka, nifi, hbase and else) is making the cut. Am I wrong?",False,0.5,https://www.reddit.com/r/dataengineering/comments/15e17ma/nonproprietary_data_architecture/,dataengineering
Docparser reviews?,15dr3pv,Jamese0,1690737968.0,,False,True,Help,4,False,2,"Hi All, 

Just starting a new project which involves a full migration of all types of data into a data warehouse (which I have to design). I am currently looking at non-digital data i.e paper data records and I have been recommended Docparser for digitsation and transformation into SQL  and NoSQL setups. Has anybody got any experience with this product, or could you recommend any others?

Many thanks in advance.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15dr3pv/docparser_reviews/,dataengineering
Privacy-preserving LLM App for real-time data,15dn8x8,muditjps,1690728059.0,,False,True,Open Source,0,False,3,"Am currently exploring this open-source project which provides a fascinating approach to building a privacy-preserving LLM App. It functions in less than 30 lines of code using up-to-date knowledge from a data store. Notably, it omits the need for a separate vector database, potentially reducing the complexity of a fragmented LLM stack (basically Langchain + Redis + FastAPI, etc.). However, this approach might trade off vector database scalability.

**🔗 - Here's the repo link:** [**https://github.com/pathwaycom/llm-app**](https://github.com/pathwaycom/llm-app).

What could be the best use cases for this? Please feel free to explore and share your thoughts.

PS - I'm not a developer or contributor to this while I'm posting this. The developers are inviting contributions to the repo, though. Do feel free to check it out. :)",False,0.8,https://www.reddit.com/r/dataengineering/comments/15dn8x8/privacypreserving_llm_app_for_realtime_data/,dataengineering
Data Engineer presentation,15dm9us,Cilippofilia,1690725455.0,,False,True,Interview,16,False,2,"Hello guys,

I have recently applied to a Data Engineering apprentice position and had the first interview. Everything went well and now I have a zoom call on Wednesday as the second stage of the interview.

They asked me to prepare a presentation about any company that has implemented Digital Transformation in their data flow.
The presentation will last 5 minutes and then 25 more minutes of discussion - this forms the first part of the interview.
The other 30 minutes are related to general questions about data engineering, why I picked this path, ecc.

Here is where I need some help:
- should I assume that people have already a background of anything I am talking about? (I want to bring Snoop Dogg and Death Row records stepping into Web 3.0 and blockchain - empowering artists, owners and creators - this seems to be revolutionary in the music industry.

The second part of the interview:
- this will be competency based interview, where they will be looking to understand how i approach problems and my way of thinking… can anyone suggest some resources where I can learn the complete basics to nail this second part?

Here is some examples of Digital Transformation shared with me:
https://brainhub.eu/library/digital-transformation-examples",False,0.67,https://www.reddit.com/r/dataengineering/comments/15dm9us/data_engineer_presentation/,dataengineering
Career advice for a CPA exploring pivoting into Data Analytics or Data Engineering or Data Science,15dtcle,Numerous-Ad2412,1690743590.0,,False,True,Career,18,False,1,"I'm currently a CPA, working in b4 audit, my day to day work is really boring and I drag myself to do it. Before moving to the US, I worked in an international bank, heading the regulatory reporting unit. My responsibilities included designing, automating and submitting a complex set of regulatory reports which I really enjoyed. I also lead a data warehouse project where we designed and automated every single report. I have a good understanding of SQL, data warehousing, python, SAS enterprise guide and SAP Business Intelligence.   


My question is what is your advice for me to pivot into data engineering or data science? Should I seek a masters degree in data science or data engineering? Are online courses enough to secure a good position? What would you do if you were in my place?",False,0.6,https://www.reddit.com/r/dataengineering/comments/15dtcle/career_advice_for_a_cpa_exploring_pivoting_into/,dataengineering
Data Engineer moving to Support Engineer,15dnddo,caveman_fpv,1690728376.0,,False,True,Career,6,False,2," 2 years of experience with Azure, 1 with Synapse Analytics and Pyspark on a big data project. I got a technical interview for a dream company for a Azure Support Engineer and I have been reading through threads and threads here on reddit and all seem to say ""use support engineering position to get into DE"". But I'm already a Data Engineer. I LOVE working with SQL, python, pyspark and building pipelines. The pay isn't awesome also but I'm unemployed. Should I take this job while no other show up?",False,0.75,https://www.reddit.com/r/dataengineering/comments/15dnddo/data_engineer_moving_to_support_engineer/,dataengineering
How important is Leetcode for DE interviews?,15d0a1c,aaloo_chaat,1690657978.0,,False,True,Career,22,False,57,"I placed into my junior Data Engineer role (current) through a rotational development program. 
I've never done a leetcode-style interview before and was wondering whether Data Engineering job interviews usually require this type of testing.",False,0.96,https://www.reddit.com/r/dataengineering/comments/15d0a1c/how_important_is_leetcode_for_de_interviews/,dataengineering
What am I doing? What is what I'm doing called?,15d8ie2,Acidwits,1690679959.0,,False,True,Career,9,False,6,"Hi,

I'm trying to figure out if what I'm doing is one job or several and if not, what the typical title is for the kind of work I'm doing.

I work at a company of 700 as a ""Business Intelligence Analyst"" alongside a Data Engineer. We both report to the Head of Engineering. I was part of an acquisition of a company of 200 where I designed the entire architecture, created warehouse, ETL'd apps to it, created reporting for end users etc.

What I typically DO is three-fold:

- I grab raw data from a warehouse (snowflake) (Having advised on its ETL requirements), manipulate it into a model in Power BI, replicate the tables required to run reports off'f that model in SQL, and then get the data engineer to create that table in our warehouse. I do this for 3 different product lines (acquisitions) being folded into one.

- In parallel I work with C-Suite and/or operations leads from research/sales/support/finance/Hr/half a dozen others. I gather their requirements for commissioned reports or upgrades to existing ones, create the reports in pbi, oversee the power bi workspaces and environments, advise on where to find data and what events are going to be needed.

- Advise on the transfer of and maintain access to the legacy warehouse.

Output's 3-fold:

- I consult on what the data Execs are looking at means

- I create reports/dashboards for departments to use

- I create semantic/processed data marts/sets and models for people to use

The above all feels like its a few different jobs strung together being done by one man. 

My question is this: 

What'd be the different jobs I'm doing were they all done by a team? Alternatively, What would the title for the one man be? Preliminary research seems to suggest it's ""Data Analytics Engineer"" but I'm looking to second opinion that opinion.

I ask because I'm considering a move to someplace less stressful/more profitable where the title's ""Architect"" and want to know what kind of roles I've held here for reference.

Thank you all in advance!",False,0.75,https://www.reddit.com/r/dataengineering/comments/15d8ie2/what_am_i_doing_what_is_what_im_doing_called/,dataengineering
GCP cloud digital leader resources,15d9p3u,felipeHernandez19,1690683561.0,,False,True,Career,1,False,4,"Hi team, I will start working on the GCP cloud digital leader certification. Do you have some good practice resources regarding this ?

&#x200B;

I really appreciate any help you can provide. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15d9p3u/gcp_cloud_digital_leader_resources/,dataengineering
Does most of the SQL coding interview requires a one-take pass?,15d2nue,Old-Astronomer-471,1690664133.0,,False,True,Interview,7,False,9,"I am currently grinding the easy-medium difficulty sql problems, and notice I need 2-3 attempts to pass all test cases because of some minor errors.

I am wondering if the actual sql interview will expect an one-take pass from me, or will I have to write down the solution on a white board without any test cases?

Suggestions about how to become sql proficient just like doing 1+1?",False,0.91,https://www.reddit.com/r/dataengineering/comments/15d2nue/does_most_of_the_sql_coding_interview_requires_a/,dataengineering
I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube,15cnwi1,onurbaltaci,1690622900.0,,False,True,Blog,3,False,54,"Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!

[https://www.youtube.com/watch?v=aiHSMYvoqYE](https://www.youtube.com/watch?v=aiHSMYvoqYE)",False,0.92,https://www.reddit.com/r/dataengineering/comments/15cnwi1/i_recorded_a_crash_course_on_polars_library_of/,dataengineering
dbt Command Cheatsheet - join our LinkedIn dbt Developer Group for more content: https://www.linkedin.com/groups/12857345/,15cwagm,Datafluent,1690647697.0,,False,False,Discussion,0,False,8,,False,0.78,https://i.redd.it/8vv2c73ijxeb1.png,dataengineering
"Building a Python App to Fetch, Store, and Display Data - Need Advice for Deployment",15cyq11,interferemadly,1690654003.0,,False,True,Help,5,False,3,"I will consume an API using Python, specifically the YouTube API, to get data from channels. It won't be a massive amount of data, maybe around 20 channels. 

So, I want to store this data in a database, probably SQL. After storing this data, I'll need to perform a query to filter items with a specific string.

  
Then, I need to send the filtered data to a WordPress page where some data visualizations are present. I still need to familiarize myself with the implementation, but basically, I need to send the filtered data from the database to the WordPress front-end. 

What do you recommend for deploying this application? Considering that it's not a huge amount of data (neither in terms of data acquisition nor the site's expected traffic). 

I thought about using Heroku, but I'm also wondering if the free tier of AWS or GCP could handle it. Any advice? ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15cyq11/building_a_python_app_to_fetch_store_and_display/,dataengineering
Questions about data products,15cxlfb,cyyeh,1690651079.0,,False,True,Discussion,9,False,4,"1. How would you define data products?
2. If you are in a company, and you need to share data products with other departments or people outside the company, what would you care about?
3. What approach is good to you for sharing data products?
4. Do you think there is value for sharing data  products?
5. Applying the mindset of thinking backwards. What is the ideal world to you about data and what are missing?

Feel free to answer any question I came up with. Thank you!",False,0.75,https://www.reddit.com/r/dataengineering/comments/15cxlfb/questions_about_data_products/,dataengineering
AbInitio a yes or no?,15cod1t,byeproduct,1690624550.0,,False,True,Discussion,12,False,10,"At a recent meetup I saw guys punting AbInitio for all ETL and WH processes. Is this the way to go? I know it's got some nuances to the language, but it's got some simple looking connectors and components. Is it worth it? Connections I deal with are the normal SQL, XLS, Kafka, etc. I'm proficient in Python and SQL. Will it be tough to learn? And is it worth the effort?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15cod1t/abinitio_a_yes_or_no/,dataengineering
Experience with open source warehouse?,15cr30l,Public_Fart42069,1690633590.0,,False,True,Open Source,10,False,5,"Anyone use an open source warehouse for their solution? Something like clickhouse, Doris or Starrocks? Curious what your architecture is like? Do you load the data and raw and transform in the warehouse or transform in your storage and load final tables into the warehouse? Do you deploy in a container on a k8s cluster to manage compute? 

Was thinking about going the open source route and deploying in a docker container on a k8s cluster but wanted to get opinions?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15cr30l/experience_with_open_source_warehouse/,dataengineering
Dbt within enterprise data model,15d0lq0,Used_Ad_2628,1690658823.0,,False,True,Discussion,9,False,1,"I am needing to maintain an enterprise data model for the company and don’t want any table changes in prod unless it is approved by the architect. Since it is very easy to change DDLs with dbt, how would you do that? Make sure architect reviews all dbt code changes?  Plus I would like to start having ERDs through like sqldbm as our data documentation for users. Any recommendations?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15d0lq0/dbt_within_enterprise_data_model/,dataengineering
The intersection of DE and Backend Dev.,15cglfc,gxslash,1690597909.0,,False,True,Discussion,4,False,21,"I just started writing the backend service with Fiber (GoLang) in my job. Surely it is a backend development, but I wonder if it is also covered under the scope of data engineering?

How do backend developers and data engineers communicate generally?

And are there any parts that both do?",False,0.92,https://www.reddit.com/r/dataengineering/comments/15cglfc/the_intersection_of_de_and_backend_dev/,dataengineering
PandaPlyr - Pipe Syntax for Pandas,15cyzmw,HungryQuant,1690654694.0,,False,True,Open Source,0,False,1,"I've always wanted pipe syntax like R has in Python. Although you can chain pandas methods, there are a lot of cases where (to me) it feels clunky to write and read. There are a couple of existing packages, but I didn't feel like any of them really met my needs and preferences. 

If anyone wants to give it a shot and provide feedback, I'd like to improve it. There are built in datasets and examples in the README.md. 

Installation:
pip install PandaPlyr

Repo:
https://github.com/OlivierNDO/PandaPlyr",False,0.67,https://www.reddit.com/r/dataengineering/comments/15cyzmw/pandaplyr_pipe_syntax_for_pandas/,dataengineering
SCD Type 2 (Snowflake),15cyw1f,electronicentropy5,1690654439.0,,False,True,Help,15,False,1,"Greetings, 

I am creating this post to discuss slowly changing dimensions of type two and implementation plans, strategies in Snowflake. 

Example: 

Imagine one has a table that is being delivered nightly via Snowpipe (full-load) into the same table daily, and in the table there is an added column ETL-DATE which is of data type timestamp_ntz, and for this table one must implement a slowly changing dimension (type 2) to unlock historical analysis over time.

What are some strategies that are used for this set up? I have read a bit about streams and tasks for SCD, but placing a stream on this table only categorizes items as all update since this is a full load with new ETL-DATE everyday the table is fully loaded. 

Any ideas, thoughts, or different documentation guides that you could help me point me to in the correct direction?

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15cyw1f/scd_type_2_snowflake/,dataengineering
DE to cybersecurity and then back to DE?,15cix3x,Perfect_Kangaroo6233,1690605298.0,,False,True,Career,5,False,10,"Im a junior data engineer and due to budget cuts within my company it looks like I’m getting laid off. I’ve been with this company for 8 months. My only offer right now is for a cybersecurity job with decent pay for a pretty large company. Assuming that while working in this cybersecurity job, I maintain my knowledge of DE while also practicing leetcode, would it be possible to make the switch back to DE in a year or so? Any help would be appreciated!!",False,0.86,https://www.reddit.com/r/dataengineering/comments/15cix3x/de_to_cybersecurity_and_then_back_to_de/,dataengineering
Grab Reduces Traffic Cost for Kafka Consumers on AWS to Zero,15cnrhf,rgancarz,1690622386.0,,False,False,Blog,0,False,2,,False,1.0,https://www.infoq.com/news/2023/07/grab-apache-kafka-aws-cost/,dataengineering
Struggling to Roadmap My Data Engineering Processes,15cgjd1,gxslash,1690597743.0,,1690597990.0,True,Help,4,False,8,"I have just started my new job as a junior data engineer a month ago. In the beginning, I was re-designing a relational database, which includes more than +100. As I am at the very beginning of my career, it was nice to have experience with relational systems in detail, before deep diving into dimensional modeling, etc. for a data warehouse later on.

Unfortunately, I'm the only one that works with data in the company (It's a small startup). Because of that, I'm having trouble deciding how the whole system should work. I need to mine data from different sources in batches for scheduled periods. I'm thinking to write this operation using Airflow, and putting the raw data in S3 buckets as parquet files. After merging, cleansing, and quality-checking with Pandas, I'm thinking to insert the data into the database using the API I am writing now. For analytics, I'm planning to take snapshots of the database weekly and insert them into a warehouse (probably Snowflake) using DBT for transformations.

I have no idea if this is a good plan. What tools&frameworks do you recommend me to look at? What sources may help me to cover what's under the hood in batch processing and data mining? As a data engineer titled employee, what other things do I need to check for, or what else is considered in the data engineering scope?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15cgjd1/struggling_to_roadmap_my_data_engineering/,dataengineering
What's the coolest data you've worked with,15bs36m,srevolve,1690535024.0,,False,True,Discussion,105,False,106,I know we're all cool and blase and here for the money (except for us euro folks that are chronically underpayed) but i'm curious: what's the coolest data you've ever had to deal with in your projects? Maybe something bio related? space image data? massive network traffic data? ,False,0.98,https://www.reddit.com/r/dataengineering/comments/15bs36m/whats_the_coolest_data_youve_worked_with/,dataengineering
A List of Database Certifications Here,15bycwi,sequel-beagle,1690552984.0,,False,True,Discussion,7,False,41,"I see there are posts every week about database certifications and such.  I compiled a list late last year which I'm sharing below.  Please let me know of any that I have missed.  Or any dead links and wrong information for that matter.  Certifications are in the eye of the beholder; some employers value them, and others don't.

Here is a link if you want to bookmark my most recent list.

 [Database Certification List - Advanced SQL Puzzles](https://advancedsqlpuzzles.com/2022/11/18/database-certification-list/) 

**And below is just a copy and paste from the above link.**

Enjoy!!

\-------------------------------------------------------------------------------------------

[**Microsoft**](https://learn.microsoft.com/en-us/certifications/)

Microsoft has unfortunately sunsetted its SQL Developer focused certifications ([70-761](https://learn.microsoft.com/en-us/certifications/exams/70-761) and [70-762](https://learn.microsoft.com/en-us/certifications/exams/70-762)) and is focusing on role-based cloud certifications. 

* [DP-900: Microsoft Azure Data Fundamentals](https://learn.microsoft.com/en-us/certifications/exams/dp-900)
* [Exam DP-203: Data Engineering on Microsoft Azure](https://learn.microsoft.com/en-us/certifications/exams/dp-203)
* [Exam DP-300: Administering Microsoft Azure SQL Solutions](https://learn.microsoft.com/en-us/certifications/exams/dp-300)

\--------------------------------------------------------------------------------------------------------------------

[**Amazon**](https://aws.amazon.com/certification/)

Amazon offers the following database certification.

* [AWS Certified Database – Specialty exam (DBS-C01)](https://aws.amazon.com/certification/certified-database-specialty/)

\--------------------------------------------------------------------------------------------------------------------

[**Google**](https://cloud.google.com/certification)

And let’s not forget about Google and their cloud platform.

* [Professional Cloud Database Engineer](https://cloud.google.com/certification/cloud-database-engineer)
* [Professional Data Engineer](https://cloud.google.com/certification/data-engineer)

\--------------------------------------------------------------------------------------------------------------------

[**Oracle**](https://education.oracle.com/certification)

Oracle has numerous certifications ranging from high availability to administration to development. Here are a couple that I recommend. The 1Z0-149 is absurdly difficult, btw.

* [Oracle Database SQL Certified Associate Certification (1Z0-071)](https://education.oracle.com/oracle-database-sql-certified-associate/trackp_457)
* [Oracle Database PL/SQL Developer Certified Professional (1Z0-149)](https://education.oracle.com/oracle-database-pl-sql-developer-certified-professional/trackp_OCPPLSQL19C)

\--------------------------------------------------------------------------------------------------------------------

[**MySQL**](https://education.oracle.com/certification)

Oracle also offers MySQL certifications as it purchased Sun Microsystems in 2010.

MySQL is free and open-source software under the terms of the GNU General Public License, and is also available under a variety of proprietary licenses. MySQL was owned and sponsored by the Swedish company MySQL AB, which was bought by Sun Microsystems (now Oracle Corporation).

* [MySQL 8.0 Database Developer Oracle Certified Professional (1Z0-909)](https://education.oracle.com/mysql-80-database-developer-oracle-certified-professional/trackp_MYSQLPRG80OCP)

\--------------------------------------------------------------------------------------------------------------------

[**MariaDB**](https://education.oracle.com/certification)

MariaDB is a popular open-source relational database management system (RDBMS) that was initially developed as a fork of MySQL by the original developers of MySQL. It was created in response to concerns over Oracle’s acquisition of MySQL in 2010 and its potential impact on the open-source nature of the MySQL project.

MariaDB offers a database administrator exam, but not a developer exam.

* [MariaDB Certification Exam](https://mariadb.com/wp-content/uploads/2019/02/mariadb-certification-exam_datasheet_1005.pdf)

\--------------------------------------------------------------------------------------------------------------------

[**PostgreSQL**](https://www.enterprisedb.com/)

PostgreSQL is a free and open-source relational database management system emphasizing extensibility and SQL compliance. Because it is open-source, there are no vendor certifications, but there is a company called [EDB ](https://www.enterprisedb.com/)that offers solutions, training, and certifications for PostgreSQL. Their [certifications](https://www.enterprisedb.com/training/postgres-certification) appear to be focused on the DBA side.

* [PostgreSQL 12 Associate Certification](https://www.enterprisedb.com/course/postgresql-12-associate-certification)
* [PostgreSQL 12 Professional Certification](https://www.enterprisedb.com/training/postgres-certification)

\--------------------------------------------------------------------------------------------------------------------

[**IBM**](https://www.ibm.com/training/credentials/)

DB2 is a set of relational database products offered by IBM that traces its root all the way back to the 1970s. Currently IBM appears to be withdrawing many of its DB2 certifications and issuing new certification exams. The following appears to be the only DB2 exam currently offered, which is more DBA focused.

* [Exam C1000-122: Db2 12 for z/OS DBA Fundamentals](https://www.ibm.com/training/certification/C8003803)

\--------------------------------------------------------------------------------------------------------------------

[**Databricks**](https://www.databricks.com/learn/certification#certifications)

Databricks offers an associate and professional level data engineering certifications. These are great resources for understanding the product features.

* [Databricks Certified Data Engineer Associate](https://www.databricks.com/learn/certification/data-engineer-associate)
* [Databricks Certified Data Engineer Professional](https://www.databricks.com/learn/certification/data-engineer-professional)

\--------------------------------------------------------------------------------------------------------------------

[**Snowflake**](https://www.snowflake.com/certifications/)

Snowflake is a fully managed multi-cluster shared data architecture platform that capitalizes on the resources of the cloud. The SnowPro Core certification highlights the product features the best.

* [\[COF-C02\] SnowPro Core Certification](https://learn.snowflake.com/courses/course-v1:snowflake+CERT-SPC-GUIDE+B/about?_ga=2.109990149.1818508944.1668788648-1953636227.1655820550)

\--------------------------------------------------------------------------------------------------------------------

[**Teradata**](https://www.teradata.com/University/Certification)

Teradata (formed in 1979) provides cloud database and analytics-related software, products, and services.

* [Vantage Certified Associate Exam 2.3 (TDVAN1)](https://www.teradata.com/University/Certification/Vantage-Certifications/Associate-Exam-2-3)
* [Vantage Data Engineering Exam (TDVAN4)](https://www.teradata.com/University/Certification/Vantage-Certifications/Data-Engineering-Exam)

\--------------------------------------------------------------------------------------------------------------------

[**MongoDB**](https://university.mongodb.com/certification?_ga=2.155916475.143515463.1668790358-1726176162.1668790358)

MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas. MongoDB is developed by MongoDB Inc. and licensed under the Server Side Public License which is deemed non-free by several distributions.

It appears MongoDB offers certifications tailored to various languages like C#, Java, Python and Node.js.

* [MongoDB Associate Developer Exam](https://learn.mongodb.com/pages/mongodb-associate-developer-exam?_ga=2.155876411.143515463.1668790358-1726176162.1668790358)

\--------------------------------------------------------------------------------------------------------------------

[**SAP HANA**](https://training.sap.com/certification/)

SAP HANA (High-performance ANalytic Appliance) is a multi-model database that stores data in its memory instead of keeping it on a disk. There are a number of HANA certifications that you can choose from. The following appears to be the most SQL focused.

* [SAP Certified Development Associate – SAP HANA 2.0 SPS05](https://training.sap.com/certification/c_hanadev_17-sap-certified-development-associate---sap-hana-20-sps05-g/)

\--------------------------------------------------------------------------------------------------------------------

**Below are a few vendor neutral certifications that you may be interest in.**

\--------------------------------------------------------------------------------------------------------------------

[**CIW Database Design Specialist**](https://ciwcertified.com/ciw-certifications)

CIW has vendor neutral IT certifications focusing on web professionals, but it does offer a Database Design Specialist certification. This certification focuses on concepts such as the relational model, relational algebra, design, modeling, and the SQL language. The [study guide for the exam](https://www.amazon.com/Study-Guide-1D0-541-Specialist-Certification/dp/1941404073) is a great encapsulation of many database concepts that we should all know.

* [1D0-541: CIW Database Design Specialist](https://ciwcertified.com/ciw-certifications/web-development-series/database-design-specialist)

[**ICCP**](https://iccp.org/index.html)

The Institute for the Certification of Computing Professionals (ICCP) is a non-profit (501(c)(6)) institution for professional certification in the Computer engineering and Information technology industry. It was founded in 1973 by 8 professional computer societies to promote certification and professionalism in the industry, lower the cost of development and administration of certification for all of the societies and act as the central resource for job standards and performance criteria.

Here are a couple of their certifications that may be of intetest

* [Certified Data Professional (CDP)](https://iccp.org/certified-data-professional-cdp.html)
* [Certified Big Data Professional (CBDP)](https://iccp.org/certified-big-data-professional.html)

[**DAMA International**](https://cdmp.info/)

Certified Data Management Professional (CDMP) is a globally recognized Data Management Certification program run by DAMA International.

This exam is centered around [DMBOK ](https://www.dama.org/cpages/body-of-knowledge)and is geared more towards Data Management and Data Governance.

* [About CDMP – Certified Data Management Professionals](https://cdmp.info/about/)

\--------------------------------------------------------------------------------------------------------------------

**Below are a few educational websites that advertise certifications, but these (most probably) do not meet the stricter guidelines of the above certifications. Regardless, they may be a good option for students beginning their learning path.**

\--------------------------------------------------------------------------------------------------------------------

[**W3 Schools**](https://campus.w3schools.com/collections/course-catalog)

W3Schools is a freemium educational website for learning coding online. Initially released in 1998, it derives its name from the World Wide Web but is not affiliated with the W3 Consortium. W3Schools offers courses covering all aspects of web development.

* [Certified SQL Developer](https://campus.w3schools.com/collections/course-catalog/products/sql-course)

[**Datacamp**](https://www.datacamp.com/certification)

DataCamp is an online learning platform that helps students build data skills at their own pace.

* [Data Analyst Certification](https://www.datacamp.com/certification/data-analyst)
* [Data Scientist Certification](https://www.datacamp.com/certification/data-scientist)

You have reached the end! Happy coding!",False,0.99,https://www.reddit.com/r/dataengineering/comments/15bycwi/a_list_of_database_certifications_here/,dataengineering
Microsoft data fabric and working with AWS via shortcuts,15colib,detaurus,1690625352.0,,False,True,Help,1,False,1,"The company im working in is an AWS shop now and most of our data sits in S3. My boss is seriously considering Microsoft Data Fabric after meeting with sales team. 

Can anyone give an opinion on using Shortcuts in Microsoft Data Fabric to connect to S3 in order to realise a data mesh architecture? Will it rack up massive egress costs from s3?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15colib/microsoft_data_fabric_and_working_with_aws_via/,dataengineering
"First technical interview with another company, not sure what to expect. Advice?",15byxaz,wtfzambo,1690554274.0,,False,True,Interview,11,False,36,"Hello folks, here's the situation:

4 years ago I started as an intern in a small company, and then just leveled up there to senior DE.

Since I was an intern obviously there wasn't a technical interview, just a couple ""let's know each other"" talks with HR and the hiring manager.

Recently I interviewed with another company, another small one, which is looking for a senior DE to move forward their data endeavors (they don't have a dedicated data team yet).

The first interview was with their tech lead, who just today confirmed we're moving forward, and the next interview will be a technical one, with the tech lead + another SWE at their company.

I really have no idea WTF to expect. I am confident in my skills, but I also know I don't really perform well in an ""exam setting"", so I'm afraid my brain will freeze.

Any advice you have is more than welcome",False,0.9,https://www.reddit.com/r/dataengineering/comments/15byxaz/first_technical_interview_with_another_company/,dataengineering
Need your inputs on the mentioned scenario,15cjvjs,Delicious_Attempt_99,1690608485.0,,1690608772.0,True,Help,3,False,2,"I’m working on a side project. It is to just fetch the data from the API. There are 2 columns which changes its status. Like say - instore availability and online availability of the product.
My aim to track these two column to see how much time the product was available instore and how much time the product was online available.

For now, I have designed a pipeline, that fetches the data from the api and store in GCP cloud storage. The data is appended every time.
Next I’m planning to apply Slowly Changing Dimension - Type 2, to track both history data and new data. Is it the correct way to do that,? I’m using big query for this step.

Also to get new data, I would be polling the data every say 15 minutes using Airflow.

For the target, I would be polling big query and create a dashboard may be using streamlit - https://streamlit.io ( need your inputs here too, since I’m new to creating dashboards )

What are your thoughts on this?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15cjvjs/need_your_inputs_on_the_mentioned_scenario/,dataengineering
Linkurious like software but cheaper,15cjv36,justwaiyan,1690608435.0,,False,True,Help,5,False,0,"Is there any  Linkurious  like software but cheaper? cus  Linkurious  is 990$ per year so for learning purpose, it is kinda expensive... is there any software like  Linkurious  but at lower cost?",False,0.5,https://www.reddit.com/r/dataengineering/comments/15cjv36/linkurious_like_software_but_cheaper/,dataengineering
UI based tools disadvantage for career growth?,15c1g3u,aaloo_chaat,1690560054.0,,False,True,Career,9,False,10,"Apologies if this is a repeated/obvious question. My company uses UI based tools for building data pipelines. I am new to the team and to DE in general.

I'm worried that working with UI based tools will mean that I will be at a disadvantage skillwise. If I have never built a pipeline from scratch using Scala/Spark or performed performance optimizations through code changes, how will I advance in my career? 

Is low-code the future? I will do some MOOCs that teach DE fundamentals through code but I'm worried that won't be enough. What can I do to ensure employability?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15c1g3u/ui_based_tools_disadvantage_for_career_growth/,dataengineering
How to store and query 3 billion rows,15bvyf1,wackomama,1690546921.0,,False,True,Help,36,False,14,"Hello I’m working with some students and want to give them some guidance for a project. They’re analytics students. 

We want to analyze 3 billion rows of data that are currently stored as 2500 or so flat files. We want to do some machine learning and present visualizations on a website. 

I generally work in sql server and anaconda but never with data sets this large.

We can use cloud technology, etc., but need to keep costs to a minimum.

Question: how would you store the data? Azure? AWS? What tools would you use to query the data? How would you connect the DB to a machine learning tool and how would you display the results on the web?

Obv I can google though I just thought I would try here and see if anyone had quick recommendations. Thank you!",False,0.9,https://www.reddit.com/r/dataengineering/comments/15bvyf1/how_to_store_and_query_3_billion_rows/,dataengineering
How Snowflake makes the dbt Python models shine,15c3esa,fhoffa,1690564659.0,,False,False,Blog,0,False,4,,False,0.75,https://hoffa.medium.com/how-snowflake-makes-the-dbt-python-models-shine-a36d22960edb,dataengineering
Stream processing without JVM,15byd2c,romanzdk,1690552994.0,,False,True,Discussion,5,False,6,"What are the possible no-JVM tooling around stream processing? What I found:
- bytewax
- streamz
- prefect
- faust


Could you recommend what works for you?",False,0.8,https://www.reddit.com/r/dataengineering/comments/15byd2c/stream_processing_without_jvm/,dataengineering
Faux data load for unit testing,15c2e83,ExistentialFajitas,1690562262.0,,False,True,Discussion,1,False,3,"Back again with another unit testing post.

When writing tests for transformations, what is your method for having data to suit your test cases?

We have 74 tables being ingested in one way or another to support transformations for a singular data domain that have a slew of primary keys and foreign keys used for joins. The keys must match when present. EG table_id is always 1 when occurring in X table for X test scenario.

Current method is handwriting the data out to CSV files that are loaded through the dbt seed command. Kinda sucks.

I could write an algo to populate data throughout for this particular data domain, but the engineering cost seems high for being domain specific. Recommendations on solutions that are flexible for multiple domains? Preferably a writable solution. We’re not looking to add *another* tool to the stack, preferably.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15c2e83/faux_data_load_for_unit_testing/,dataengineering
Accelerating Database Backup and Restore with MinIO Jumbo,15ca9wz,swodtke,1690580976.0,,False,False,Blog,0,False,1,,False,1.0,https://blog.min.io/database-backup-restore-minio-jumbo/,dataengineering
How to be able to reuse code for both batch and stream processing,15bpsoy,jentlej,1690527034.0,,1690527232.0,True,Help,10,False,15,"Hello,

I am a big fan of this sub reddit as a lurker, and would like to dip my toes into the water.  You guys are rock stars.

&#x200B;

My question is how can I avoid rewriting code for batch and streaming pipelines?

Context:

I am designing an NLP application that will involve a pipeline that takes text, and transforms it into embeddings, and other engineered features.

I am looking for a way to be able to reuse code from the batch model training pipeline, for the inference pipeline when the model is running in production? What key principles should I consider, and are there potential landmines to avoid?

I am looking at using different frameworks and tools for their specific tasks rather than one massive platform. For example, using Airflow only as an orchestrator, due to it's many integrations with other tools that would be more suited for specialized jobs.

The idea is to use the executors are  a wrapper that calls other tools to avoid debugging multiple layers of the stack at once.

In production, we'll need to have an inference pipeline. However, Airflow of course is not meant to be an streaming pipeline, and from my research it appears that Airflow creates a lot of problems when it is not used the way it is intended to be used.

A solution I have looked at is windowed inference, with a message broker that will collect messages from the web app in a queue, then process and transform the windowed batches several times a second.

The individual messages can be identified with a unique pass through key so that we can scale horizontally and don't need to worry as much about ordering the arrays so that they always need to come back in the right order.

I am open to any suggestions

Other Conceptual Holes in my knowledge:

What is the role of a feature store in this?

What are your opinions on Seldon Core for serving ML Models in inference, specifically ensemble models that pipe output to each other?

&#x200B;

Thank you for your time reading this

&#x200B;

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/15bpsoy/how_to_be_able_to_reuse_code_for_both_batch_and/,dataengineering
Stored procedures as a data engineer,15c3mxk,Ok_Ticket6016,1690565175.0,,False,True,Career,7,False,3,"Hi , I need some help and advice on how stores procedures are being used as data engineer in migration project, from mysql to azure cloud. It says incremental loading.  I am new to this field and role and this is the job , I am reading about stored procedures but simply sql commands, but I am missing how exactly it is used and why? Anyone with such experience can please guide me ? So I can explain this in interview well.",False,0.71,https://www.reddit.com/r/dataengineering/comments/15c3mxk/stored_procedures_as_a_data_engineer/,dataengineering
Who has worked with both Snowflake and Databricks and what do you enjoy/dislike about each?,15b85up,MasterKluch,1690479473.0,,False,True,Discussion,83,False,114,"As someone who works in Snowflake day to day but has had little to no exposure to Databricks I'm curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?   
Yes, I know it's not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.   
This isn't meant to be a comparison of which you think is necessarily *BETTER*... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b85up/who_has_worked_with_both_snowflake_and_databricks/,dataengineering
Seeking guidance for data modeling scheme creation,15c66g0,Fragrant-Switch-9881,1690571181.0,,False,False,Help,3,False,1,"I'm an aspiring data engineer intern, and I've been presented with an exciting opportunity to work on a mini-side project for my team. My manager has entrusted me with creating a data model scheme, which will be crucial for our data engineering processes. However, I'm feeling a bit overwhelmed and could really use some help and guidance from the experienced members of this community.

Project Background:
As part of my internship, I'm trying to transition to the data engineering team, and this project is a fantastic chance for me to showcase my skills and prove my worth. The data model scheme I'm working on will be used to organize and structure our data, ensuring efficiency, reliability, and ease of access for various data engineering tasks.

The Ask:
I'm reaching out to all the experienced data engineers, architects, and anyone knowledgeable in data modeling to lend me your expertise and advice. If you have experience in designing data models or have insights into best practices, I'd greatly appreciate any feedback you can provide.

What I'm Hoping For:
Tips and best practices for data modeling and schema design. This is my first data model so roast me!!!!

Link to the reference: https://docs.uipath.com/insights/automation-cloud/latest/user-guide/real-time-data-export-data-model",False,1.0,https://i.redd.it/yint0x038reb1.jpg,dataengineering
How to choose the right ETL tool,15cboyc,nathan_data_engineer,1690584461.0,,False,True,Blog,0,False,0,"Hello hello! I just wrote this article on [how to choose the right ETL tool for your business](https://www.polytomic.com/blog-posts/how-to-choose-the-right-etl-tool-for-your-business).

Am I missing anything here?

Full disclosure: I work for the company where I made this blog post.",False,0.33,https://www.reddit.com/r/dataengineering/comments/15cboyc/how_to_choose_the_right_etl_tool/,dataengineering
What's the best solution to match different data sources into a single database?,15bxdth,Leru76,1690550621.0,,False,True,Help,8,False,2,"Hi guys, sorry in advance for my english first of all. I have a small company and we are using data warehousing program. Up till now, we take the flow of data of different warehouses, and we manually do the match with our database codification, to be able to utilize the data for statistic ecc.

Since this takes a crapload of time and resources, we start wondering if there is a way/a program to do this work. I make an example:

Let's take a single record (usually the montly flow of data from the warehouse is 10.000-20.000 records), the product description is ""Beautiful Water Still VAP 24 500"". In our database the product description is ""Water Beautiful Vap Still pz24 CL.50"" and the code is 000035. Our work is assign the code 000035 to the original description. Keep in mind that we have plenty of very very similar descriptions, that vary just in format or number of pieces (e.g. ""Water Beautiful Vap Still pz.24 CL.75""), and this make all our custom solution (excel formulas i.e. v.lookup index match ecc.) struggle, since most of times they match the wrong product.

So my question is: there are programs that can do a perfect match, even if the description is misleading, incomplete and there are many similar result in the core database? 

Sorry if this not the right place / the question is too stupid.",False,0.75,https://www.reddit.com/r/dataengineering/comments/15bxdth/whats_the_best_solution_to_match_different_data/,dataengineering
Help with data warehouse selection - on-prem Oracle vs AWS Redshift,15bu5ge,sportage0912,1690541800.0,,False,True,Help,16,False,4,"Hi all,

my organization is in the process of establishing an end to end BI process and we have a good chance to make things right as it is basically non-existent. There are two main options when it comes to picking a data warehouse and I need your comments on how to analyze the situation best and make the right call:

1. There is an existing on-prem data warehouse developed and maintained by an external vendor. It's Oracle based, and all data from operational system is deployed there. It has a very limited use, most data consumption is done directly from the data source (which is of course extremely inefficient). Limited use might suggest quality issues but I need to still delve into more analysis to understand better. 
2. Redshift (with data ingested through S3 with DBT) - this one is arranged by corporate in terms of   billing, contract, etc. but we need to take care of technical side (data modelling, integration, transformation, etc.). So we need to make sure we have the right resources in place.

Organization is pretty fixed on powerBI as bi tool so that would be non-negotiable.

Current data size is around 2-3 TB and expected to increase 50% in the next 5 years.

Option1 is easiest in short-term and the company already invested some money into it but I am not sure if it's the best long term and they are actually open to migrating. What facts do I need to get straight so that I make best informed decision?

Many thanks.",False,0.83,https://www.reddit.com/r/dataengineering/comments/15bu5ge/help_with_data_warehouse_selection_onprem_oracle/,dataengineering
Anyone using Activity Schema for data modelling?,15bs5vm,optimalbiscuit,1690535287.0,,False,True,Discussion,2,False,3,"Hey all. Our team are looking to add some more structure to our data warehouse, so have been looking at all the different modelling methodologies (data vault, activity schema, Kimball etc.). We've identified Activity Schema as a really interesting option, that works well for us since we're an online business that works with events mostly.

Anyone have experience working with Activity Schema, and can share what they thought of it? Equally interested in hearing anyone who tried it but didn't end up having a good time with it. 

Spec is here for anyone who hasn't heard of it: https://github.com/ActivitySchema/ActivitySchema/blob/main/2.0.md",False,0.72,https://www.reddit.com/r/dataengineering/comments/15bs5vm/anyone_using_activity_schema_for_data_modelling/,dataengineering
How to get free azure subscription(I do t have student id and by 1 month free trial credits are over),15bzh0d,Brunda_,1690555538.0,,False,True,Career,6,False,0,"Hello folks, 

Im preparing for azure dp -203 certification and I really want to practice services in azure. Due to some work issue I was not able to make use of $200 credits free trial, now it is over and asking for pay as u go subscription and here in this subscription it is charging amount for some of the resources, I hardly practiced anything. I need to practice different resources required for data engineering and do some projects which would help to get a job. I don't even have student id. May I please please know is there any other way we can access azure resources for free. 

I have already wasted most of my time is searching how can I acess free subscription to practice.
It would be really helpful if I get some tips here. 

Thank you.",False,0.5,https://www.reddit.com/r/dataengineering/comments/15bzh0d/how_to_get_free_azure_subscriptioni_do_t_have/,dataengineering
What is your favourite ETL/ELT tool?,15bec37,seayk,1690493889.0,,False,True,Discussion,48,False,23,"There are plenty of tools, some are good, some are nice to have and some are just a pain in the ass.
What is your favourite ETL/ELT tool?",False,0.9,https://www.reddit.com/r/dataengineering/comments/15bec37/what_is_your_favourite_etlelt_tool/,dataengineering
Seeking Reviews: Snowflake vs. Databricks - Building a Revolutionary Data Product!,15by27n,Prior-Apricot8771,1690552285.0,,False,True,Help,0,False,1,"Hey r/dataengineering community!

I hope everyone's having a fantastic day! 😊

I'm currently working on an exciting project that aims to revolutionize data management within organizations, and I could really use your valuable insights and experiences!

My team and I are evaluating two powerful data tools, Snowflake and Databricks, to build a product that enhances the lives of data organizations. We believe that your expertise and experiences in using these tools could be the key to unlocking the full potential of our project.

**Why Snowflake and Databricks?** Both Snowflake and Databricks have garnered immense popularity for their data management capabilities, and we're particularly interested in exploring how these tools handle various aspects like processing, scalability, and data orchestration. We want to ensure that the product we create addresses the unique challenges faced by data engineers, architects, and analysts on a daily basis.

**What We're Looking For:** We're keen to hear from professionals who have hands-on experience with Snowflake and/or Databricks. If you have used either or both of these platforms, we'd love to know about your experiences, your opinions on their pros and cons, and how they have positively or negatively impacted your data organization's workflow.

**Topics to Discuss:** We're especially interested in learning about:

* Performance and scalability of data processing jobs
* Handling of complex data workflows and data orchestration
* Ease of data integration and data sharing with other tools
* Quality of documentation and support from the respective platforms
* Cost considerations and value for money

Please feel free to share any other insights, tips, or challenges you've encountered while using Snowflake and Databricks. Your feedback will be immensely valuable in shaping the direction of our data product and making it truly transformative for data organizations.

**How You Can Help:** If you're willing to share your experiences, please drop a comment below or DM me directly. I'd love to set up a 30-minute chat at your convenience to discuss in more detail. Your input will be treated with the utmost respect and confidentiality.

Thank you so much for being an amazing community, and I'm looking forward to the insightful discussions ahead! 🚀",False,1.0,https://www.reddit.com/r/dataengineering/comments/15by27n/seeking_reviews_snowflake_vs_databricks_building/,dataengineering
Options for creating an end user interface for an API call,15bx2hn,kkchn001,1690549841.0,,False,True,Discussion,2,False,1,"I'm a data analyst slowly transitioning into data engineering. I have good knowledge of python modules related to data analysis.

Now, our vendor has provided APIs to request data. For example, let's say there is a sales API. The API format is such that I will have to enter date_1 and date_2 to get sales from date_1 to date_2.

www.dumnyurl.com/saleslines/date_1/date_2

I want to create a dashboard or interface where I can have the end user enter date_1 and date_2 and then a button to retrieve the data. 

My org. currently have powerbi but there doesn't seem to be this functionality in powerbi. I also read about using dash, but how would I be able to provide access to the dashboard to only people in my organisation and stop outside access. Can something low cost be done on Azure cloud for this?

I was just wondering if anyone has worked on something like this before or if there are any recommended solutions.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15bx2hn/options_for_creating_an_end_user_interface_for_an/,dataengineering
First steps with the Apache Kafka® Java client library,15bqsll,Marksfik,1690530482.0,,False,False,Blog,0,False,2,,False,1.0,https://aiven.io/developer/first-steps-kafka-java-client-library?utm_source=reddit&utm_medium=organic&utm_campaign=reddit_activation_Q3Q4_2023,dataengineering
How good Rust is for DE,15ba7ei,AdClean1116,1690484334.0,,False,True,Discussion,26,False,21,"Last time, all the time hear that rust is a great language. How good it is for DE and what as a DE I can do with Rust?",False,0.84,https://www.reddit.com/r/dataengineering/comments/15ba7ei/how_good_rust_is_for_de/,dataengineering
Who is using Delta Lake outside Databricks environments?,15bap0n,yfeltz,1690485477.0,,False,True,Discussion,32,False,17,"Hi, great data people.

The **direct question** I want to understand is: Who is using Delta Lake outside Databricks environments?

The **indirect question** is: what is the ""type"" of companies and use cases using Delta Lake and Iceberg?  


Using Delta Lake when your data platform sits on Databricks is a no-brainer. It works just wonderfully (I used it myself). Delta Lake OSS is also a very good piece of software for itself.

However, how is the usage distribution outside Databricks envs? 
I've heard Iceberg is more used for on-prem companies, mainly big tech. 
On ""non-databricks-cloud"", Iceberg has better support AFAIK. For example, for Snowflake and BigQuery.
On the other hand, Delta-rs integration with Arrow made Delta available to all tools supporting Arrow, which is a huge step towards being a standard outside the JVM, like for ML tools. While Icerbeg still lives in the JVM mostly.

So, how is the market using Delta Lake and Iceberg outside Databricks, in your opinion and/or experience?",False,0.91,https://www.reddit.com/r/dataengineering/comments/15bap0n/who_is_using_delta_lake_outside_databricks/,dataengineering
Learning curve for Hana + snowflake,15bnrwu,untalmau,1690520256.0,,False,True,Career,7,False,3,"last year I began in a new project,  hired by a consultancy firm, as a sr. data engineer (nearshore external consultant ).

This project was just big query and airflow, which made sense as I am a certified gcp pde. Then after 7 months the customer cancelled a lot of ext consultants after running short of budget, including me, but the consultancy told me to don't worry as they will be keeping paying me as usual while locating me in a new project with some other customer.

So, in that context, someone from HR arranged me into a call with a technical internal team to make sure my profile matches a specific requirement from other customer. These guys asked me if I had some experience working with Hana, which I said no, and also if I had experience working with snowflake, -again not-. 

So I expected after that, that I was not a match and for they to keep trying with other candidates, but it appears that they gave me green light because now they arranged an interview with the customer, so now I am concerned about being assigned something that's really out of my toolbox.

Of course I am open to learn new skills and assume challenge, but this sounds to me like starting from zero and I'm not sure it these skills make sense to learn, and also no idea about the learning curve I could expect.

What should I do? I really want to stay in this company, but I'm not sure if I am supposed to accept whatever project they ask me.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15bnrwu/learning_curve_for_hana_snowflake/,dataengineering
Is this AWS DMS task possible?,15bmwrt,No_Growth_9813,1690517426.0,,False,True,Help,1,False,3,"
I have a project where I need to migrate data from a mysql database to a postgres database. The tables are pretty much the same, except the target tables in postgres have an additional column with a uuid datatype. The default value for this column is an autogenerated uuid.

Is it possible to setup a DMS task that can handle automatically insert a uuid value for every record it migrates, or will that automatically happen since the target tables additional column is defined so that it will generate a uuid value on default?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15bmwrt/is_this_aws_dms_task_possible/,dataengineering
Transitioning from DS to DE,15bfdkd,Bira-of-louders,1690496405.0,,False,True,Career,3,False,6,"After 2 years of experience in data science, having done multiple ETL pipelines, prediction models and some dashboards here and there, I've decided to mainly focus on data engineering. However, I'm currently applying for data engineering jobs in my country and not getting responses for my applications.

I've decided to study and brush up my Airflow and Apache Spark skills as well as study to get the azure dp-203 cert. Could someone give me additional tips or maybe even insights into why I'm not hearing back from those applications?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15bfdkd/transitioning_from_ds_to_de/,dataengineering
The data engineer came to me... tears in his eyes,15ae6kp,shed_antlers,1690397313.0,,False,False,Meme,67,False,800,"Turns out databases are ""relational"" or something",False,0.99,https://i.redd.it/1wm37l33vceb1.png,dataengineering
Help on Python (pandas) script for data identification logic. Working to extract and transform data from PDF’s into Excel. (Long post),15b68s8,dkampien,1690474956.0,,False,True,Help,6,False,7,"**DISCLAIMER**

I am no data engineer and I don't know python or any programming language. I can put together stuff into VScode but thats about the extent of my programming abilities. 

&#x200B;

**INTRO & CONTEXT**

I’m working on a personal project that takes a PDF containing medical lab results, extracts the data and processes the data into a new centralized schema. From that schema, data is imported into an Excel or Sheets table, which is used as a data source for a no-code website builder. (like Noloco)

I designed the centralized database schema both as a JSON and as an Excel.

I’ve managed to extract some data successfully using AWS Textract GUI console demo. The outputs that AWS gives are JSONs and CSVs.

I now need to transform the data from the AWS output, to match the centralized schema so I can map the data. And here is where im stuck..

**PROBLEM**

My table has a hierarchical layout with nested rows under one column. I need to transform the data, split the first column into 2 columns, do some parsing etc.

But in order to transform the data I need to programmatically identify what data needs to be manipulated and somehow mark it. Theoretically, my table has 2 types of entries / records. Type 1 entries are singleTests and type 2 entries are collectionTests.

[The table output sample and entries types.](https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&format=pjpg&auto=webp&s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2)

I need to think of a parsing logic or a logic to identify the data.

* I first thought about **identification by keywords** and indeed there is an “LC” at the beginning of each record. That LC stands for Central Lab and its the location where the tests have been processed. The problem with this that LC appears irregularly and its not reliable.
* The second method I thought was **identification by pattern**. For a singleTest there are always 2 rows, the first one having empty values (besides the first column). For a collectionTest, the first 2 rows are always empty (besides the first column). A collectionTest has a variable number of sub-tests.

Here is a draft of the centralized schema database:

https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&format=png&auto=webp&s=c06740e81178ea509d56aefa1849db26e50796cd

In this first iteration, the original TestName values that were spanning multiple rows have been split into CollectionName and CollectionMethod according to the entry type.

&#x200B;

**NEED**

I need a parsing logic implemented in python (pandas) that can identify entries types and mark them accordingly. Im not sure if a script can do this. Suggest alternatives if you know.

&#x200B;

**PROGRESS**

I’ve tried about 25 iterations of threads with GPT-4 and Code Interpreter. The problem here is I don’t know exactly what to ask since I don’t know data science specific terms and what technique or collection of techniques should be used. Even so, I’ve managed to get very close to marking the entries correctly with Code Interpreter but it seems that, even though it understands the logic, It can’t write a working script.

&#x200B;

**RESOURCES**

* A Notion with my progress on the data identification logic with my prompts [LINK](https://dennis-kampien.notion.site/Parsing-logic-c70e92b49a8b433398a85d49a0d06343?pvs=4)
* Links to the sheets containing the tables [LINK](https://docs.google.com/spreadsheets/d/1U7bbTX7qPVFb9FeSGzxxWZyDVk1q48lx/edit?usp=sharing&ouid=107105976591677253200&rtpof=true&sd=true)
* A link with a partial successful GPT thread [LINK](https://chat.openai.com/share/fbac4366-5b57-4dc4-82d0-f1cf946fa58f)

&#x200B;

 If you have ideas where else I can get help, would be appreciated. Thank you.  ",False,0.83,https://www.reddit.com/r/dataengineering/comments/15b68s8/help_on_python_pandas_script_for_data/,dataengineering
Processing 160m of records (with 4 JOINs and 1 aggregation) in real-time,15axba2,matteopelati76,1690450727.0,,False,True,Blog,13,False,23,"Hi r/dataengineering,

We recently ran an experiment at [Dozer](https://github.com/getdozer/dozer) that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:

[https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce](https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce)

Would love to get your feedback.  


Thanks  
Matteo",False,0.83,https://www.reddit.com/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/,dataengineering
Is there something wrong by applying in my country?,15bkwnl,DataSenpai,1690511320.0,,False,True,Help,0,False,1,"First of all, I am from Mexico with 9 years in the data world. SQL, Python, Pyspark, Pandas, (trying polars), Airflow, Autosys, Linux, EC2, EKS, Git, Graphana and other stuff that is not related to modern Data Engineering (SSIS, C#, etc)

&#x200B;

I was in a US based company working for 6 years and got tons of experience but my salary was very low, so I tried to do some freelancing and then I found a job as Data Engineer. When I entered, they told me, lol just joking, you have to do some reports in Excel.I only stood there 3 weeks until another company where I had a process ongoing called me to offer me also a Data Engineer position and more salary and I said, sure.

&#x200B;

It's been 1 year since I am in that company and again, the only thing for data is that I use SQL but I do not build pipelines, I do not do analysis or even automate process. Just manually insert data (around 1 record per week) and answer some tech support tickets. Of course I have been trying to find another job related to Data Engineer but the thing is the salary is way worse than before. I got hired during the pandemic and nowadays they are offering like half of what I am currently making and they do not believe my current salary.

The interviews had been a mess, either they ghost me, they tell me that they can only offer half of what I am asking or they expect me to do some insane stuff and either they just ghost me again, or at the end they offer me even less money. One example, for a Data Engineer position, they requested to build a API using docker that can insert and query data, they told me that I have only 2 hours to do it and they never said me to download stuff in my personal computer to do it. I was able to make it using FastAPI and they were really surprised that I was able to do it in such a short time, but then they started to lowball me, like 30% less of what I accepted on the first negotiation.

I see a lot of success stories here but it seems it only applies for US or some european countries but here is hard. I am thinking of applying directly to US companies but I haven't found a way to do it, I tried LinkedIn but also no luck there. Is there any place for folks like us that don't live in countries with good jobs for Data Engineering?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15bkwnl/is_there_something_wrong_by_applying_in_my_country/,dataengineering
Does Python Polars completely release the GIL ?,15b1lnf,nightwolfomar,1690463652.0,,False,True,Discussion,13,False,12,"We got a FastAPI server at work that that's used for calculating machine learning features. It receives a big json blob, calculates features and responds with another big json blob. We run these complex CPU operations in a process pool so that we don't block the main event loop. These operations are mostly Pandas operations that can be written in Polars nowadays. So what I'm thinking is that if Polars completely released the GIL we would be able to use a thread pool inside this service rather than a process pool, incurring in less overhead and resource consumption. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b1lnf/does_python_polars_completely_release_the_gil/,dataengineering
Resources for learning how to do DB design with AI-centric tagging/annotating as a priority,15banxs,TylerP3358,1690485405.0,,False,True,Help,1,False,2,"Hey guys,

I am going to be designing a DB soon for a medium sized business and other than the foundational tenets of DB design, I was hoping for book suggestions/courses on curating good training data.  What are some pitfalls that I need to know about, how should I be tagging data, what data is important to tag, etc.  I have a good grasp on the business needs, but I am nervous about missing the low hanging fruit while in the early stages of design.

&#x200B;

Thanks for the suggestions!",False,0.76,https://www.reddit.com/r/dataengineering/comments/15banxs/resources_for_learning_how_to_do_db_design_with/,dataengineering
Micro-batch architecture,15b1l0m,romanzdk,1690463603.0,,False,True,Discussion,6,False,5,"So we obviously have classic batch pipelines and now evaluating whether we really need/want realtime pipelines. The problem we have with the realtime pipelines is there is no idempotency, if something fails, no retries etc. What we are thinking is to utilize something like micro-batch architecture where we would run jobs e.g. every minute.. Or every x seconds. Until the pipeline triggers, messages would buffer in some message queue (Kafka?). Then e.g. Airflow would spawn a job that would take all messages from T-1min and process them. 

What do you think of such approach? How common is it? How good/bad idea is it? Do you use it as well? Are you aware of some do's/dont’s? Some references?",False,0.86,https://www.reddit.com/r/dataengineering/comments/15b1l0m/microbatch_architecture/,dataengineering
Building a vs code extension to help write dbt docs for you,15b5uxb,StartCompaniesNotWar,1690474026.0,,False,True,Blog,0,False,3,"Hi Reddit 👋

Data documentation is still too messy. Editing YAML files is tedious and oftentimes when teams buy data cataloging tools they are faced with sparse and out of date documentation.

I'm excited to show an early preview of a my new vs code extension - Docs Composer ✨ for dbt core.

  
Read & edit your dbt model docs side-by-side as you develop and leverage our AI assistant to help write a first draft for you.  


It's free and takes 30 seconds to get started. Reach out if you'd like to try it out!

&#x200B;

https://reddit.com/link/15b5uxb/video/hjboe9u17jeb1/player",False,0.81,https://www.reddit.com/r/dataengineering/comments/15b5uxb/building_a_vs_code_extension_to_help_write_dbt/,dataengineering
Software and learning material to improve my data profiling -> data cleansing work?,15b1i8y,funkyman50,1690463403.0,,False,True,Help,3,False,5,"3.5 years as a data analyst. Learning SSIS ETL to try to become my company's data engineer. So far, for data profiling I have just been using custom SQL queries to find basic information about the columns (length, count of blank cells, precision of numerics, etc), but it's very manual work. 

Is there a good piece of software that can answer these basic questions for me? Are there tutorials out there that would teach more efficient methods?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b1i8y/software_and_learning_material_to_improve_my_data/,dataengineering
Seeking the Best SQL Course to Kickstart My Career with Zero Experience! With some CapStone Project,15bel6r,priyasweety1,1690494495.0,,False,True,Discussion,1,False,0,"would you recommend THE BEST SQL course that could help me get a job even if I have no prior SQL experience? At the end of project I would like to complete some real world projects as well..  I really want to be an SQL expert and work at domain level expertise/ projects in Retail, Telecom, and other domains.  

 ",False,0.33,https://www.reddit.com/r/dataengineering/comments/15bel6r/seeking_the_best_sql_course_to_kickstart_my/,dataengineering
How to Optimize Data Transformations in a Buy Now Pay Later Fintech Startup,15b2gh2,IllRevolution7113,1690465916.0,,False,True,Help,2,False,4,"Hello everyone,

I work as the main data person for a fintech startup running a Buy Now Pay Later scheme. My tasks involve business intelligence and data engineering, where my final output is generally dashboards on Power BI for different teams. As we are planning to scale up, I am tasked with ensuring our data tasks follow good data engineering practices.

While I'm experienced with Power BI, Python, SQL, and Python, I'm not a seasoned data engineer. Hence, I am seeking advice on how to optimize our data engineering further.

Our current setup uses AWS RDS and DynamoDB as primary data sources. I create MySQL views for AWS RDS to transform tables, add auxiliary columns, and then connect to Power BI with a Mysql connector. For DynamoDB, I use AWS Glue to catalog data into S3, clean up necessary parts with Athena views, and connect to Power Bi with an Amazon Athena connector.

The tables in AWS RDS are transactions, users, and payment links with thousands of rows and the table of the DynamoDB is the biggest one with millions of rows.

We also use Google Sheets for additional data like exchange rates (converted manually once a month), and some user information provided by our sales team. I have considered automating the exchange rates by using a Python script with AWS Lambda, and perhaps storing them in RDS or somewhere better than Google Sheets. All these tables have just hundreds of rows.

Transformations are mostly done in MySQL views and some in DAX.

My question is, should I consider incorporating data engineering tools and concepts like pyspark, scala, airflow, dbt, git, etc., that I have been learning but not yet using? Any advice or recommendations will be appreciated.

Thank you!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b2gh2/how_to_optimize_data_transformations_in_a_buy_now/,dataengineering
Trust the Data: A Guide to Building Data Trust in Your Organization,15b0869,David_starc150,1690459849.0,,False,True,Blog,1,False,3,"Hey Reddit community!  
  
I came across this insightful blog post on SG Analytics about ""Data Trust: How to Build Data Trust."" I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.  
  
**URL:** [***Data Trust: How to Build Data Trust***](https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/)  
  
In this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.  
  
I found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.  
  
Let's dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!",False,0.64,https://www.reddit.com/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/,dataengineering
Column to column lineage at transform,15bd12s,chcahx,1690490853.0,,False,True,Discussion,0,False,1,SQL Server shop here moving to Databricks with Unity Catalog. We've started to curate data in Databricks and we are getting column to column level lineage captured at transform. I'm expecting this to be game changimg for us. Anyone else getting many to many column level linage like this with other stacks?,False,1.0,https://www.reddit.com/r/dataengineering/comments/15bd12s/column_to_column_lineage_at_transform/,dataengineering
Databricks Photon engine question,15b159i,Gators1992,1690462447.0,,False,True,Help,15,False,4,"Hi,

We are doing a POC for DB right now and it's going pretty well.  One thing though is we don't have access to the photon engine for querying, only as the executor for Spark SQL.  It's been performant compared to what we have now, but I guess I get paranoid when someone tells me I can't see something they want me to buy.  

My concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?

Appreciate any feedback.",False,0.84,https://www.reddit.com/r/dataengineering/comments/15b159i/databricks_photon_engine_question/,dataengineering
anyone try out astro's cosmos/dbt integration?,15b2b7w,zazzersmel,1690465537.0,,False,True,Discussion,0,False,3,"For reference, i manage my org's airflow instance. all on prem, running on a linux server via docker containers. pretty basic. i've been wanting to push us gently into dbt, at least for some jobs where it makes sense, and i feel like this is the impetus i've needed.

so... any thoughts? any gotchas, especially with dbt core and self managed open source airflow? it looks relatively simple, install in a separate venv on your airflow instance. until i came across this, my plan was to just use dbt in a container and run via docker operator.",False,0.81,https://www.reddit.com/r/dataengineering/comments/15b2b7w/anyone_try_out_astros_cosmosdbt_integration/,dataengineering
Need help with Stream-Stream joins with spark structured streaming,15b9c6d,chilllman,1690482265.0,,False,True,Help,0,False,1,"Hello! I have 2 streams coming from event hub and I need to implement stream-stream join and perform an upsert operation into a delta table using spark structured streaming. The issue is that structured streaming doesn't allow joining operations in update mode while writing the stream to the table. Just wanted to know if someone has implemented stream-stream join with upsert and if so, how?

One possible solution that comes to mind is to persist the two streams in two temporary tables (which will be static) and then perform the join and upsert operation but I don't want to use this solution as this won't be a continous job and the job will have to be scheduled with some frequency (x minutes recurrence).

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b9c6d/need_help_with_streamstream_joins_with_spark/,dataengineering
Data Engineering with Airbyte and Teradata Vantage,15b6g3a,kittu211,1690475444.0,,False,True,Blog,0,False,1,"Learn how you can seamlessly load your data from a source system like google sheets to Teradata Vantage using Airbyte. I wrote a comprehensive step by step guide covering the ELT process and complete configuration steps to extract and load the data with Airbyte. 

[https://medium.com/teradata/using-airbyte-to-load-data-from-hubspot-crm-to-teradata-vantage-9a25f41e2355](https://medium.com/teradata/using-airbyte-to-load-data-from-hubspot-crm-to-teradata-vantage-9a25f41e2355)  


Please let me know if you find this blog useful and you learned something new. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b6g3a/data_engineering_with_airbyte_and_teradata_vantage/,dataengineering
Which are good tools and references to learn design and architecting data engineering solutions?,15ay13l,rockeyjam,1690453100.0,,False,True,Discussion,3,False,3,"I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/,dataengineering
Viability / Value of Web Scrapers,15b5r40,Live-Cartoonist6247,1690473782.0,,False,True,Discussion,0,False,1," 

Hey All,

In the past year, I've been working on a project that is heavily focused on web scraping. It started out as a small project to automate some business workflows and web searches. However, it has since been positioned as something that is 'key' to the company's success with multiple other pieces of software, web apps, and APIs built around the data that the web scrapers collect.

I have also noticed a rise in this sort of work being advertised in job descriptions. This is interesting since web scraping at its core seems difficult to rely on and scale. The feeling of developing a web scraper for me is almost like going to goodwill or another thrift store in search of discarded treasure haha.

I'm curious if anyone has seen web scraping deliver much value to a project/company? How much value-add is really possible for scrapers?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b5r40/viability_value_of_web_scrapers/,dataengineering
Microsoft Fabric - have you used it?,15am3g2,Culpgrant21,1690415941.0,,False,True,Discussion,24,False,18,"Has anyone used Microsoft Fabric yet? If your a Microsoft shop are you planning on adopting it? 

Not coming from any angle I just haven’t talked with anyone who has been using it a lot.",False,0.88,https://www.reddit.com/r/dataengineering/comments/15am3g2/microsoft_fabric_have_you_used_it/,dataengineering
Do you use OOP in your daily tasks?,15b3031,LeftHelicopter5297,1690467241.0,,False,True,Discussion,9,False,1,"Is OOP often used in your DE work?

[View Poll](https://www.reddit.com/poll/15b3031)",False,0.67,https://www.reddit.com/r/dataengineering/comments/15b3031/do_you_use_oop_in_your_daily_tasks/,dataengineering
dbt Spark vs dbt Trino,15av4jc,chuqbach,1690443143.0,,False,True,Open Source,1,False,3,"dbt on Spark vs dbt on Trino

Hi, we're running a on-prem data platform solution with stacks of **Iceberg** (parquet), **Hive**, **Spark**, **MINIO**, etc. and planning to use **dbt** for our **SQL transformation** of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch

* Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? 
   * I don't have much experience with Trino, so I'm not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I've seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it's true.
* For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? 
   * Spark Thrift: Spark Thrift Server seems to have many issues mentioned here ([https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi\_vs\_thriftserver.html#kyuubi-vs-spark-thrift-server](https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server)), which make us think Apache Kyuubi would be a good choice. However, we couldn't find any document/post related to running dbt on Spark Apache Kyuubi
   * dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn't have much update recently, so I'm afraid we will go into another dead end with this approach

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/,dataengineering
Assistance needed - method for simple automated data ingestion/analyses,15b0t80,Pelecabra,1690461498.0,,False,True,Help,0,False,1,"In my new position, I am putting together robotic workflows to pump out experimental data on multiple local hard drives in my lab. Unfortunately, this new lab had not previously put much effort into standardizing their sample and experimental data structures and are manually moving output files and copypasting them together in spreadsheets to analyze them. Additionally, i am the only one with the experience needed to standardize the data structures, write scripts, and come up with a strategy to replace the manual file transfers, copypasting and spreadsheets.  Together with my actual position as automation engineer, its a lot of work for one person. 

In a past job, I was on a team of data scientists/engineers that set up a stack with a dedicated network PC monitoring all the lab hard drives for new files to trigger file transfers to Google Drive (or Cloud Storage, not sure), development/testing in Saturn Cloud, pipelines stored in GitHub and set up to trigger Google Cloudbuilds on commits of new .yaml files, with data being stored in Google Drive (may have been Google Cloud Storage).

Theres no way i can manage to set all that stuff up myself and also do the job i was hired for. I also doubt that at this time, my current company needs something that sophistocated as we are doing basic and predictable data transformations, merging, and analyses of relatively small data sets. 

It seems like if i set up local network monitoring to send data to Google Drive/Cloud Storage, and develop/test scripts in Google Colab for Google Cloud Functions to trigger when the new data lands in Drive/Storage, that might be a relatively simple route to ultimately cure my lab of manual file transfers, transformations, and analyses. 

I'd love any input regarding limitations of this general approach, whether its completely off base and im missing some glaring detail, or if anyone has an alternative suggestion for a simple way to acheive this goal. For the record, i am proficient in R and Python. Thanks in advance!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15b0t80/assistance_needed_method_for_simple_automated/,dataengineering
How A Database Get Rid of OOM Crashes,15b03up,ApacheDoris,1690459507.0,,False,False,Blog,0,False,1,,False,0.6,https://medium.com/p/d3e91fb39ea5,dataengineering
A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node.,15azrwj,AnthonyofBoston,1690458554.0,,False,False,Blog,0,False,1,,False,0.6,https://www.academia.edu/42243993/A_hypothesis_that_the_Federal_Reserve_can_set_interest_rates_based_on_the_movements_of_the_planet_Mars_Here_I_have_data_going_back_to_1896_that_shows_how_the_Dow_Jones_performed_when_Mars_was_within_30_degrees_of_the_lunar_node_from_appendix_of_Ares_Le_Mandat_4th_ed_,dataengineering
Multi-Container Docker Application for ETL Task,15arvak,EarthEmbarrassed4301,1690432327.0,,False,True,Help,1,False,3,"Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.

Currently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -> performing transformations -> load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the ""origin"" for specific IDs. 

I'm treating it like a standard application architecture, with the ETL process acting as the ""front end,"" and the database holding the persisted data serving as the ""back end."" The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. 

 Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task ""front end"" container needs to execute on the PostgreSQL database backend, given that the database doesn't exist until after the application is up and running. I can't just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can't seem to wrap my head around this one. 

Hope this makes sense, I can clarify and makes edits if needed. Thanks! ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/,dataengineering
Dimensional model for a library,15axj5g,Top-Preparation-4231,1690451468.0,,False,True,Help,0,False,1,"Hey guys, I'm new to data so don judge my dumb question, anyway I'm supposed to build a simple dimensional model for a library, I already made the data model, I'm using book loans as the fact table, and members, library staff, reservations and books as the dimensions, any advise  to better this because it didn't impress my boss

guys, please help",False,1.0,https://www.reddit.com/r/dataengineering/comments/15axj5g/dimensional_model_for_a_library/,dataengineering
Concurrent API with bulk hit,15atkim,rishabhdev_rd,1690437849.0,,False,True,Help,1,False,2,Hi. i want to build a concurrent api that can be used for bulk hitting. I just want to know what are some industry standard practices that I can put in the process. I have previously used flask with asynchronous process but just curious about how the process get performed in the industry.,False,1.0,https://www.reddit.com/r/dataengineering/comments/15atkim/concurrent_api_with_bulk_hit/,dataengineering
"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)",15ar04s,AMDataLake,1690429699.0,,False,False,Blog,0,False,3,,False,1.0,https://youtu.be/Q3g8NFhvddk,dataengineering
Cloud Security Consultant or Cloud Data Engineer?,15awncy,YuriHaThicc,1690448460.0,,False,True,Career,2,False,1,"I want to work within the cloud space but it's tough to decide which route to take.
This is at one of the big 4, I am ok with either or but I am scared in cloud security consulting role I won't be as techinal as the data engineering role in cloud.

If any data engineer has worked in cloud,could you tell me about the wlb,salary,and remote opportunities?",False,0.67,https://www.reddit.com/r/dataengineering/comments/15awncy/cloud_security_consultant_or_cloud_data_engineer/,dataengineering
Great Expectations is bloaty. What are the alternatives?,15a45gt,Majestic-Weakness239,1690373573.0,,False,True,Discussion,39,False,46,I set up GE on databricks but I am not happy with it. To the point where I am considering to just keep the very plain selfmade validation modules. Are there other options worth exploring? What I particulary dislike is the creation of custom expectations and all this data context bloat.,False,1.0,https://www.reddit.com/r/dataengineering/comments/15a45gt/great_expectations_is_bloaty_what_are_the/,dataengineering
How would you learn DE if you had to start over ?,15adu9z,Pillstyr,1690396515.0,,False,True,Career,18,False,13,"If  you were to start over in the Data Engineering or would have to mentor someone, how would you do.   
Taking into account all these new tools in today's tech stack.  
I've researched all this, but TBH I don't have idea about 90% of these:  
  

**· ETL and Scheduling or Orchestration or Jobs =**

1. Open source Tools (Airflow(most used), Dagster, Argo, Prefect, Luigi)

2. Traditional UI Tool (SSIS, Informatica, Talend, FiveTran)

3. Cloud Tool (Azure Data Factory, Google Dataflow, AWS Glue)

4. Modern Proprietary Tool (Databricks, Trifacta)

5. Fivetran (just usually mostly used but a little old school) | MWAA (cost effective) | Apache Beam

**· Data Warehouse =** Snowflake or BigQuery | For Cloud = Amazon S3

**· Data Lakes =** DataBricks, Redshift

**· Data Transformation or Data Quality Control testing or Parallel Processing Tools =** dbt, Pandas, Apache Spark, 

**· BI =** Power BI

**· Exploratory Data Analysis =** Snowflake, Jupyter, Alteryx, Snowflake Snowsight

**· Scaling Workers =** aws lambdas, kubernetes

**· Devops Methods =** Observability, alerting, incident management, ci/cd and good pr hygiene apply as much to DE as regular backend SE

**· Stream Processing =** Apache Kafka,

**· AWS Services =** RDS (to setup and scale db in cloud)",False,0.84,https://www.reddit.com/r/dataengineering/comments/15adu9z/how_would_you_learn_de_if_you_had_to_start_over/,dataengineering
Am I being naive?,15ad17i,Marble_Kween,1690394712.0,,False,True,Help,15,False,13,"I’m a cs software manager at a public company. I’m not currently in a data engineering role, but I want to make a pivot into DE. I’ve had a DE ticket open for several months for them just to scope and there’s no end in sight. It could be several more months before they can even think about looking at our ticket. Suffice it to say our DE team is under resourced. 

This is where I feel that maybe I’m being naive: I feel like the request is simple. We want to create a daily job for some API reports and bring it into our data warehouse. 

How many hoops does one need to jump through in a mid-level organization to write a script then push it to the warehouse? 

What level of difficulty is this normally? 

Would it be rude to write the script and hand it over to the DE team?

Looking for advice to navigate the situation. Thanks!",False,0.94,https://www.reddit.com/r/dataengineering/comments/15ad17i/am_i_being_naive/,dataengineering
Streaming vs Batch modeling discussion,15ak8si,harrytrumanprimate,1690411292.0,,False,True,Discussion,4,False,3,"My org uses kafka to ingest data from source teams into snowflake, but then uses dbt + airflow to do nearly every model that happens downstream of raw data. I had a recent conversation with a coworker about how it's kindof a shame that we have real-time data, but don't actually serve it on our data platform except for once or a few times per day. 

Is it generally recommended to push everything to stream processing (if it can be)? 

For example, suppose you're a retail company and a typical use case could be serving data for downstream about the price of products. You might maintain: 

* the latest price of the product in a dimensional table - `dim_product_current`
* the historical prices of the product in a type 2 scd history table - `dim_product_history`
* a daily snapshot of the prices for all of your products on a given day

Of the three, the current use-case seems most appropriate (and easiest to implement) for real-time. Does it even make sense to enable maintaining scd type 2 for streaming data?

Curious how other teams/orgs approach these problems",False,0.81,https://www.reddit.com/r/dataengineering/comments/15ak8si/streaming_vs_batch_modeling_discussion/,dataengineering
Help with internship,15atc0d,EngineeringHour484,1690437083.0,,False,True,Help,0,False,0," I'm currently interning at a tech company, and I've been assigned an AI project that involves helping startups improve their performance in various areas. I would love to get your valuable insights and suggestions on how to approach this challenge.  my task is to create an AI model that analyzes a startup's responses to different questions, focusing on multiple axes or dimensions.   Based on the startup's answers, the AI model needs to recommend specific sessions that can boost their scores in these axes. Im currently struggling in the data collection step, I wanted to know what the dataset structure should look like in order to get good results before I start generating fake data from chat gpt. any advice would be helpfull at this point specially that I'm not getting neither a dataset from the company neither a roadmap for the project, 

thank you \^\^",False,0.5,https://www.reddit.com/r/dataengineering/comments/15atc0d/help_with_internship/,dataengineering
Hive external table on Parquet files issue,15at8lp,PR0K1NG,1690436762.0,,False,True,Help,0,False,1,"I'm using Hive external table to read parquet files with 100 columns. Some of my files don't have a column (let's say its position no is 3), and this column is there in the Hive table schema. When I read the table from Impala, I see that my data in the parquet files that don't have the column gets shifted to the left instead of just adding null in the not present column.

I'm using case-sensitive column name parquet files, which match the hive table column names. 

I've not seen this issue happen on Glue Catalog.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15at8lp/hive_external_table_on_parquet_files_issue/,dataengineering
"List of ETL tools, short description, and their primary purpose",15aeh6s,jbrune,1690398006.0,,1690493106.0,True,Discussion,10,False,6,"I don't know if anyone else was confused about the multitude of ETL tools out there.  With the help of ChatGPT I came up with this table.  It really helped turn a light bulb on for me.  It would be great if there could be a public list like this that we could keep updated.

Edit: updated table

&#x200B;

|Product Name|Description|Specialization|
|:-|:-|:-|
|Airbyte|Open source ELT tool. Full table and incremental via change data capture. Integrate deeply with Kubernetes, Airflow, Dagster, Prefect, and dbt. |Extract, Load, Transform|
|Apache Airflow|Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It is commonly used for orchestrating data pipelines.|Transform, Load|
|Apache Kafka|Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications.|Extract, Transform|
|Apache Spark|Apache Spark is an open-source distributed data processing engine that provides fast and general-purpose cluster computing for big data processing.|Transform|
|AWS Glue|AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services, making it easy to prepare and load data for analytics.|Extract, Transform, Load|
|Databricks|Databricks is a unified data analytics platform that provides a collaborative workspace for data engineering and data science tasks, built on Apache Spark.  |Transform|
|dbt|dbt (data build tool) is a popular open-source data transformation tool that enables data analysts and engineers to transform, analyze, and document data.|Transform|
|Fivetran|Fivetran is a fully managed data integration service that continuously syncs data from source systems to data warehouses, making it easy to centralize data.|Extract, Load|
|Google Cloud Dataflow|Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines. It supports both batch and stream processing on Google Cloud Platform.|Transform|
|Matillion|Matillion is a cloud-native ETL and data integration platform that simplifies the process of loading, transforming, and joining data on cloud data warehouses.|Extract, Transform, Load|
|Stitch|Stitch is a cloud-based data integration service that replicates data from various sources into data warehouses for analytics purposes.|Extract, Load|
|Talend|Talend is a popular data integration and data quality platform that supports data engineering, data integration, data profiling, and more.|Extract, Transform, Load|
|dlt|Declarative incremental loading with schema evolution. The simplest pythinic way to load data.  Open source Library for data loading, featuring schema evolution, native support for airflow, dbt ,etc.|Extract, Transform, Load|
|FME|Data can be read from any format and written to any other. Provides tremendous transformation functionality, resulting in output that can be much greater than the sum of the inputs.  Has a rich data model designed to cover all possible geometry and attribute types.|Extract, Transform, Load|
|SSIS|SQL Server Integration Services (SSIS) is a Microsoft SQL Server database built to be a fast and flexible data warehousing tool to perform high-performance data integrations.|Extract, Transform, Load|
|Qlik| | |
|Ab Initio| | |
|Datastage| | |
|ODI| | |
|Glue| | |
|ADF| | |
|SQLMesh| | |
|preql| | |
|WhereScape| | |
|Informatica| | |
|Coalesce.io| | |
|Estuary| | |
|integrate.io| | |
|Cdata| | |
|Portable| | |
|CloudQuery|Open-source, high-performance data integration framework built for developers. Extracts, transforms, and loads configuration from cloud APIs to variety of supported destinations such as databases, data lakes, or streaming platforms for further analysis.|Extract, Transform, Load|

^(Table) ^(formatting) ^(brought) ^(to) ^(you) ^(by) [^(ExcelToReddit)](https://xl2reddit.github.io/)",False,0.88,https://www.reddit.com/r/dataengineering/comments/15aeh6s/list_of_etl_tools_short_description_and_their/,dataengineering
API -> Postgres -> S3 -- what is the best design pattern?,15amy4f,riv3rtrip,1690418191.0,,False,True,Help,16,False,2,"I have an API endpoint that writes to a postgres database on each call. The writes are insert only, rows are not mutated, and can be partitioned by their write time.

Ultimately I want to replicate the database in Snowflake via S3, and I don't want to use something like Fivetran.

S3 to Snowflake is straightforward enough. We would do this on a 30 minute schedule running a COPY INTO statement.

The decision I am stuck on is how to do Postgres to S3.

As far as I can tell, I have two options:

- Copy directly from Postgres to s3 with the aws_s3 postgres extension as part of the batch job
- Write to s3 after each API call as a background task in the app

I'm unclear on the best option to go with.

Arguments in favor of aws_s3 extension: Copying directly from Postgres is safe for data integrity since the app code's transactions are atomic and reliable, whereas the background job can potentially fail even if a request and transaction succeeds. It also leads to simpler infrastructure. (I'd rather push more logic into batch jobs than the app code.)

Arguments in favor of app dumping directly to S3 from the app: Mainly I'm worried about having to manage performance of the Postgres instance. Performance wise, I'd rather just scale up the number of web app instances than have the Postgres database take a hit copying 30 minutes of data at a time.

Any thoughts? What's the best way to go about this? I'm leaning toward the aws_s3 Postgres extension, but I'm not 100% sure.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15amy4f/api_postgres_s3_what_is_the_best_design_pattern/,dataengineering
AWS Aurora not usable for OLAP workloads?,15agi5q,golangcafe,1690402621.0,,False,True,Discussion,3,False,4,"50-60GB database on aurora, everythings fine on writes/updates and transactional operations. We hooked the db to be used as analytics dashboard pulling a fair amount of data but disk i/o is extremely slow from query plan around 5-15mb/s. Bumping up instance ram and loading all working database into shared_buffers cache everything works fine. So I came up to the following conclusions:

- aws aurora is basically unusable for olap workloads as i/o timings are terribly slow (and couldn't find an official figure from aws)
- aws aurora storage layer uses a storage attached network observed disk i/o is very low and there is no OS level cache so they recommend loading up all databse in memory
- does aws rds postgres with provisioned ssd suffer from the same problem? iops and throughput are 100x better on paper
- anyone had similar experiences with aurora and working with relatively large datasets?
- considering moving to redshift",False,1.0,https://www.reddit.com/r/dataengineering/comments/15agi5q/aws_aurora_not_usable_for_olap_workloads/,dataengineering
What is streaming SQL?,15al69d,mwylde_,1690413596.0,,False,False,Blog,1,False,2,,False,0.76,https://www.arroyo.dev/blog/streaming-sql-explained,dataengineering
How do I become a better data engineer?,159v9e8,heyveryfunny,1690345164.0,,False,True,Help,30,False,44,"I just started my career as a data engineer. I have been learning a lot at work, but I think it’s slowing down and not fast enough. I really want to get good at data engineering, I aspire to become a better data engineer. What can I do during my spare time to work on that? I am planning to start work on Leetcode, to practice my Python and build up DSA knowledge. What else can I do to learn faster?

I use mainly SQL and no code GUI for ETL (which is bad I believe)",False,0.94,https://www.reddit.com/r/dataengineering/comments/159v9e8/how_do_i_become_a_better_data_engineer/,dataengineering
Meta DE onsite,15amv6w,justacommonbitch,1690417978.0,,False,True,Interview,2,False,0,"Hi all!

I have an onsite coming up for a de role at meta. I’m looking for any resources that can help as well as a general understanding on how to prepare for it. I hate my current job due to my toxic manager and I’m looking to switch asap

Thank you!",False,0.5,https://www.reddit.com/r/dataengineering/comments/15amv6w/meta_de_onsite/,dataengineering
Solutions Architect role at Databricks?,15a26zd,DataApe,1690368116.0,,False,True,Career,7,False,12,"I'm a data engineer at one of the big techs, and I'm considering a role as a Specialist Solutions Architect at Databricks. It specializes on data engineering but it seems to be a lot of customer facing role, which is different from my current role.

How is this position in terms of career progression? I do want the option to get back to data or software engineering in the future, and I feel I will get shoed in to a client facing role if I take this.

Also if anyone knows anything about the pros and cons of this position at Databricks, like wlb/comp, etc. I would love to know!",False,0.87,https://www.reddit.com/r/dataengineering/comments/15a26zd/solutions_architect_role_at_databricks/,dataengineering
Purpose of developing REST API,15almcn,cyamnihc,1690414724.0,,False,True,Discussion,9,False,1,"I am unable to understand the need to develop REST API to spit out data. I have data sitting in redshift. If I give consumers access to a particular table that they need, isn’t that a better option than creating rest api and providing data thru that. Why the need to add REST APIs as an intermediary ?",False,0.67,https://www.reddit.com/r/dataengineering/comments/15almcn/purpose_of_developing_rest_api/,dataengineering
Recommandation needed for building an Open Source pipeline / datalake,159yg87,Proof_War6424,1690355604.0,,1690359315.0,True,Help,25,False,19,"Hey fellow data enthusiasts!

I've recently been hired to work in a new small DE team, and I've been tasked with building a more ""big data"" environment for our company. Unfortunately, we have a strict policy against using cloud storage, so I'm looking for recommendations and guidance on creating a robust solution with open-source tools only.

Currently, most of our data (CDRs files) is continuously stored on a server using cron jobs, and we load it into Oracle. Additionally, we have dblinks connections from other databases to our main Oracle database. However, we're facing some challenges with slow PowerBI dashboards due to the large volume of data ( hundreds of billions of rows), complex joins, and aggregation functions.

&#x200B;

Here's what I'm thinking of for our new environment:

1. Data Ingestion: I'm considering using Apache Airflow and Kafka. or is kafka overkill ? any alternative if ?
2. Data Processing: Spark seems like a great choice for data processing ?
3. Datalake: I've come across HDFS as a popular choice, but I'm also curious about other alternative as I heard Hadoop is a pain to manage. Any experiences or feedback on Minio?
4. Analytics Oriented Database: As our PowerBI dashboards are becoming slower with increasing data volume (overload Oracle) , I believe it's essential to have an analytics-oriented database to improve performance. I'd love to hear your recommendations for databases that work well with PowerBI and can handle large-scale analytical workloads.

Our ultimate goal is to not only improve the performance of our current analytical processes but also lay the groundwork for ""potential future"" machine learning projects.

If you have any experience, suggestions, or advice regarding these components or any other tool that you think could enhance our environment, please share your insights! I greatly appreciate any help you can provide.

Feel free to ask for more details if needed. Thank you for taking the time to read this post!",False,0.91,https://www.reddit.com/r/dataengineering/comments/159yg87/recommandation_needed_for_building_an_open_source/,dataengineering
Scared of Redshift - Any Good Resources to Learn with Low Cost,15acq1h,Scalar_Mikeman,1690394010.0,,False,True,Help,6,False,2,"Seems like Redshift would be good to learn, but the pricing I see on it scares the c#@p out of me. Does anyone know a good tutorial which also includes how much will be spent by following along and also shows thorough instructions on how to shut down and destroy the resources when finished?",False,0.75,https://www.reddit.com/r/dataengineering/comments/15acq1h/scared_of_redshift_any_good_resources_to_learn/,dataengineering
What's the right learning order for these SQL subjects?,15a94v3,TreatOk8778,1690385880.0,,False,True,Help,25,False,4," 

SQL query tuning and optimization.

SQL indexing.

SQL integration.

SQL reporting.

SQL execution plans.",False,0.83,https://www.reddit.com/r/dataengineering/comments/15a94v3/whats_the_right_learning_order_for_these_sql/,dataengineering
"My dear data engineering friends, which data lake you are currently using or do you intend to use?",15ahrnj,yingjunwu,1690405510.0,,False,True,Discussion,8,False,0,"Why do you need a data lake, instead of just S3?

Which data lake do you use? Delta Lake, Apache Iceberg, or Apache Hudi?

Any comment is welcome!!!",False,0.5,https://www.reddit.com/r/dataengineering/comments/15ahrnj/my_dear_data_engineering_friends_which_data_lake/,dataengineering
Which databases are good for ELT (Extract-Load-Transform)?,15a0sux,binary-data,1690363720.0,,False,True,Help,15,False,8,"I am a backend developer involved in building an analytics pipeline for our application to display usage statistics to our users. I had no training in data engineering so far.

Before reading about ELT, I had an idea to utilize an AWS Glue job to extract data from the source (MySQL), transform it within the same Glue job, and load the resulting set into a data warehouse to perform queries. But as I understand, this would be a classic ETL approach.

But the ELT approach, as I understand it, would look different: utilize an AWS Glue job to load data into a data warehouse as is (basically copy the existing MySQL tables there), and then transform data using queries like:

    INSERT into table3
    SELECT table1.column_a, table1.column_b, table2.column_c, table2.column_d, FROM table1
    JOIN table2

Every source I read mentions that the ELT is a superior and more recent approach than ETL.

But I don't feel like this. I have a couple of concerns about ELT:

* With ETL, the tool (AWS Glue) would maintain incremental data extraction. With ELT, I must implement a mechanism for incremental data transformation (e.g. only insert changed rows into *table3*)
* AWS Glue performs transformations in parallel. This is not likely the case for relational databases like MySQL, as it would process the query above on a single node.

So my questions are:

* Are there flaws in my understanding of ELT?
* Which databases/tools can act as data warehouses which support incremental load and parallelism?
* Which of them are available in AWS?

I'd also prefer not to use sophisticated or enterprise tools since we don't have lots of data (less than 10M rows). Thanks.",False,0.9,https://www.reddit.com/r/dataengineering/comments/15a0sux/which_databases_are_good_for_elt/,dataengineering
Use-cases for provisioning Fivetran via Terraform,15ah0fx,Thybrat,1690403778.0,,False,True,Discussion,3,False,1,"Hi everyone! I've been thinking about how I can store our Fivetran configs as code and provision changes via something like Terraform. Setting this up for a new project but wonder if it is ever really necessary?  


What are your thoughts? Seems like oftentimes it's a one-time setup (that is easy enough via UI) and it doesn't have to be changed/deployed often since there is also not really a ""dev"" environment for the connectors.  


Can only think about storing the setup configs for the sake of transparency among the rest of the configs but did you think about something similar before?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ah0fx/usecases_for_provisioning_fivetran_via_terraform/,dataengineering
Recommend a good k8s / EKS guide?,15a32at,wtfzambo,1690370610.0,,False,True,Help,0,False,3,"I'm familiar with Docker and docker-compose, but I never had to use k8s and I would like to learn, in order to be able to deploy my own stuff with this method. Currently I'm working on AWS and have it available for experiments.

If you have any guide/tutorial/walkthrough to recommend, please do share!",False,0.72,https://www.reddit.com/r/dataengineering/comments/15a32at/recommend_a_good_k8s_eks_guide/,dataengineering
Effectively communicate API changes via automation,15ag8bn,Competitive_Speech36,1690401994.0,,False,True,Blog,0,False,1,"I am building an IT artchitecture for a promising startup which requires me to work a lot with application programming interfaces. Whether Experience, Process, or System APIs, application programming interfacesare way more than just app construction kits. They're the backbone supporting organizations in reaching their goals—tech, org, or financial.

Here are some insights on how to deal with API changes I want to share:

&#x200B;

**Understanding Your API Audience: The Starting Point**

The heart of the matter in communicating API changes is knowing your audience. Your audience, the ""Who"", can be anyone from internal stakeholders (devs, managers, execs) to external API consumers.

Recognizing your audience's unique needs makes all the difference in communication. The audience is your main contact point to send out notifications for API changes or new API rollouts.

&#x200B;

**Setting Up Communication Channels: The Pathway**

Next up is creating the right communication channels. These aren't just a medium for conveying API info like name, description, location, etc. They're also a tool for proactive and passive communication, giving your audience a heads-up when an API change impacts their apps.

To build this, you've got three potential strategies: collaboration platforms (like Slack), portal software, and simple code writing. Each of these can equip your audience with notifications, message delivery, and access to API documentation.

&#x200B;

**Identifying the Type of API Change: The What and Why**

Let's talk about why APIs change. Several reasons can trigger these—new business abilities, process modifications, or architectural improvements. Knowing the nature of these changes, whether they're breaking or non-breaking, is key.

Breaking changes can cause applications to crash, while non-breaking changes are mainly informational and non-disruptive. It's vital to communicate this clearly to your audience. This helps minimize deployment risks, build trust, and keep your audience informed.

&#x200B;

**Creating the Right Notification Message: The How**

Designing your API notification message needs understanding. Your aim should be to make the message easy to digest for your entire audience. Segment your audience, keep your message relevant, and timing is crucial. Avoid information overload and confusion, and your audience will appreciate it.

&#x200B;

**The Big Picture: Pulling it All Together**

When all these elements join forces, you get more than just effective communication. You create a smooth synergy that benefits everyone involved. Understanding your audience, establishing clear communication channels, recognizing API change types, and crafting easy-to-read messages are key to a successful API change communication strategy.

&#x200B;

I've learned a lot on my IT journey and I'm excited to share all my experiences and insights with you. Visit my blog for more articles like this: [https://ainsys.com/blog/2023/05/16/api-changes/?utm\_source=linkedin&utm\_medium=social&utm\_campaign=security\_forum&utm\_content=api\_automation&utm\_term=Automation](https://ainsys.com/blog/2023/05/16/api-changes/?utm_source=linkedin&utm_medium=social&utm_campaign=security_forum&utm_content=api_automation&utm_term=Automation)",False,1.0,https://www.reddit.com/r/dataengineering/comments/15ag8bn/effectively_communicate_api_changes_via_automation/,dataengineering
"Automating Data Pipeline Deployment on AWS with Terraform: Utilizing Lambda, Glue, Crawler, Redshift, and S3",15a92hh,Jealous_Ad6059,1690385721.0,,False,True,Discussion,0,False,2,"[https://medium.com/@stefentaime\_10958/automating-data-pipeline-deployment-on-aws-with-terraform-utilizing-lambda-glue-crawler-1621e0736edd](https://medium.com/@stefentaime_10958/automating-data-pipeline-deployment-on-aws-with-terraform-utilizing-lambda-glue-crawler-1621e0736edd)

&#x200B;

&#x200B;

https://preview.redd.it/rscsdc6fwbeb1.png?width=1920&format=png&auto=webp&s=c13b622f69f101645a7e291eb69e12a4ec6f1556",False,1.0,https://www.reddit.com/r/dataengineering/comments/15a92hh/automating_data_pipeline_deployment_on_aws_with/,dataengineering
Need help with this query,15af0xx,SD_strange,1690399261.0,,False,True,Help,2,False,1,"I have a pyspark dataframe having cols as student\_id, subject, and grades. I want to get only the ids that have grades 'A' for all subjects.

One way of doing this is by grouping on student\_id and getting the distinct count of grades, then filtering out the ids having count as 1 and left joining with the original dataframe to check if this single value of grade is 'A' or something else.

Is there a better way of doing this?",False,1.0,https://www.reddit.com/r/dataengineering/comments/15af0xx/need_help_with_this_query/,dataengineering
Easiest way to ingest lat/long data via GPS,15a8yck,hornfan87,1690385459.0,,False,True,Help,2,False,2,"I'm assisting a non-profit in a food supply chain project and wanting to understand the cheapest, most efficient way to track the lat/longs of their trucks over time, including real-time. Only about \~10 trucks and we don't need anything fancy. Just truck+datetime+lat+long.

What hardware/tools would be cheapest to obtain this type of information? Options I've considered, but each has their downsides:

1. My mind went to Airtags first for simplicity purposes, but accessing history seems possibly achievable, but frustrating.
2. Verizon Connect Fleet Tracking - Ability to access historical data, but I didn't see lat/long in their video. Also probably too expensive, but they're too vague on pricing on their site.
3. A quick ""GPS Fleet Tracking"" search yields a ton of different company offerings, but many seem like overkill (and would likely price as such). I'm looking for the simplest offering, not the most robust.

For what it's worth, I'm relatively new to data engineering and brand new to ingestion of lat/longs, so apologies if this is a stupid question!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15a8yck/easiest_way_to_ingest_latlong_data_via_gps/,dataengineering
DE or BI to figure out business logic requirements ?,15a7d7y,taafpxd,1690381714.0,,False,True,Help,6,False,2,"We are currently working on replacing one of our online management systems with a new one and one of the requirements for the migration is to sort out how we are going to do this on a DW side.

I know that my role is to figure out what the ETL/ELT process is and make sure that the data pipelines are feeding in but the biggest concern I have is understanding whether my job involves getting the business logic for field transformation directly or whether this is something which the BI team should do ? I've worked as a BI analyst in the business and I've always thought that this should be a BI job to understand how each column is calculated at least on a high end level and then work with the DE to figure out whether there's any extra details to any columns. One of the arguments for it being a DE job for business logic requirements is that there is already backend logic to this which I think is fair but not all the column is mapped 1-1 and the systems totally different.

Haven't asked my manager but was hoping to grab some 2nd opinions before this is raised up as it's definitely making me frustrated. As context I've work in BI for 4 years and DE for 2.",False,0.75,https://www.reddit.com/r/dataengineering/comments/15a7d7y/de_or_bi_to_figure_out_business_logic_requirements/,dataengineering
Data Observability: The Next Frontier of Data Engineering,15a6h78,Emily-joe,1690379554.0,,False,False,Blog,0,False,2,,False,1.0,https://www.dasca.org/world-of-big-data/article/data-observability-the-next-frontier-of-data-engineering,dataengineering
How important is Data Quality,15ahiwc,ninja790,1690404975.0,,False,True,Discussion,9,False,0,"With all the fuss around data Quality tools, how important it is for your organization. Please also mention your org size.",False,0.25,https://www.reddit.com/r/dataengineering/comments/15ahiwc/how_important_is_data_quality/,dataengineering
Do you all care about the type of data you're working with?,159g777,what_duck,1690308697.0,,False,True,Discussion,79,False,67,"I'm considering a job in insurance. It has all the tools and skills I want to learn, master, but I wonder what working in insurance will mean for my day-to-day. I come from a public sector background, so I'm used to feeling ""good"" about my work. Insurance isn't ""bad"" but it often has that reputation. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/159g777/do_you_all_care_about_the_type_of_data_youre/,dataengineering
Delta Live Tables Pipeline unable to read data from S3 mount,15a5734,No_Conversation_2474,1690376327.0,,False,True,Help,0,False,2,"As the title suggests, I'm trying to read in a fair bit of data from my s3 bucket, I decided to go ahead and mount it to my dbfs. While I'm able to read it using Autoloader standard at the same mount location. When I try running my Workflow using Delta Live Tables I get an error saying 

""Cannot infer schema when the input path \`/mnt/mnt\_s3\` is empty. Please try to start the stream when there are files in the input path, or specify the schema.""

However [dbutils.fs.ls](https://dbutils.fs.ls)('/mnt/mnt\_s3') gives me this \[FileInfo(path='dbfs:/mnt/mnt\_s3/flightdata2018.json', name='flightdata2018.json', size=3276800, modificationTime=1690369838000)\]

P.S Complete noob to databricks, any help is greatly appreciated ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15a5734/delta_live_tables_pipeline_unable_to_read_data/,dataengineering
Data Scientist interviewing for a Data Engineering role,15a02oy,Impressive_Fact_6561,1690361223.0,,False,True,Career,1,False,3,"Hi, I’m a Data Scientist of 5 years out of a job and have just got an interview for a Data Engineer role coming up. I’m thinking it’s a good role as I like the engineering stuff, and really need a job!

 So my questions are: 
1) What topics are worth focusing my prep/study on? 
2) I need to prepare a set of a few slides about a recent DE end to end project I’ve worked on. Any tips on that, as I have created a feature store for ML before, and have some personal projects too?

Thanks for any advice and tips!",False,0.72,https://www.reddit.com/r/dataengineering/comments/15a02oy/data_scientist_interviewing_for_a_data/,dataengineering
Load huge volume of data to redshift,159wr91,priyasweety1,1690349909.0,,False,True,Discussion,7,False,7,"I have around 500+ GB of data per day with 800 columns that I need to load into Redshift for 5 years worth of data. However, the data loading process is quite slow using copy command from s3 to redshift. Any suggestions on how to optimize and speed up the data loading in Redshift",False,0.89,https://www.reddit.com/r/dataengineering/comments/159wr91/load_huge_volume_of_data_to_redshift/,dataengineering
Single Data Source and 360 Access,15a9za1,OJ_Dyansty35,1690387807.0,,False,True,Help,6,False,1,"I work for a financial institution and they want me to create/develop a single source of truth for our data. Basically they want me to house it in one spot that has live updates so that everyone at the institution can access it and not have to worry if it's outdated or not. When this process is complete they also want me to develop a customer 360 dashboard that would also have live time updates. I have somewhat of a background in coding, data science, and analytics. I know this isn't directly up my alley but I would like to build my knowledge about this sort of thing. I have also used a little of Microsoft Azure in school but never fully tapped into it, just did the basic assignments I was given. Anyways I'm just looking for answers as to where I should start the road map or what softwares or applications would be useful for completing the task.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15a9za1/single_data_source_and_360_access/,dataengineering
Ideal system architecture for both workflow orchestration and streaming,15a9awd,Turbulent-Sand4889,1690386266.0,,False,True,Help,0,False,1,"My 4-engineer team is planning to build data pipelines to do the following - curious what everyone's thoughts are on the best architecture here - thanks in advance for your thoughts:

* Politely scrape public sites - then pop the results into a queue for downstream processing
* Clean / enrich the results (entity extraction, translation, sentiment, etc.)
* Expect around 2 million records per day through this pipeline
* Store the results in ElasticSearch - get them here as quickly as possible, as we're building 'current events' dashboards
* We also have scheduled jobs we are currently responsible for that we'd like to move into this setup
* Need to run within AWS, will be tough to bring in other vendors (i.e. Astronomer, Confluent, etc.)

We don't have an infrastructure team so we're responsible for all the infrastructure we stand up. Our devs are mostly experienced in Python. Our local setups are fully containerized, hoping to keep it that way if possible.

Want to prioritize:

* Good local setup for development
* Minimize infrastructure maintenance
* Ease of monitoring and retrying / re-running failed jobs
* Type safety between steps in data pipeline
* Ease of finding / visualizing data flow & dependencies
* Autoscaling in Production
* Speed of start to end of pipeline for individual records

Thinking of using AWS's Managed Airflow for workflow orchestration and achieving enough streaming capabilities through Sensors pointing at Redis or SQS queues... 

Curious if this sounds like a reasonable approach, if there's a better architecture these days, and/or if i'm off-base with all of these requirements. Thanks again",False,1.0,https://www.reddit.com/r/dataengineering/comments/15a9awd/ideal_system_architecture_for_both_workflow/,dataengineering
Need help in Azure Data Factory,15a8zz5,Abhi123378,1690385558.0,,False,True,Help,9,False,0,"I'm facing a challenge in Azure Data Factory that i don't know how to solve. 

I want to implement an Until loop that gives me the below output. 

End date: 2023-07-26
Total no.of days: 62

I want to subtract end date by 14 days to get the startdate until the total no.of days become 0. 

The output what I'm looking for is the start date and end date for each iteration. 

But until loop goes on infinitely.",False,0.5,https://www.reddit.com/r/dataengineering/comments/15a8zz5/need_help_in_azure_data_factory/,dataengineering
"Polars, and rust",15a74aq,Maximum_Ad3685,1690381112.0,,False,True,Help,0,False,1,"Is it possible to replicate the behavior of `DataFrame::partition_by` on a `LazyFrame`? I'm hoping to sink Hive-style partitions to disk in a streaming fashion.

Any help is appreciated 🙏",False,0.67,https://www.reddit.com/r/dataengineering/comments/15a74aq/polars_and_rust/,dataengineering
"Are data engineers taking 4X the amount of time to build, or working 18 hours a week maintaining data pipelines?",159cf8h,drc1728,1690300484.0,,False,True,Discussion,35,False,52,"In a recent Data Engineering Podcast episode I heard that there are data engineers and architects working on distributed data pipelines processing large amounts of data and a bunch of tools including distributed messaging systems or pub-sub tools like kafka, kinesis and data processing like spark, flink among others to deliver analytics and predictive algorithms.

There were a couple of anecdotes that made me curious.

First one was that in a well known networking and tooling company they planned for 9 months to setup their analytics stack and ended up taking 3 years to build something that could do the job but not scale.

Second one was that these systems were so complex that the data orgs were often working 18 hours a day 7 days a week to operate and maintain the data stack.

I can relate to the first claim, and partially to the second one. I want to learn from the community.

This community is 117k strong, therefore a pretty solid sample size. I want to learn from your experience about the veracity of the anecdotes. Here are a couple of prompts/questions for you to share.

* What is the gap between planned work and the actual time taken to build the data pipelines that you have worked on?  
9 months plan and 36 months delivery is a 300% additional time required over the planned effort, I can't see that in startups, growth stage companies, even scale ups. What is your experience?
* On an average what would you average hours per week to support the data pipelines operations?  
18 hours a day 7 days a week amounts to 126 hour weeks, I can attest to 70 maybe even 80 hour weeks with on-call issues. is 126 hours per week, for real?",False,0.92,https://www.reddit.com/r/dataengineering/comments/159cf8h/are_data_engineers_taking_4x_the_amount_of_time/,dataengineering
Why JSON still rocks as a format for data lakes,159lfpu,artsyfartsiest,1690320045.0,,False,False,Blog,19,False,16,,False,0.78,https://opendatascience.com/choosing-a-data-lake-format-what-to-actually-look-for/,dataengineering
Best practices for working within ArcGis Warehouses?,159s722,lostinthewalls,1690336366.0,,False,True,Discussion,1,False,5,"I'm consulting with a team of data engineers as a solution architect. Long story short, they're looking to do some work for this customer that will entail hopping into the database and doing the normal thing of writing procs and such to enable some capabilities like automated quality checking/testing, enforcing constraints, yada yada. 

I've worked quite a bit with GIS and with most of ESRIs products, but this was the first time really looking at the table-level structure of an ArcGIS warehouse and it doesn't seem like the team I'm on really had either. Some quick googling didn't return any solid leads, but does anybody know of any best practices when developing capabilities within or next to a running enterprise level ESRI warehouse?",False,0.86,https://www.reddit.com/r/dataengineering/comments/159s722/best_practices_for_working_within_arcgis/,dataengineering
Most complex pipeline you have built?,15a18to,TheCumCopter,1690365182.0,,False,True,Discussion,1,False,0,Keen to hear the details,False,0.5,https://www.reddit.com/r/dataengineering/comments/15a18to/most_complex_pipeline_you_have_built/,dataengineering
Data replication,15a08ss,soujoshi,1690361808.0,,False,True,Help,6,False,1,"We currently have a requirement to replicate data from postgres to oracle. We are using AWS DMS service to achieve this. We feel the cost of this is huge. Data engineering team were asked to suggest better methods to optimizer cost and move away from DMS. 

I personally feel DMS is the best service to achieve this. In terms of cost optimization, we can only look at replication instance usage and reduce size etc to reduce costs. 

Any other options to achieve data replication? Or any other aspects within DMS I should be looking at to lower the costs. 

Note - Our entire infrastructure is on Aws. We heavily use spark, airflow.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15a08ss/data_replication/,dataengineering
Orchestrator migration away from custom solution,159zipu,RealBrofessor,1690359280.0,,False,True,Help,6,False,1,"Hi,  
I manage ETL jobs of a small team of machine learning engineers. We do not use any orchestration tool per se, the various jobs are all over our ecosystem  
- 90% of them is python code downloading data from S3 bucket, transforming it and uploading it to different S3 bucket. For this we run them on jenkins (ouch). These are run once per day  
- hourly jobs, again pure python, run as a cronjob in kubernetes  
- cronjobs, various bash scripts, run as cronjob in kubernetes, once per day  
- AWS lambdas, once per week  

I went through a couple of threads in this sub and checked out the main 3 contestants - Airflow, Prefect, Dagster. However, I have no idea which one of these would be the most suitable for our usecase.  
Things I would like the solution to be:  
- deployable to kubernetes  
- one centralized UI for every job  
- provide easy way to rerun/backfill specific jobs (with a log when/who run it)  
- add dependencies between jobs  

Anybody who has a similar setup and is using one of the orchestration tools? I would greatly appreciate any recommendations or experience. Cheers",False,1.0,https://www.reddit.com/r/dataengineering/comments/159zipu/orchestrator_migration_away_from_custom_solution/,dataengineering
What position after “Manager of DE”?,159glvw,nrskmn,1690309577.0,,False,True,Discussion,16,False,12,"I’m a senior DE at a midsize company. 
I see the following path -
DE > Senior > Lead > Principal > Manager

What would come next?

I always wonder this because I can’t remember the last time I heard a VP or DE. I know VP of Analytics exists but it’s often the Analysts or Data Scientists who take up those roles. 

Hence the question.",False,0.88,https://www.reddit.com/r/dataengineering/comments/159glvw/what_position_after_manager_of_de/,dataengineering
How do you and your team write good documentation?,1594xvv,yorkshireSpud12,1690282093.0,,1690305337.0,True,Discussion,44,False,44,"Hi fellow engineers,

I’ve been recently creating documentation for a new process that we’ve built and was wondering how other engineers approach writing documentation? what tools do you use? how'd you keep documentation up to date? Etc

I think the main challenge we are running into is the keeping docs up to date part. 

We’re using confluence to write docs and we’re also using Comala Document Management to help provide a review process to docs as well as auto expiring pages after a year so we can re-review but it still requires people to be checking for expired docs etc.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1594xvv/how_do_you_and_your_team_write_good_documentation/,dataengineering
A Roadmap to AWS Certifications for Data Engineering,1598wvm,lancelot_of_camelot,1690292536.0,,False,True,Discussion,11,False,24,"Hello DEs of Reddit, 

&#x200B;

I recently passed my first cloud certification (AWS CCP) and I spent a lot of time figuring out which certification is the most suitable one to pass next. I decided to design a roadmap that helps in choosing the right AWS certification.

&#x200B;

https://preview.redd.it/fw1s85z774eb1.png?width=1501&format=png&auto=webp&s=72b9bc2927ca77ea67d149416eea18afc3171138

You can read more about it in my blog post: [https://anniscodes.com/articles/cloud-certificates-data-engineering/](https://anniscodes.com/articles/cloud-certificates-data-engineering/)  


I would love to get your input on that!",False,0.93,https://www.reddit.com/r/dataengineering/comments/1598wvm/a_roadmap_to_aws_certifications_for_data/,dataengineering
ETL Tools,159bzjg,dontevenaddme,1690299541.0,,False,True,Discussion,16,False,12,What ETL tools are people using to put data in to snowflake form sources such as SQL & SAP,False,0.93,https://www.reddit.com/r/dataengineering/comments/159bzjg/etl_tools/,dataengineering
Describing previous work experiences in an Interview.,159g9rt,Fickle-Picture-7674,1690308861.0,,False,True,Interview,7,False,4,How do we answer question about describing work experience in an interview if someone has more than 8+ years of experience in multiple organization. Sometimes I think I am going too long and sometimes I feel Its too short. Whats the best way to describe it . How long we should spend in describing it?2 mins 5 mins or more?Is there any template for this ?,False,0.84,https://www.reddit.com/r/dataengineering/comments/159g9rt/describing_previous_work_experiences_in_an/,dataengineering
What's the best strategy to merge 5500 excel files?,158sqwa,Ein_Bear,1690246162.0,,1690379927.0,True,Help,176,False,121,"I'm working with a client that has about 5500 excel files stored on a shared drive, and I need to merge them into a single csv file. 

The files have common format, so I wrote a simple python script to loop through the drive, load each file into a dataframe, standardize column headers, and then union to an output dataframe.

Some initial testing shows that it takes an average of 40 seconds to process each file, which means it would take about 60 hours to do everything.

Is there a faster way to do this?

Edit:
Thanks for all the advice. I switched to polars and it ran dramatically faster. I got the total time down to about 10 hours and ran it overnight.

Answering a couple questions that people brought up:

* It took 40 seconds to go through each file because all files were in xlsm format, and it seems like pandas is just slow to read those. There are a ton of posts online about this. The average rowcount per file was also about 60k
* All files had the same content, but did not have standardized column headers or sheet names. I needed to rename the columns using a mapping template before unioning them.
* There was a lot of good feedback about breaking up the script into more discrete steps (copy all files locally, convert to csv, cleanup/transformations, union, db load). This is great feedback and I wish I had thought of this when I started. I'm still learning and trying to break the bad habit of writing a giant monoscript.
* It was important to improve the speed for two reasons: the business wanted to go through a couple iterations (grabbing different field/sheet/file) combinations, and it wasn't practical to wait 60 hours between iterations. There was also a very expensive issue caused by having a giant shitpile of excel files that needed to be fixed ASAP.",False,0.96,https://www.reddit.com/r/dataengineering/comments/158sqwa/whats_the_best_strategy_to_merge_5500_excel_files/,dataengineering
Working in Snowflake vs Azure synapse (or the Azure suite in general)?,159cd98,MasterKluch,1690300358.0,,False,True,Discussion,6,False,7,"So, let me preface this by saying my earliest career experience involved working in SQL Server. I worked in SQL server for the first 8-ish years of my data engineering journey (in addition to using other applications that connected to SQL server - SSIS, SSAS, Tableau, PowerBI, etc.).   
However, the last 3.5 years have all been snowflake development except for one short client experience where I was basically working in ADF, azure and sql server. I really appreciate what snowflake has to offer but I'm curious to know from those of you who have worked with Azure synapse and/or the azure spectrum of products, whether you enjoy it or if (assuming you've had some experience with snowflake) you prefer working with a RDBMS tool like snowflake?   
Yes, I know this isn't exactly an apples to oranges comparison but I'm thinking of dipping my foot back into the azure space and I'd like to hear opinions from anyone who's experienced both?",False,0.82,https://www.reddit.com/r/dataengineering/comments/159cd98/working_in_snowflake_vs_azure_synapse_or_the/,dataengineering
Introduction to Sketch Algorithms,159grgy,2minutestreaming,1690309915.0,,False,True,Blog,0,False,4,"# The Big Data Problem

Imagine you start in a new company as someone in charge of their real-time streaming infrastructure.

You are tasked with the problem of **computing the percentile distribution** of the company’s terabytes’ stream of data consisting of billions of records in Kafka, each representing a latency metric data point.

Calculating a percentile for a large dataset is very expensive, to do so - you need to:

1. store all the values
2. sort them
3. return the value whose rank matches the percentile (e.g 99th item)

Such big data aggregations are very tricky to solve. There is no way you can do this with terabytes.

The solution?

# Streaming Stochastic Sublinear Algorithms

Also called **Sketch algorithms**, these are algorithms that trade off a bit of accuracy for massive efficiency gains. They are:

* **probabilistic** (not 100% correct) - they usually have a strict, known ***error bound***.
* **one-pass** \- they go over each item in the stream only once.
* have **sub-linear space growth** \- input data grows, but the algorithm’s memory requirement does NOT grow linearly with it.
* **parallelizable** & **composable** \- you can split the data into two sets, compute sketches on them and then merge the results while guaranteeing the same accuracy. Parallelization can really scale this to infinity.
* **data insensitive** \- Big Data is extremely messy and disorganized. These algorithms handle things like NaN, infinites, nulls and are insensitive to the distribution/order of the data.

# DataDog’s Example

In the example above, you were actually placed in DataDog.

They invented their own sketch algorithm named [*DDSketch*](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3ZsZGIub3JnL3B2bGRiL3ZvbDEyL3AyMTk1LW1hc3Nvbi5wZGY_dXRtX3NvdXJjZT0ybWludXRlc3RyZWFtaW5nLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0b2NoYXN0aWMtc3VibGluZWFyLXN0cmVhbWluZy1hbGdvcml0aG1zIiwicG9zdF9pZCI6IjlkNjYyN2FhLWVkMDQtNDExMC04N2JhLWQyZmE0NDZjOTUwZSIsInB1YmxpY2F0aW9uX2lkIjoiMjYyYmJjNTktZDkyNS00MTcxLWFmZDYtYjE0NGEyYzM2MzZlIiwidmlzaXRfdG9rZW4iOiI3N2Q3MzkyMi1lY2QwLTRmN2ItYTNhYS05NDYwOTBlZDMzNDgiLCJpYXQiOjE2OTAzMDk4MDYuNzcsImlzcyI6Im9yY2hpZCJ9.hC6ECvmb-GrfMbqoEglbq_vewfjS6SkKMqh6S54BG8s).

It offers a 2% relative error bound, which means that if the true p99 is 60s → the sketch would return **58.8-61.2s**.

The algorithm is pretty simple:

1. Create buckets covering ranges of the desired error rate (+-2% in this case)
2. Each bucket keeps a counter of the amount of data points within that range.
3. When processing an item (latency metric data point), increment the counter of the appropriate bucket
4. To count the desired percentile, you sum up the bucket’s values until you get to the desired percentile. Whatever bucket that percentile is in - that’s your value.

With this, you only need:

**- 275** buckets  
**-** **\~2KB**  
to cover the range from 1 millisecond to 1 minute.

Another key point? This can be endlessly parallelized.

As we [*learned from S3*](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R3aXR0ZXIuY29tL0JkS296bG92c2tpL3N0YXR1cy8xNjYzODc5MjU3NDk4NzIyMzA2P3M9MjAmdXRtX3NvdXJjZT0ybWludXRlc3RyZWFtaW5nLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0b2NoYXN0aWMtc3VibGluZWFyLXN0cmVhbWluZy1hbGdvcml0aG1zIiwicG9zdF9pZCI6IjlkNjYyN2FhLWVkMDQtNDExMC04N2JhLWQyZmE0NDZjOTUwZSIsInB1YmxpY2F0aW9uX2lkIjoiMjYyYmJjNTktZDkyNS00MTcxLWFmZDYtYjE0NGEyYzM2MzZlIiwidmlzaXRfdG9rZW4iOiI3N2Q3MzkyMi1lY2QwLTRmN2ItYTNhYS05NDYwOTBlZDMzNDgiLCJpYXQiOjE2OTAzMDk4MDYuNzcsImlzcyI6Im9yY2hpZCJ9.V9XUMqa4mMnJvdtbkT4CM9HUqBEpiVW0eOyfvPYCnCk), parallelization is key to unlocking great performance at tremendous scale.

Notice - merging the sketch results together is as simple as merging two dictionaries/hashmaps of size 275!

# Other Uses

Sketch algorithms are used heavily in the industry for other things like:

* uniqueness - distinct elements
* frequency of items (heavy hitters)
* set union/intersection/difference
* AI large vector/matrix decomposition
* graph analysis - connectivity, weighted matching

For more examples & visuals, see [the original place this was posted](https://2minutestreaming.beehiiv.com/p/streaming-sketch-algorithms).",False,1.0,https://www.reddit.com/r/dataengineering/comments/159grgy/introduction_to_sketch_algorithms/,dataengineering
From Python to Elixir Machine Learning,159709y,semicausal,1690287837.0,,False,False,Discussion,0,False,9,,False,0.92,https://www.thestackcanary.com/from-python-pytorch-to-elixir-nx/,dataengineering
DMS Task setting and replication instance size,159depf,PidKiller09,1690302627.0,,False,True,Discussion,4,False,3,"Hi all.

I'm curious to hear your opinions about DMS tasks parallelization and replication instance size when running DMS for a database migration.

I understand many factors to consider here. I will try to list them:

* Database Size
* Data distribution on tables (some tables can be really large while other are small, so best to load large tables in parallel per partition or by range)
* Time window to perform Full Load (downtime)
* Use of Full Load + CDC (needed if database is too huge and migration time does not fit in the downtime window)

&#x200B;

So if I choose a replication instance like

dms.r4.4xlarge16 vCPU122 GIB

Does it make sense to have 16 threads in  MaxFullLoadSubTasks task settings? Like one thread per vCPU? Would it be better to split that in multiple DMS Migration Tasks? Maybe 2 tasks with 8 MaxFullLoadSubTasks?

How does one estimate how long it is going to take to migrate a 4TB database using this configuration?

Just wondering how you guys make this kind of design decisions and your train of thought on it.

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/159depf/dms_task_setting_and_replication_instance_size/,dataengineering
Making Early Investments Into Your Data Teams Through Valuable Trainee Experiences,159m336,Western-Opening-9212,1690321467.0,,False,False,Blog,0,False,0,"A few weeks ago I wrote up my first blog!

Having started my career on a Data & Analytics graduate scheme, I have been able to appreciate how internships, graduate programmes, re/up-skilling programmes — or any kind of trainee opportunities — are critical avenues to investing in people who can evolve your team and bring in new ideas and ways of thinking, ultimately making your team more resilient to the ever-changing data landscape.

Therefore, the purpose of this blog is to outline a generalised, structured approach to how data teams across any industry can deliver impactful training opportunities, by wrapping the projects you align to trainees into a templated project proposal.

By the end of the blog, you will understand that by investing the time and effort upfront to have this in place for your team, not only will you reap the benefits outlined, but you are far more likely to have someone (who you are already acquainted with) wanting to be a part of your team down the road.

I particularly think this would be an interesting read for those leading Data Engineering teams (that have an extensive list of backlog items). 

I would greatly appreciate if you could have a read and let me know your thoughts and feedback!",False,0.33,https://medium.com/@ronanwalters/making-early-investments-into-your-data-teams-through-valuable-internship-experiences-657edbee8908,dataengineering
From iPython-sql to JupySQL: new updates,159fmf5,ploomber-dev,1690307453.0,,False,True,Open Source,2,False,2,"**TL;DR incorporate SQL functionality within Jupyter, access to modern data processing DBs (like DuckDB), polars and data exploration through plotting easier with JupySQL.**

hey r/dataengineering! I'd like to share some news about JupySQL, an open source project my team has been working on!

JupySQL allows you to run SQL and perform exploratory data analysis in Jupyter via magics %sqland %%sqlmagics, and visualize your data with %sqlplot.

This project is open source, and a successor to [iPython-SQL](https://github.com/catherinedevlin/ipython-sql#legacy-project). It is compatible with all major databases (e.g., PostgreSQL, MySQL, SQL Server), data warehouses (e.g., Snowflake, BigQuery, Redshift), and embedded engines (SQLite, and DuckDB).

This week the team released version 0.8.0 with improved data profiling and DuckDB performance when converting resulting tables to other formats (pandas and polars).

Learn more: [https://jupysql.ploomber.io/en/latest/quick-start.html](https://jupysql.ploomber.io/en/latest/quick-start.html)

if you have any questions, feel free to ask! And show your support with a star on [GitHub](https://github.com/ploomber/jupysql)!",False,1.0,https://www.reddit.com/r/dataengineering/comments/159fmf5/from_ipythonsql_to_jupysql_new_updates/,dataengineering
Azure Data Factory Scheduling,159jj7v,InvestigatorMuted622,1690315896.0,,False,True,Help,9,False,1,"Hello,

I was wondering how are people in the industry scheduling azure data factory pipelines. I know that triggers exist but how to establish a dependency between two pipelines, is there a way to do that in ADF. 

Currently, I just have one master pipeline that links all the other pipelines but at some point of time it gets out of hand.

Also, I am trying to schedule fact and dimension. Any help would be appreciated.",False,1.0,https://www.reddit.com/r/dataengineering/comments/159jj7v/azure_data_factory_scheduling/,dataengineering
About Data Factory on Microsoft Fabric,159j3tv,Relsen,1690314989.0,,False,True,Discussion,2,False,0,"I work on a financial consultancy and I discovered today about Fabric. I would like to know if it is possible to use this software to make a better link with clients and take their data directly in real time.  


So, for example, lets suppose I have a restaurant as a client, and that I want a system where he writes his daily revenues, foot traffic, costs and so on, so that these data go directly to my database without being necessary for me to receive his database, organize it with power query and make the entire process... His data would go direclty to an organized database that I can use fast to make analysis and to uptade Power BI Dashboards.  


Do Factory and Fabric have this capacity? It would be very useful for me.

&#x200B;

Thank you.",False,0.5,https://www.reddit.com/r/dataengineering/comments/159j3tv/about_data_factory_on_microsoft_fabric/,dataengineering
Open-Source Data Entry and Validation Tool,15981gq,ZKR2000,1690290410.0,,False,True,Discussion,4,False,2,"Hello everyone,

Our data infrastructure pulls in data from a variety of sources, with a heavy emphasis on well-known APIs such as Shopify, Google, Salesforce, and so forth. I'm looking to enhance this infrastructure with an open-source tool, one that allows for the creation of forms to facilitate data entry. At the moment, we're utilizing Google Sheets, but it's falling short in several areas, especially as users often fail to adhere to the prescribed format. While I understand Google Script can assist with validation, it doesn't adequately cope with our data volumes, rendering it impractical for our needs.

I'm curious to hear how other Data Engineers are addressing this challenge.

Here's a list of my requirements for the tool:

\- Open-source: It should be freely available, and I should be able to host it on our server.

\- Form creation: Admin users should have the ability to construct forms.

\- Role-based access control: Ideally, it should support this feature.

\- Data validation: The tool needs to be able to validate user input.

\- Grid view: It would be beneficial if the tool could provide a grid view (akin to Excel or Google Sheets), permitting users to enter multiple rows of data at once.

&#x200B;

Looking forward to your insights and suggestions.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15981gq/opensource_data_entry_and_validation_tool/,dataengineering
PIM System,159dzb0,smilycat_ag,1690303884.0,,False,True,Help,1,False,1,"Hello anyone working on Informatica PIM tool. Need help !!!
I want to gain It's P360 certification.
Please recommend available resources or courses. 
Whats the right approach?
Thanks in advance.",False,1.0,https://www.reddit.com/r/dataengineering/comments/159dzb0/pim_system/,dataengineering
The e-graph data structure,1593052,codorace,1690276170.0,,False,False,Blog,1,False,4,,False,1.0,https://www.cole-k.com/2023/07/24/e-graphs-primer/,dataengineering
Spreadsheets. Foe or Friend?,159bj8f,Crafty_Combination54,1690298529.0,,False,True,Blog,3,False,0,"I've always thought of them as foe. But are they really friend?  


When we built our previous company Datastay - a product lifecycle management platform - back in the mid 2000s, our biggest competition was the ""spreadsheet status quo"".  


Today, in the data and analytics space where we currently play, business users still desire the familiarity of the spreadsheet. But a lot has changed.  


Emerging tools like [Equals.app](https://Equals.app) and [coefficient.io/](https://coefficient.io/) enable spreadsheets to connect to live data and behave more like front end applications.  


I've started to embrace the spreadsheet as a friend - even recommending them for certain use cases with [lassoo.io](https://lassoo.io)  


What's your experience with these new breeds of connected spreadsheets? Any really interesting use case you've come across? Recommendation on other tools to check out?

Full post:[https://lassoo.io/blog/2023/07/24/analytics-vs-spreadsheets/](https://lassoo.io/blog/2023/07/24/analytics-vs-spreadsheets/)

&#x200B;",False,0.5,https://www.reddit.com/r/dataengineering/comments/159bj8f/spreadsheets_foe_or_friend/,dataengineering
"Users of delta/iceberg/hudi in production, how has your journey been so far?",158l3pt,aerdna69,1690228467.0,,False,True,Discussion,33,False,36,"I think the technology is old enough at this point to start aggregate some first-hand developer experiences in production.

Any particular pros or cons to signal? Do you wish you would've implemented a proper DWH instead? This is the thread where you can complain (or praise) about this framework!",False,1.0,https://www.reddit.com/r/dataengineering/comments/158l3pt/users_of_deltaiceberghudi_in_production_how_has/,dataengineering
Feels like data engineering has no future in my team,158bq43,ExcitingAd7292,1690207672.0,,False,True,Career,40,False,75,"I am a senior data engineer and working in team of 4 people. 2 data scientists and 1 manager. It was new team when I joined, I help them and built automated data pipeline from scratch to extract the data in S3 but they use it less frequently and no recurring jobs or cron jobs setup. I discussed importance of recurring ETL jobs and importance of data warehousing but feels like my manager is not interested in it. He only consider data science work as real work and ready to hire 4 more data scientists. Usually they prefer to develop model with single CSV/parquet file with bare thousand rows data and complete the model with power point presentation. no deployment or nothing because we even don’t have CI/CD/ devpos/ MLops pipelines. I force them to start GitHub for version controlling but it’s only 1 pr in a year so they are not using it anyways. My team only run sagemaker notebook and store any resultant output csv file along with notebook. It’s smaller team so I feel like micromanaging but that’s not issue for me but I am frustrated with they are not considering importance of data engineering at all. My manager is not even handling jira work. He now hired scrum master!!! I am confused since hiring freeze how do I change my team and because I on visa I cannot change my company in upcoming next 2 years. I can stay and chill here but I feel guilty wasting my time in my career and not learning anything new!",False,0.98,https://www.reddit.com/r/dataengineering/comments/158bq43/feels_like_data_engineering_has_no_future_in_my/,dataengineering
Importing large file system from OneDrive,1598kkd,faalschildpad,1690291704.0,,False,True,Help,6,False,1,"Hi Data Engineers,

My company has used a client tracking system a few years ago (2010-2016) but choose to switch to a better system after that. They left all the data in the previous system and only accessed it on request via a read only portal.

Now that we have some new policies, we are required to get the data that is still stored in the old application to ourselves, preferably in cloud storage.

As the supplier of this old system is a small party and they are not very tech savvy, they can only deliver the files in a OneDrive folder where all the data would be, folders ... What a nightmare. For reference, the data is about 200 GB large with loads of different files roughly counted it should be about 700.000 files (not sure yet).

Now I am looking into options to get things from a large OneDrive folder to a storage solution while maintaining folder structure. Initially I thought Azure Blob with hierarchical namespaces would be good (whole company is Azure based, so anything Azure/MS related has a huge pre).  


While looking at the options to move data I've tried things such as Power Automate/Logics Apps/Data factory but cannot find a good tool to do so. Power Automate/Logics apps have a throttled API which only allows 100 calls/60 seconds which makes it very inefficient. ADF doesn't have connectors for OneDrive which makes a Linked Service impossible to setup.

Does anyone have an idea how we can make this work? I'm sure there are possibilities we haven't considered yet! ",False,1.0,https://www.reddit.com/r/dataengineering/comments/1598kkd/importing_large_file_system_from_onedrive/,dataengineering
Views on merge vs delete insert,158xxyj,Stoic_Akshay,1690260308.0,,False,True,Discussion,7,False,5,"What are your views on delete insert performance against merge? In snowflake since its hybrid column store, more often than not it is advised to use a merge which goes to say yiu dont really know which micropartition the deletes would be on and it can get expensive. While i agree to that, but isnt that true for a merge too? I would like to get clarity on this and a rough idea of the considerations where one method may outrace the other.",False,1.0,https://www.reddit.com/r/dataengineering/comments/158xxyj/views_on_merge_vs_delete_insert/,dataengineering
Why learn terraform & docker for data eng? Am I missing something?,158fbot,IceStallion,1690215779.0,,False,True,Discussion,42,False,35,"I don’t understand what the need for docker is, having every pipeline set up as a docker image, even if it’s from scratch would still take up significantly more space than just a simple script with the correct environment already setup.

As for terraform, what need is there if the infrastructure is already set up? Do I really need to know how to deploy 5 ec2 instances? Wouldn’t they already be running in the background?

I’m at a point in my career where I’m picking up devops and cloud, including terraform but I’m not entirely sure if it’s particularly useful.

Hoping someone can help me understand",False,0.84,https://www.reddit.com/r/dataengineering/comments/158fbot/why_learn_terraform_docker_for_data_eng_am_i/,dataengineering
What does integration testing look like for DE?,158mov6,wtfzambo,1690231904.0,,1690273532.0,True,Discussion,18,False,12,"As title says.

How is integration testing setup for data pipelines, and could it have helped in the following situation?

I have a data pipeline that goes like this: batch extract daily from postgres to S3 -> read with pyspark, perform SCD2 and merge into a delta table.

Unit testing was all good, but we ran into an issue that couldn't be spotted with it: due to a bug in the backend code, some records that received updates since the last run retained their old `timestamp_updated`, thus the change wasn't detected and missing from the final table.

We found out because one data analyst found this inconsistency, and eventually traced the problem all the way back to the data source.

---

Edited for clarity",False,1.0,https://www.reddit.com/r/dataengineering/comments/158mov6/what_does_integration_testing_look_like_for_de/,dataengineering
Multi-tenant analytics pipeline,158vd7v,skuutti,1690253016.0,,False,True,Help,2,False,4,I struggle to figure out how to build a pipeline from Postgres to BigQuery since in Postgres every customer is a separate db. Do I really need to build a separate pipeline for every db or can I somehow squeeze the db tables (since all the customer dbs have the same schema) all into same tables that I can export using e.g airbyte? Help!,False,1.0,https://www.reddit.com/r/dataengineering/comments/158vd7v/multitenant_analytics_pipeline/,dataengineering
Remember that database migration making your revenue metric swing by +5%?,1592ku0,Srammmy,1690274881.0,,False,True,Open Source,1,False,0,"Hello people!

Me and a friend have been working on an open source tool to improve data quality issue, related to non-moving data that moves 😬 We call it a drift  
✅ Configure your smart drift alerting as code  
✅ Get instant ChangeLog of your metric  
✅ Audit changes and tag your drift patterns to drive continuous data quality improvement

If you are interested, star us and join the waiting list :D [https://www.data-drift.io/join-the-waitlist](https://www.data-drift.io/join-the-waitlist)  


https://preview.redd.it/to1o65guo2eb1.png?width=5760&format=png&auto=webp&s=c6112ba0e13a71c9e37d8cf3326cd0cca7a90f7b",False,0.43,https://www.reddit.com/r/dataengineering/comments/1592ku0/remember_that_database_migration_making_your/,dataengineering
freeze market has arrived in Latin America,158uahr,felipeHernandez19,1690250177.0,,False,True,Career,5,False,3,"While the job market was getting to its lowest point in the USA, Latin American companies keep hiring and hiring offering great job opportunities. My daily received 4 to 5 job offers from March to early July, in my mailbox and LinkedIn. But now the job offers have stopped, and only a few good ones are out there. I guess the freeze finally arrive here ( Colombia ) and the only thing we have left to do is to wait.",False,1.0,https://www.reddit.com/r/dataengineering/comments/158uahr/freeze_market_has_arrived_in_latin_america/,dataengineering
Unleashing the power of Data: Data Storage Systems,1590ooi,RemarkableAttempt311,1690268944.0,,False,False,Blog,0,False,1,,False,1.0,https://waruithemystery.hashnode.dev/unleashing-the-power-of-data-data-storage-systems,dataengineering
When is time to move away from a Postgres DW Solution to Snowflake?,158b3q0,JoseyWales10,1690206222.0,,False,True,Discussion,39,False,24,I am see many use cases where it is advised not to over-engineer and just use Postgres as your datawarehouse solution. What are some indicators that it is time to move away from Postgres and into a more traditional DW solution such as Snowflake? Or signs that Postgres is not even a good starting point? I am trying to prevent a pitfall where I go with a simple solution that does not scale/age well.,False,0.92,https://www.reddit.com/r/dataengineering/comments/158b3q0/when_is_time_to_move_away_from_a_postgres_dw/,dataengineering
Masters in Data Science or computer science???,158h71u,AnishNehete,1690219883.0,,False,True,Career,18,False,8,"I am currently in my final year of college and I would like to pursue my masters in a data related field so as to greatly increase my knowledge and job opportunities . I have seen many colleges offer data analyst courses / data science courses but none in data engineering?? Is there any overlap between data science courses(numpy, pandas etc.) offered by the college and true data engineering(Working on tools and technologies and creating ETL pipelines and config. them with datalakes, clouds etc.) 

Would love to know your point of view guys :)

P.S. - I am student from a tier 3 college in India and therefore doing Masters in an another country makes a lot of sense :)",False,1.0,https://www.reddit.com/r/dataengineering/comments/158h71u/masters_in_data_science_or_computer_science/,dataengineering
Handling Missing Values in Data Streams,158po8s,semicausal,1690238597.0,,False,False,Blog,0,False,3,,False,1.0,https://bytewax.io/guides/handling-missing-values,dataengineering
Snowflake developer,1588jm2,TestUser__,1690199711.0,,False,True,Discussion,5,False,17,"In LinkedIn I am noticing open positions with the job title as “Snowflake developer” , am I missing something here (in terms of why would they want a snowflake developer over sql developer)",False,0.91,https://www.reddit.com/r/dataengineering/comments/1588jm2/snowflake_developer/,dataengineering
Airflow In Production,158b3ge,cuckflemson1,1690206203.0,,False,True,Help,15,False,12,"Hey All - Does anyone have experience or a guide of how to run airflow in production? I currently have a Python script that builds the DAG and runs as expected with a sequential executor and Postgres backend.

While this works ok, the job will only initialize when my laptop is running. Ideally I’d like to get the job to run in the background each week regardless of whether my laptop is open. 

I’ve had a hard time getting this sorted out and was wondering if anyone here could point me in the right direction.",False,0.93,https://www.reddit.com/r/dataengineering/comments/158b3ge/airflow_in_production/,dataengineering
Rebuilding Tables?,1588eeb,TheCumCopter,1690199315.0,,False,True,Discussion,17,False,6,"I heard the term, “we rebuild the tables overnight” by a data engineer today. Not really sure on the context, but why is this done and what is it ?",False,1.0,https://www.reddit.com/r/dataengineering/comments/1588eeb/rebuilding_tables/,dataengineering
How to scrape an e-commerce site in an uninterrupted manner using JavaScript,158ly9a,9millionrainydays_91,1690230287.0,,False,False,Blog,0,False,1,,False,0.57,https://javascript.plainenglish.io/web-scraping-in-javascript-how-to-scrape-an-e-commerce-site-fa944eefa55f,dataengineering
"One Year With Redpanda, a Retrospective.",158ju70,sap1enz,1690225740.0,,False,False,Blog,1,False,1,,False,1.0,https://streamingdata.substack.com/p/one-year-with-redpanda-a-retrospective,dataengineering
SO but for cloud data engineering,158a6y5,srevolve,1690204018.0,,False,True,Help,9,False,3,What resource do you guys use to ask questions and interact with other people facing similar problems as you while using similar tools? This is the purpose of SO but i feel that for anything cloud DE related (i'm on AWS) i never get any answers. ,False,0.8,https://www.reddit.com/r/dataengineering/comments/158a6y5/so_but_for_cloud_data_engineering/,dataengineering
I have never had my code reviewed thoroughly,157ip1t,Agitated_Ad_1108,1690128456.0,,False,True,Career,64,False,129,"I'm a senior engineer, but have never had a proper code review because my managers and colleagues tend to have a data analyst/DBA background. Sometimes they ask questions about high level logic, but nobody has ever spent more than 5 minutes on reading my code or nitpicked small things which means I can't learn and grow.

What if I join a REAL DE team in the future as a senior with REAL code reviews and everyone can see how amateurish my code looks? That's going to be quite embarrassing and it will probably take forever to ship something to production.",False,0.99,https://www.reddit.com/r/dataengineering/comments/157ip1t/i_have_never_had_my_code_reviewed_thoroughly/,dataengineering
What is your unit testing implementation?,157tf13,ExistentialFajitas,1690153965.0,,False,True,Discussion,47,False,33,"Curious to see what others use to unit test in the DE space. I work primarily on the transformation side, where we’ve picked up DBT. Our stack is GitHub Actions for CI/CD, AWS/Airflow for orchestration, logging, observability, restartability, and a light runtime to create the initial DBT compilation, that’s then pushed down to SnowFlake and utilizes that run time to populate the objects and run data quality checks in the SnowFlake runtime.

I’ve been addressing unit testing in this stack. My team’s skill set more so pertains to SQL developers rather than DE, and even then their SQL is half baked at best. 

Unit testing within DBT is spotty at best, flat out doesn’t exist at worst. I’ve written a Python script that extends DBT to dynamically generate a test to every node, and allows for additional tests to be written and compiled/ran, within the CI/CD pipeline. This enables the team to continue using SQL, and the script will do the rest. If the written or generated test fails, the deployment fails.

Data is loaded to and read from the warehouse. I hoped to abstract from the warehouse, but SnowFlake having a closed source engine wasn’t conducive to that approach. My thought was to ingest the node sql files into duckdb and have an in memory database. Not the case given SnowFlake, but oh well.

What are your stacks and unit testing solutions?",False,0.97,https://www.reddit.com/r/dataengineering/comments/157tf13/what_is_your_unit_testing_implementation/,dataengineering
"Where do you find your DE job (recently, like within 1-2 years)",157x027,uniznoir,1690163793.0,,False,True,Career,25,False,18,"I'm looking for my first DE job, transitioning from DA. I'm just curious where did you look for a DE job posting. The current sites I'm following are Linked In and Indeed. Is there any other sites that specializes in jobs like SWE or DE? Maybe Stackoverflow? 

Thank you for your input",False,0.95,https://www.reddit.com/r/dataengineering/comments/157x027/where_do_you_find_your_de_job_recently_like/,dataengineering
"Self taught ""CTO/Data Engineer"" cofounder. Looking for feedback on architecture.",157yjwy,mcdunald,1690168215.0,,False,True,Help,42,False,13,"I'm a cofounder at a non-tech company in the sense that we don't have any client-facing software, just internal tools in the form of tableau dashboards and excel reports.

I did a web dev bootcamp years ago, but what I'm doing now has been learned from googling and Udemy over the years as problems emerged.

We're now in year 3 and I feel like this architecture I've hacked out, with 0 production experience in engineering, could use a review, especially as our operations scale.

1. AWS MySQL RDS database (db.t4g.xlarge), with read replica(db.t4g.large) for analytics. Currently at 100GB of data.
2. On the write side: Data is constantly written to the DB throughout the day, mainly through scheduled AWS Lambda functions that are making API calls (currently 10+ different API systems). 
   1. Other sources: S3 Uploads/SNS/Webhooks -> lambda triggers.
   2. Proxy endpoint is used to pool connections
   3. Write data is about 250MB a day
   4. Write used to be a bottleneck until I figured out proxy endpoints, so I think I'm good for now.
3. On the read side: I have about 40 tableau dashboards, with data extracts (some reused) that are refreshing daily/hourly from the read replica DB (db.t4g.large). 
   1. Some (full) refreshes can run for up to 5 minutes, which is fine at the moment. 
   2. Also have scheduled reports that run sql and convert to excel reports, to be emailed out.
   3. This is usually where I run into bottlenecks and cause for upgrading my db instance. I'll upgrade when my extracts take 10+ minutes to refresh, or if I start getting alarm notifications for CPU Utilization (80% and I'm looking to upgrade)
   4. I've optimized my sql to the best I could using indexing and other tricks, but a combination of joining 4 different tables on 3 keys each, with sorting and group by deduplicating, can push CPU compute to the limit.

While I still have room to upgrade my DB, I am a bit concerned as we are heading into growth stage, potentially 4x our current usage. While I know that vertical scaling is going to be fine for me, I do have to consider the cost implications, and I'm wondering if there are new tools and tech that I should be looking into, as I continue to improve on this architecture. ",False,0.81,https://www.reddit.com/r/dataengineering/comments/157yjwy/self_taught_ctodata_engineer_cofounder_looking/,dataengineering
Tool that keep documents offline,158abs5,uk_dataguy,1690204350.0,,False,True,Discussion,1,False,2,"Might be a dumb question - 

I have 1.5 hour commuting distance, and I would like to keep some of the API or Google Cloud docs on my iPad so that I can read them offline. (There's  a bad signal on the UK trains generally)  


I wonder if there's app to let me fiddle around the document offline so that I can use it as if I am accessing the documentations online",False,1.0,https://www.reddit.com/r/dataengineering/comments/158abs5/tool_that_keep_documents_offline/,dataengineering
Are Remote Data Engineering jobs still available in 2023?,1586upj,anemone_amenoma,1690194802.0,,False,True,Help,11,False,4,"Hi everyone,  
So recently I left my previous job as a Data Analyst at a big MNC as I wasn't really finding the projects I worked on as something that helped me learn and grow. Actually, I worked there for about a year and I didn't learn a thing.  
So I dropped the whole thing altogether and started studying for a Data Engineering role. After 3 months of studies and a couple of projects, I started applying for Data Engineering jobs. My first priority was to apply for global remote jobs but I can't seem to find any success and being a fresher in this field doesn't help my case either.   
Moreover, the packages in the country I live in for a data engineer role as a fresher are far lower than my previous package as a Data Analyst.  
So just wanted to connect with some people who might have some advice on my next steps and if someone is facing a similar issue, I would love to know your experience.",False,0.75,https://www.reddit.com/r/dataengineering/comments/1586upj/are_remote_data_engineering_jobs_still_available/,dataengineering
How to Prepare for a Data Analytic Engineer Interview in a Large Retail Industry,158bzov,datapopcorn,1690208277.0,,False,True,Interview,3,False,1," 

I recently got an interview opportunity for the position of a Data Analytic Engineer in a major retail industry. Below is the job description provided by the company:

Job Description: The position is a combination of a Data Analytics Engineer and a Technical Project Manager under the Data team of the IT department.

Responsibilities include working primarily with SQL, Python, BI tools, and Google Cloud Platform to maintain the daily operations of the Data Warehouse and its data applications. This involves tasks such as data problem identification, data retrieval, data analysis, and application development.

During data-related projects, the role involves providing technical assistance and overseeing project quality.

Preferred Tools: MS SQL, Python, AWS, Google Analytics

Required Skills: No specific requirements Other Qualifications:

* Advanced SQL query skills, experience with BigQuery is a plus.
* Python data analysis and experience with machine learning libraries or other third-party machine learning services.
* Practical experience with GCP or AWS/Azure cloud services.
* Problem-solving abilities, including the ability to identify issues, propose constructive solutions, and implement them effectively.
* Enthusiasm for exploring the potential impact of data, actively learning, and generating ideas.

The interview process consists of a 60-minute online test and a 90-minute oral interview. I would like to ask for any advice on how to prepare for this interview, such as:

1. Topics covered in the online test (specific SQL or Python syntax to be used).
2. Types of questions the interviewer may ask during the oral interview.
3. Questions I can ask during the interview to demonstrate my interest and knowledge.",False,0.6,https://www.reddit.com/r/dataengineering/comments/158bzov/how_to_prepare_for_a_data_analytic_engineer/,dataengineering
Newb question: how can I think about fivetran vs AWS glue?,158bwpj,poopbrainmane,1690208087.0,,1690212362.0,True,Discussion,6,False,0,"Can somebody walk me through the difference in implementing AWS glue as the data transfer layer vs fivetran?

What are you rolling on your own when using glue vs what you get out of the box when using fivetran?

What do you lose control over with either of them?

 pros/cons of each?

Sorry for the newb question but I’m having trouble getting a straight answer via my Google research",False,0.43,https://www.reddit.com/r/dataengineering/comments/158bwpj/newb_question_how_can_i_think_about_fivetran_vs/,dataengineering
Tableau for automated analyzation and dumping that into some database.,1586jvw,Key_Consideration385,1690193883.0,,1690200458.0,True,Discussion,27,False,1,"So I want to know if I can achieve this or not?

I wanna get data from some database, analyze it (should be automated) and dump that automated output data to some other database.Does Tableau allow me to do this? If so, how?

So that I can take that dumped data into my database and visualize it using my own React view.

Update : How about Tableau PREP do this work? I'm seeing some promising expectation from that.

PS : I'm a newbie, so please consider helping me out before attacking I guess!",False,0.55,https://www.reddit.com/r/dataengineering/comments/1586jvw/tableau_for_automated_analyzation_and_dumping/,dataengineering
Does anyone have experience building out a data warehouse from scratch?,157sk9t,biga410,1690151763.0,,False,True,Help,17,False,16,"Hi,

Ive recently been tasked to lead the development of a new data warehouse at my company. The problem is, while I am experienced in the field, ive never worked on the more backend components (ive been more of an analyst/analytics engineer) and could use some guidance on how to structure and approach this whole process, if youd let me pick your brain a bit, or perhaps theres a better forum for this somewhere?",False,0.94,https://www.reddit.com/r/dataengineering/comments/157sk9t/does_anyone_have_experience_building_out_a_data/,dataengineering
Do you use Snowflake with DBT?,158aza3,sync_jeff,1690205934.0,,False,True,Discussion,2,False,1,"I am curious what % of people here use DBT with Snowflake. I know it's popular, but I'm curious to see the numbers!

[View Poll](https://www.reddit.com/poll/158aza3)",False,1.0,https://www.reddit.com/r/dataengineering/comments/158aza3/do_you_use_snowflake_with_dbt/,dataengineering
Is Data Engineering a Complex Career? Challenges Faced by Data Engineers,158ao7m,Emily-joe,1690205204.0,,False,False,Blog,0,False,0,,False,0.5,https://losanews.com/is-data-engineering-a-complex-career-challenges-faced-by-data-engineers/,dataengineering
Scope creep in DE?,157lwnh,jonesaphore,1690136121.0,,False,True,Discussion,22,False,32,"I'm currently in the market for a DE position, couldn't sleep last night and was reflecting on interviews I've had so far. In 4+ cases, the hiring manager has specifically asked how I  would use data to increase sales. I completely understand that this role requires being a liaison to every department but how far does/should that go?

This isn't verbatim, but one conversation went like this:

Hiring Manager (HM): ""Say for instance, we had a new director of marketing. How would you build a dashboard to help their department?""

Me: ""I would meet with the director to understand what metrics are most important for their team.""

HM: ""What if the director didn't know what metrics they should be looking at.""

Me: ""Then I would ask what goals the department has and work backwards from there. Additionally,  I would prepare some visualizations for data exploration that we could review together. 

HM: ""What if the director didn't have any goals?""

At this point, I was thinking I wouldn't want to work at a company without any leadership from someone in a director role. I think I understand what he was getting at, but it was still awkward. Is this equivalent to asking a prospective hire in sales this:

HR: ""How would you design a data pipeline that incorporates salesforce and hubspot data?""

Sales Hire (SH): ""I would work with the analytics engineering team, outlining what I need the data for.""

HR: ""What if the analytics engineering team didn't know how to design a pipeline?""

SH: ...

Another example of a question I got verbatim for a DE position:

HR: ""How should we increase air conditioner sales?""

Me (Never having sold an air conditioner): ""I would assume that air-conditioner sales are driven by new home construction. I would identify areas that are experiencing growth/new development and focus marketing there.""

Maybe it's just this market, but I feel like I keep flubbing these questions because I don't have a degree in marketing. ",False,0.96,https://www.reddit.com/r/dataengineering/comments/157lwnh/scope_creep_in_de/,dataengineering
Where does DE belong?,157j46z,Sad-Fail-5337,1690129459.0,,False,True,Discussion,20,False,21,"Long story short, I run a data team at a 100 person company that includes Data Science and Data engineering. Unexpectedly, I find myself fending off a hostile takeover from a leader on the engineering team, who is declaring that data engineering needs to be moved in with the engineering org. I won’t give any more details because everybody reads Reddit nowadays, but I am curious for people’s opinions here.  I have my own, of course, but would like to hear from you all. 

What are the pros and cons of having the data engineering function on the same team as data science, versus having them separate?",False,0.89,https://www.reddit.com/r/dataengineering/comments/157j46z/where_does_de_belong/,dataengineering
"Fluke, a simple API to object storage and message queues in the cloud",1584n2p,WerdenWissen,1690187741.0,,False,True,Open Source,0,False,1,"A while ago I posted about Fluke, an open-source project of mine in Python that could be used as a higher-level API in order to interact with objects in the cloud as if they were files within an ordinary filesystem. With Fluke 0.4.0 just being released, message queues have been added into the mix, making it extremely easy to accomplish various tasks that previously needed a ton of boilerplate code.

For example, we can constantly poll a message queue for messages, parse these messages in order to extract the path of an object in some S3 bucket, and then transfer these objects to some remote server, all in just a few lines of code!

    from fluke.auth import AWSAuth, RemoteAuth
    from fluke.queues import AmazonSQSQueue
    from fluke.storage import AmazonS3Dir, RemoteDir
    
    # This object will be used to authenticate
    # with AWS.
    aws_auth = AWSAuth(
        aws_access_key_id=""aws_access_key"",
        aws_secret_access_key=""aws_secret_key"")
    
    # This object will be used to authenticate
    # with the remote machine.
    rmt_auth = RemoteAuth.from_password(
        hostname=""host"",
        username=""user"",
        password=""password"")
    
    with (
        AmazonSQSQueue(auth=aws_auth, queue='queue') as queue,
        AmazonS3Dir(auth=aws_auth, bucket='bucket') as bucket,
        RemoteDir(auth=rmt_auth, path='/home/user/dir') as rmt_dir
    ):
        for batch in queue.poll(polling_frequency=60):
            for msg in batch:
                path = parser_fun(msg)
                bucket.get_file(path).transfer_to(dst=rmt_dir)

I'll be happy to respond to any feedback/questions you may have.

\- PyPI: [https://pypi.org/project/fluke-api/](https://pypi.org/project/fluke-api/)

\- Github: [https://github.com/manoss96/fluke](https://github.com/manoss96/fluke)

\- Docs: [https://fluke.readthedocs.io/en/latest/index.html](https://fluke.readthedocs.io/en/latest/index.html)

&#x200B;",False,0.67,https://www.reddit.com/r/dataengineering/comments/1584n2p/fluke_a_simple_api_to_object_storage_and_message/,dataengineering
"Introducing Prism: A Novel, Open-Source Data Orchestration Software. Feedback needed!",157vab7,runprism,1690158966.0,,False,True,Open Source,0,False,5,"Hey Reddit community!

I'm thrilled to introduce you to Prism, an innovative open-source data orchestration platform. We've been working tirelessly to develop this tool, and now we're excited to invite you all to be a part of our alpha testing phase.

🔗 Website: [https://runprism.com/](https://runprism.com/)

📢 GitHub Repository: [https://github.com/runprism/prism](https://github.com/runprism/prism)

📄 Documentation: [https://docs.runprism.com](https://docs.runprism.com/)

# What is Prism?

Prism is a powerful yet user-friendly orchestration platform designed to streamline your processes, boost productivity, and enhance collaboration across teams. Whether you're a developer, sysadmin, data analyst, or any professional who deals with workflows, Prism has something valuable to offer.

# Key Features

**🔹** **Workflow Automation**\*\*:\*\* Real-time dependency declaration: With Prism, analysts can declare dependencies using a simple function call. No need to explicitly keep track of the pipeline order — at runtime, Prism automatically parses the function calls and builds the dependency graph.

**🔹 Intuitive logging:** Prism automatically logs events for parsing the configuration files, compiling the tasks and creating the project, and executing the tasks. No configuration is required.

🔹 **Flexible CLI:** Users can instantiate, compile, and run projects using a simple, but powerful command-line interface.

**🔹 Extensive Plugin Library:** Enjoy a rich collection of integrations with your favorite tools and services.

**🔹 “Batteries included”:** Prism comes with all the essentials needed to get up and running quickly. Users can create and run their first project in less than 2 minutes.

# How to Get Involved

By joining our Alpha testing phase, you have the unique opportunity to be among the first users to experience Prism in action. Your invaluable feedback will directly impact the development of this platform, helping us make it even better, more stable, and tailored to your needs.

Visit our website [https://runprism.com](https://runprism.com/) to learn more about the platform and its features. In addition, check out our documentation at [https://docs.runprism.com](https://docs.runprism.com/) to get started right away!

Access the GitHub repository [https://github.com/runprism/prism](https://github.com/runprism/prism) to view the source code, report issues, and contribute to the project.

Try out Prism in your own workflow environment and let us know what you think!

We highly encourage you to share your thoughts, suggestions, and bug reports with us. Feel free to post your feedback directly in this thread, or if you prefer, you can raise issues on GitHub. Your input is invaluable to us, and together, we can shape Prism into the go-to platform for data workflow orchestration.

# Who We Are

Our team is a group of passionate data engineers, scientists, and developers. We're open-source enthusiasts who believe in the power of community-driven software, and we're committed to making Prism an exceptional orchestration tool and fostering an inclusive environment for all contributors.",False,1.0,https://www.reddit.com/r/dataengineering/comments/157vab7/introducing_prism_a_novel_opensource_data/,dataengineering
How you switched from data engineering to software Backend development?,157fm9c,Maleficent_Bad5837,1690120905.0,,False,True,Career,13,False,26,"As the title, has anyone made the switch from data engineering to software dev (backend)? How was the transition (outside of internal switch)? What steps did you take to achieve this (new programming language? side projects?  Leetcode?). Any pointers on where to find side projects to collaborate with experienced backend engineers? How has this switch affected progression in your career? What level did you apply for (junior? Mid?) How’s the learning curve on starting the first SWE job?

Background: I am a mid level data engineer who enjoys solving problems writing code but have found that DE domain offers less opportunities for me to improve my coding skills. I primarily use Python, SQL and spark in my day to day but feel like I could do more with Python, SQL and other languages. In my spare time, I practise by taking on leetcode challenges and reading on backend architecture and design and find it very interesting. I have also created some personal fullstack projects in the past and looking to learn better by collaborating with more experienced backend engineers on projects before applying for SWE roles.

Has anyone had this experience and how did they successfully make the switch? Any recommendations will be highly appreciated.",False,0.94,https://www.reddit.com/r/dataengineering/comments/157fm9c/how_you_switched_from_data_engineering_to/,dataengineering
Rant-Microsoft Fabric is most annoying POS I have ever used,1575ma8,boogie_woogie_100,1690088891.0,,False,True,Discussion,67,False,61,"Seriously, I don't know where to begin. It's so annoying. I am not understanding the UI part of Microsoft fabrics. It's so click heavy. I am completely lost where to start and where to end.",False,0.87,https://www.reddit.com/r/dataengineering/comments/1575ma8/rantmicrosoft_fabric_is_most_annoying_pos_i_have/,dataengineering
Packing n querries into 1,157yia1,ankurcha,1690168080.0,,False,True,Help,0,False,1,"Hi all, I am looking for some advise. I am designing a ""batch data generation pipeline"" that is essentially a batch execution version of a this problem (for now a spark job):

1. User provided id, query parameters, output location.

2. Query a catalog dataset that represents a large list of files and filter the ones that matches exactly the ones needed to produce the output.

3. Execute a set of transformation (groupby, sort, etc) that take output of #2 and produces some output dataframe.

4. Write to output location in some custom format.

So, I have N instances of the same problem with different (possibly overlapping) inputs. I want to build an efficient way to structure this spark job(s) so that the overhead is low. 

Option 1: build a spark job that submits n jobs with different inputs in parallel or in batches with no shared steps (except the spark context)

Option 2: build a spark job that does a sort of dataframe union before submitting it all as a single bigger job.

Option 3: ???

Has anyone done something like this? In terms of scale size - the step 1 (list of files) is about ~5TB and the input used to generate the data is ~10GiB output. The N in the batch is adjustable but I think 100 at a time is probably a minimum point.",False,1.0,https://www.reddit.com/r/dataengineering/comments/157yia1/packing_n_querries_into_1/,dataengineering
What are some common data engineering mistakes you’ve seen in your career?,1570ujm,SeriouslySally36,1690074320.0,,False,True,Discussion,105,False,88,Related to any stage of the process. Or anything at all?,False,0.97,https://www.reddit.com/r/dataengineering/comments/1570ujm/what_are_some_common_data_engineering_mistakes/,dataengineering
Observable Plot App,157tbjy,SirLagsABot,1690153722.0,,False,True,Discussion,0,False,1,"Does anyone here use [Observable Plot](https://observablehq.com/plot/) for visualization? I’m a software dev and data engineer (background in analytics) by day and indie hacker/software bootstrapper by night. I recently found Observable Plot, and I like how it’s a more simplified D3 library without the extreme complexity of D3. As a fan of analytics, I was thinking it would be cool to have an app or platform powered by Observable Plot. Does anyone use it? Do you find it useful? I think it would be fun to build something for it one day.",False,1.0,https://www.reddit.com/r/dataengineering/comments/157tbjy/observable_plot_app/,dataengineering
Rust for data engineering?,157mn03,mrlmld,1690137835.0,,False,False,Discussion,1,False,2,found this on substack and reposting here for more reach.,False,0.57,https://i.redd.it/o908zbyifrdb1.jpg,dataengineering
Help with Pyarrow behavior - .write_to_dataset,157mlmc,bigfishindoggytown,1690137738.0,,False,True,Help,0,False,2,"Hello, I'm fairly new to Pyarrow and parquet files, but have experience with Python.

I'm working on a codebase that processes daily data. We have a Pyarrow Table with 3 columns, one of which is a date. The data goes back to 2007, so ~ 5800 unique dates. The table has 43 million rows, so avg of ~7500 rows per date. The output should be a parquet dataset, partitioned by the date column. So you have an folder with ~5800 folders, named by date. Each folder should contain a single parquet file.

The repo switches between pandas dataframes and pyarrow tables frequently, mostly pandas for data transformation and pyarrow for parquet reading and writing. The internal processing can be updated, but to work with our other processes, the output needs to remain as daily parquet files.

This has worked for a long time until we tried updating the Pandas and Pyarrow version, for performance improvements. We went Pandas 1.2 -> 2.0, and Pyarrow 2 -> 12. The original code, that worked:

    pq.write_to_dataset(table, root_path=str(output_filepath),
                        partition_cols=['action_date'],
                        allow_truncated_timestamps=True)

Now, with updated pandas and pyarrow on that code, I get the error:
    pandas_to_parquet Fragment would be written into 4141 partitions. This exceeds the maximum of 1024

I researched and found that pyarrow's behavior changed after version 4.0. It defaults to 1024, and must be set higher. Since I know there's 5800 partitions needed, growing every day, I set it to 7000.

    pq.write_to_dataset(table, root_path=str(output_filepath),
                        partition_cols=['action_date'],
                        allow_truncated_timestamps=True,
                        use_legacy_dataset=False,
                        max_partitions=7000)

This took significantly longer to run, many times over. The output folders were correct, but inside each was many parquet files, each only about 2kb. Previously there would be a single parquet, around 20 to 80kb.

I'm trying to figure out how to ensure a single parquet file per daily folder, and have similar or better performance to the previous version with older pandas and pyarrow. The original in memory table I'm trying to write to a parquet dataset is about 280mb. This runs on a larger EC2 instance, AWS Linux.



TLDR: I updated Pyarrow from 2.0 to 12, and am trying to make the write_to_dataset method function as before, with at least comparable performance. 

Thank you for any insight or help here, it's very much appreciated!",False,1.0,https://www.reddit.com/r/dataengineering/comments/157mlmc/help_with_pyarrow_behavior_write_to_dataset/,dataengineering
Improving at Command line,157ke6v,machinegunke11y,1690132544.0,,False,True,Help,5,False,1,"We have airflow on WSL but are otherwise a Windows shop. What sorts of activities would be good practice for improving my command line knowledge? Or what sorts of command line activities are most useful/common for data engineering?

I am very much a learn by doing kind of person. I have to seek out ways to transform whatever I am working on to incorporate what I  am interested in learning, even if it isn't necessary. So identifying what I should be thinking to try in command line will go a long way. Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/157ke6v/improving_at_command_line/,dataengineering
Is there any AI tools / plugins to check data accuracy and data structure ?,157b6f7,Ok_Pick_8431,1690107615.0,,False,True,Discussion,10,False,3,"Data reporting business is generally needs a lot of humans involvement to ensure the quality of data, any traction for AI in this space & recommendations please",False,1.0,https://www.reddit.com/r/dataengineering/comments/157b6f7/is_there_any_ai_tools_plugins_to_check_data/,dataengineering
"To those who are self-taught coders/data engineers, how did you study?",156sgv3,Weary-Individual-309,1690053001.0,,False,True,Help,38,False,38,"In college a lot of time I spent taking handwritten notes and then doing practice exams or problem sets for economics, but I feel like I need a better way to study since I’m self teaching when not working.

My questions is

- is it even worth taking handwritten notes when coding or should I just read and try to do a problem right away 
- is there any time I should taken handwritten notes? Like maybe definitions for OOP, or data structures?

I’m just feeling pretty dumb because I’m going through a Python book and I feel like it’s taking me forever to get through using my old study habits.",False,0.97,https://www.reddit.com/r/dataengineering/comments/156sgv3/to_those_who_are_selftaught_codersdata_engineers/,dataengineering
How to covert interviews to job offers?,157plwk,umermd,1690144697.0,,False,True,Interview,9,False,0,"With 4 years of experience in the field, I have mostly worked on Spark, Python, Nifi, Airflow, AWS and Azure.
I moved to Dubai to look for better opportunities however after multiple rounds of 4-5 interviews in the last 2 months, I am unable to convert into a job offer.
How to prepare for DE interviews and any resources which can help in the preparation of interviews?",False,0.38,https://www.reddit.com/r/dataengineering/comments/157plwk/how_to_covert_interviews_to_job_offers/,dataengineering
DP 203 Vs databricks DE associate,157e9t5,Leonjy92,1690117215.0,,False,True,Career,3,False,0,"
Hi everyone,

I am planning to switch  from data analytics/BA to data engineering.

My current skills include SQL (select statements), python pandas, power BI(data modelling and dashboarding), Azure data factory and spark using databricks.

I've just got my DP 900 certificate and contemplating completing either dp203 and/or databricks DE associate.


I will also be picking up docker with airflow and Kafka.

Which of the two certificates mentioned will be better for me ?

Thanks in advance for your advice.",False,0.5,https://www.reddit.com/r/dataengineering/comments/157e9t5/dp_203_vs_databricks_de_associate/,dataengineering
Polars vs Pandas. Inside an AWS Lambda.,156yqjp,DarkClear3881,1690068587.0,,False,False,Blog,1,False,12,,False,0.93,https://www.confessionsofadataguy.com/polars-vs-pandas-inside-an-aws-lambda/,dataengineering
I want to be able to programmatically integrate dbt into an end-to-end pipeline in python. Is this the wrong usage of dbt?,156v3fo,NFeruch,1690059483.0,,1690059713.0,True,Discussion,18,False,14,"Essentially what the title says. I have an end to end pipeline that at a high level is 
* extract data
* do some python transformations
* export data  
  
I want to add a 4th step of  
  
* run the relevant dbt models

The thing is that I’m running into problems with invoking it programmatically, as the pipeline file is in a different directory than the dbt project (so it doesn’t detect the project yaml file) and I can’t use absolute paths to specify which model to run, which I need to do because the pipeline is in a different directory

This is for a personal project, so I am trying to implement dbt just for the sake of learning it. The thing is though, that the pipeline runs automatically on a cron job, and I need the model to be run at the time of the pipeline

Is this the wrong usage pattern for dbt? I suspect it is because it’s not straightforward to do so far",False,1.0,https://www.reddit.com/r/dataengineering/comments/156v3fo/i_want_to_be_able_to_programmatically_integrate/,dataengineering
AWS Glue Crawler Exclude pattern,156usm1,random_name_362,1690058735.0,,1690060211.0,True,Help,4,False,4,"Hello everyone i have a question with Glue Exclude pattern, really appreciate if anyone can help me

I'm having an S3 bucket that contains many csv files in same folder and i want to run Crawlers on that folder to create some tables. The S3 bucket structure:

* bucket\_name/folder\_name
   * xxx-apple-xxx-1.csv
   * xxx-apple-xxx-2.csv
   * xxx-orange-xxx-1.csv
   * xxx-strawberry-xxx-1.csv
   * xxx-chicken-xxx-1.csv
   * xxx-chicken-xxx-2.csv
   * xxx-dog-xxx-1.csv
   * xxx-cat-xxx-1.csv
   * and many more....

And i want to create Crawler to create 1 table for each category, such as 1 table for apple, 1 table for orange (1 crawler --> 1 table)..... And the number of categories is over 100. I see that Glue Crawler has a property, name [Exclude](https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude) pattern (but i do not see the Include pattern property) that we use filter out files when running crawler.

Now i want to create a Crawler for apple, so it should ignore all files except \*apple\* ones. I've tried with the Exclude but not achieve this yet. Do anyone know how to do this ?

Crawler name: apple\_crawler

S3\_path: bucket\_name/folder\_name

Exclude: i have tried things like: \*\[!{apple}\]\* or \*!(apple)\* or \*\[!apple\]\* but it did not work

&#x200B;

Thank you !",False,0.84,https://www.reddit.com/r/dataengineering/comments/156usm1/aws_glue_crawler_exclude_pattern/,dataengineering
Real-Time Twitter Data Acquisition for Sentiment Analysis: Seeking Alternatives to Tweepy,156zpo3,Kratos_1412,1690071181.0,,False,True,Help,4,False,2,"I'm looking to obtain real-time data from Twitter to conduct sentiment analysis using Spark Streaming. However, I've encountered difficulties while attempting to retrieve this data through Tweepy. After some research, I believe the issue might be related to limitations with Tweepy's free access. Could you suggest alternative methods or approaches to acquire real-time data without relying on Tweepy or any other alternatives to it?",False,0.75,https://www.reddit.com/r/dataengineering/comments/156zpo3/realtime_twitter_data_acquisition_for_sentiment/,dataengineering
Use cases for LLMs on structured data (sourced from databases and streaming data),156ltst,anupsurendran,1690036730.0,,1690074186.0,True,Discussion,11,False,10,"In one of our use cases, a ""compliance alert"" must be delivered depending on guidelines found in a collection of documents.  A transactional database houses the criteria data (triggers), and some event data is streamed.

a) Does anybody have a **similar use case**?

b) Does anybody have use cases around data pipelines for LLMs and structured data sources?  If so how do you deal with the following :

1. **Context mismatch** \-  since enterprise databases have very specific schemas and terminology that the LLM is unaware of, how do you prevent misunderstanding?
2. **Entity ignorance -**  entities are a relational db terminology but how do you get that into the LLM world?
3. **Compositionality** \- LLMs lean towards a holistic view, and our database queries require hierarchical, compositional views. How do you deal with this?

Not to mention other performance-related issues ...",False,0.92,https://www.reddit.com/r/dataengineering/comments/156ltst/use_cases_for_llms_on_structured_data_sourced/,dataengineering
Need help with automation and alerts in AWS,156nl1s,fun_coordinator,1690040960.0,,False,True,Help,8,False,5,"We use AWS at my work for ETL to work with small datasets (~5000 rows). We have multiple pipelines with the following setup.

1. External systems send files using SFTP to S3 raw files folder.
2. Lambda gets triggered and moves files to staging folder in S3.
3. Glue job gets triggered and transforms the data and moves transformed file to transformed folder in S3.
4. Lambda gets triggered and uploads data to Salesforce.

The painful part about all of this is that all AWS resources are created manually. All the S3 folders, lambdas, roles, glue jobs, SFTP, etc. There really is no way easy to see if a lambda or glue job failed. I usually look at the logs after the failure to find out if they failed. My question to you guys is, how do I automate this pipeline? Also, how do I get alerted when a lambda or glue job fails?",False,0.86,https://www.reddit.com/r/dataengineering/comments/156nl1s/need_help_with_automation_and_alerts_in_aws/,dataengineering
Modern Data Engineering and the Lost Art of Data Modelling,156ftu3,aidantcooper,1690019424.0,,False,False,Blog,0,False,14,,False,0.95,https://www.aidancooper.co.uk/the-lost-art-of-data-modelling/,dataengineering
Data analyst/engineer at Tesla,15614vp,buianhthy1412,1689975689.0,,False,True,Interview,94,False,84,"I just had 20 minutes interview (1st) with Tesla on a role called data analyst/engineer, which requires these skills below. I was asked right off the bat some technical questions without giving me chance to introduce myself. I was asked what confusion matrix is and I couldnt pull out from my brain what they are. I know it's very basic but I wasn't prepared. I told her I came in with DE readiness so they asked me on DDL, how to drop a column (I swear I never had to drop a column but I manage to give an answer that works lol). This interview makes me feel so rushed from their end and at the same time I feel underqualified.😭

What You’ll Do
Create and/ or enhance action-driven dashboards (e.g., using Tableau). 
Support ad hoc data, SQL query, analysis, and debugging requests. 
Create and maintain an optimal database schema and data pipeline architecture. 
Create ETL pipelines in Airflow for analytics team members that assist them in building and optimizing their reports. 
Communicate with stakeholders, gather business requirements, and brainstorm KPIs. 
Develop/ maintain internal documentation. 
Proficiency in SQL, and comfort with a scripting language (e.g., Python) is a plus. 
Proficiency with a data visualization tool (e.g., Tableau). 
A good understanding of relational databases and database engineering concepts. 
Familiarity with data pipelines and a Workflow Management Tool (e.g., Airflow) is desirable.",False,0.94,https://www.reddit.com/r/dataengineering/comments/15614vp/data_analystengineer_at_tesla/,dataengineering
Data importation issues,156ogaf,GhostPrime3111,1690043067.0,,False,True,Help,0,False,2,"Importing files into mysql using mysql command-line client. How do nk I modify the 'secure-file-priv' option to an empty string?.

I'm unable to import files directly into mysql using the Data Import Wizard",False,1.0,https://www.reddit.com/r/dataengineering/comments/156ogaf/data_importation_issues/,dataengineering
Data Engineering Course,156aq9z,sankextend,1690002264.0,,False,True,Help,12,False,13,"Hi everyone, I'm working a non-it job, having a CS Bachelor's degree from a tier 3 college. I am looking to switch to a data engineer role. Please suggest a complete course/s for data engineering where I can begin learning from the basics.

&#x200B;

Note: I have a basic understanding of Python and access to Udemy through my company portal.

&#x200B;

Any help is much appreciated😇

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/156aq9z/data_engineering_course/,dataengineering
Certified Data Engineer Offering Learning Roadmap and Technology Stack - Seeking Community Feedback,156qo29,lSniperwolfl,1690048500.0,,False,True,Help,5,False,1," Hello everyone,

I wanted to express my gratitude to this subreddit for helping me become a certified Data Engineer. It feels amazing to have achieved this milestone with your support.

Recently, my company approached me to create a learning roadmap for aspiring Data Engineers. The goal is to provide newcomers with a clear and structured learning path to follow. Understanding that it can be overwhelming to determine which skills to develop and the right order to do so, I have carefully designed a learning roadmap that consists of recommended courses to take, arranged in a specific order.

Below, you'll find the learning roadmap I've created:

[Here it is](https://imgur.com/oKh3S4r) 

Furthermore, I've compiled a stack of technologies that the company should prioritize and gain experience with for the Data Engineer and Data Architect department.

I would greatly appreciate your feedback on this roadmap. If you feel that any modifications, additions, or removals would be beneficial, please let me know.

Thank you all for your continuous support and contributions to this community.

&#x200B;

 **Relevant Technologies**

&#x200B;

|GCP|
|:-|
|Azure|
|AWS|
|Pentaho|
|SQL|
|Python|
|Spark|
|DBT|
|Airflow|
|Databricks|

&#x200B;",False,0.67,https://www.reddit.com/r/dataengineering/comments/156qo29/certified_data_engineer_offering_learning_roadmap/,dataengineering
My first article focused on the field of data engineering,156qfc5,adabbledragon85,1690047884.0,,False,False,Blog,0,False,0,Machine learning in data engineer,False,0.4,https://medium.com/@thiago2002sr/machine-learning-in-data-engineering-turning-data-into-precious-knowledge-201224922a3c,dataengineering
Is a MacBook m1 air with 16gb of memory good enough to learn data engineering?,1568qwa,Weary-Individual-309,1689996097.0,,False,True,Help,39,False,13,"I am about to embark on learning the command line, and python (already know SQL) before looking at data talks zoomcamp. 

I haven’t my own personal computer in years, and I’m leaning towards M1 Mac air but I heard it has some issues with python libraries, docker (and other virtualization softwares) - self-teaching is hard enough (while working full-time) will this derail my learning experience?",False,0.7,https://www.reddit.com/r/dataengineering/comments/1568qwa/is_a_macbook_m1_air_with_16gb_of_memory_good/,dataengineering
"Dont like Cloud, but DE",15673lv,binnsyboysg,1689991290.0,,False,True,Career,13,False,14,"I really like coding, building ETL's, plain the overall architecture, building it but the Devops/Cloud part is smth thay i really dont enjoy that much.

Is there a way to be smth like ETL dev or software developer for data intentensive apps?

I wouldn't mind doing it if its only like 10%-20% of the job. In my current job is only like 1%

I dont want to be 40/50% of time configuring stuff at cloud.

Thx!!?",False,0.79,https://www.reddit.com/r/dataengineering/comments/15673lv/dont_like_cloud_but_de/,dataengineering
At what point do you give up on training a co-worker?,155rksi,recentcurrency,1689954096.0,,1689954511.0,True,Discussion,95,False,72,"I have a jr co-worker I have been trying to train and enable to reduce work on my plate.

However it has been almost a year, and despite multiple documentation guides, live enablement sessions, working groups, and daily training sessions, I have not seen this teammate actually improve.

It to be quite frank has gotten to a point where it is more time-saving if I just do the work for him. Something I have been forced to do b/c they continually make the same errors over and over again.

At what point do I just give up on this Jr Employee and throw in the towel here?

Also, not sure if this is helpful, but this Jr Employee is an off-shore resource. But the training techniques I have used has worked for other off-shored an on-shored Employees with similar profiles

&#x200B;",False,0.98,https://www.reddit.com/r/dataengineering/comments/155rksi/at_what_point_do_you_give_up_on_training_a/,dataengineering
What Counts As a Product-led Digital Tech Company?,156k1ma,Senior_Anteater4688,1690032123.0,,False,True,Help,7,False,1,"I'm currently working for a big energy company in the UK, and was thinking of switching my visa to the global talent visa after getting endorsed by [technation](https://technation.io/). One of their requirements state that you must have experience working in a product-led digital tech company and have recommendations from people working there.

Now I am confused about what counts as a 'product-led digital tech company'. I work in the information systems department of my present company, tasked with making the whole data infrastructure for all their data science, analytics and machine learning workloads. My company is not a service based company for technology, but their main business driver is exploration/production of oil/gas and carbon capture.

I work on the tech side of things for them and helping them reach their net zero targets. Is this experience not counted as working for a 'product company'?",False,0.67,https://www.reddit.com/r/dataengineering/comments/156k1ma/what_counts_as_a_productled_digital_tech_company/,dataengineering
Stored procs vs sql statements for your ETL,1568i2g,Exchange_Neat,1689995376.0,,False,True,Discussion,6,False,7,"Hey everyone, 

I recently took up a contract role with a startup where they are using stored procedures in their airflow DAGs instead of standard SQL statements like create/insert/truncate/drop statements. These stored procs are not reusable they have tens of stored procs for each transformation. 

 I can see an obvious downside for debugging and scaling these pipelines, every time you want to push a change to the stored procedure you might have to go into the DB and make that change instead of using the git/airflow route. And it beats the purpose of a stored procedure they are not generic/reusable. What are some of the other downsides/upsides of this implementation? I am curious to know if this is a norm in some companies. 

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/1568i2g/stored_procs_vs_sql_statements_for_your_etl/,dataengineering
Learning to implement SWE practices as a newbie,156ifdd,,1690027664.0,,False,True,Discussion,0,False,1,"So I have a traditional BI and data analyst background, I’ve done some DE but not full time. Recently I completed a data engineering certification, working with team members to develop a working data pipeline from an external api to redshift using AWS glue, AWS Athena, and redshift.

I understand basics and have good chops in technicals. If I want to continue my personal growth and keep developing my personal projects, at what point do I try to introduce SWE practices or tools into my skill set? 

I know of docker/K8s for containerizations or CI/CD using GitHub or AWS code commit,or even a little about IAC using Terraform, but it’s a big difference between knowing when and where to implement these features in an actual pipeline.",False,1.0,https://www.reddit.com/r/dataengineering/comments/156ifdd/learning_to_implement_swe_practices_as_a_newbie/,dataengineering
Test cases to try on a delta table in databricks,156dy2q,theaitribe,1690012909.0,,False,True,Help,13,False,2," I have been asked to test a delta table inside databricks and i'm running in short of ideas to test case them.

Other than basic sanity checks like nulls - dupe check - count match. Are there are any creative test cases that i can use over my tables in databricks? We are doing an sap to databricks data transfer which updated the records based on scd-1. I'm not able to think much of a test case to try on with.

Any help/resources highly appreciated!",False,1.0,https://www.reddit.com/r/dataengineering/comments/156dy2q/test_cases_to_try_on_a_delta_table_in_databricks/,dataengineering
Will do Data Engineering in a Data Scientist Internship,156gy2r,lavadao,1690023157.0,,False,True,Help,3,False,0,"Hello everyone.

&#x200B;

I got an internship in data science but I was told I was going to do some data engineering as well.   
I come from a math background, so my programming skills are not as strong as someone from computer science or something like that.

However, I got some time before I start and asked what I could learn before starting and they gave me this list:  


* ETL Processes
* MQTT
* Kafka
* Airflow
* Cube
* Superset

&#x200B;

Is there any youtube, course or book that covers all of this?  


Thanks a lot for the help.  
",False,0.5,https://www.reddit.com/r/dataengineering/comments/156gy2r/will_do_data_engineering_in_a_data_scientist/,dataengineering
First DE Interview,155tjt1,Sea_Fill6043,1689958496.0,,False,True,Interview,14,False,19,"For context I’m currently an undergraduate student studying SE. The position I will be interviewing for is a DE internship position for a large electronics and semiconductor company based in Tokyo. My interview will be conducted on teams directly with a Senior Data Engineer (the person who emailed me regarding my application), and a colleague of his. I was given a 3 day notice to setup the interview. 

I wanted to know what exactly should expect in this first round? I assume it could be a variety of technical questions and some behavioral. I was thrown off by the fact that they wanted to conduct an interview on such short notice and that they think of themselves as project owners and that I could dabble into any segment of the stack (UI, backend, etc) or I can ask the person leading that part of the stack for help. Because of this, I don’t really know how to prep. I’m quite nervous and usually do very poorly in interviews as is. I appreciate any advice and help you can give!",False,0.95,https://www.reddit.com/r/dataengineering/comments/155tjt1/first_de_interview/,dataengineering
Does anyone maintain a pipeline that has a good mix of Polars and Spark?,156dhxt,ddanieltan,1690011379.0,,False,True,Discussion,1,False,1,"My legacy code base is pretty much Spark-focused (mix of Pyspark and Scala Spark). We have a few Python UDFs but for performance reasons, we try to avoid udfs wherever possible. Lately, I'm pretty impressed by Polars and I'm wondering if anyone has started to mix in Polars (either for its expressive API or ability to handle larger than RAM data on a single node) with an existing Spark cluster. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/156dhxt/does_anyone_maintain_a_pipeline_that_has_a_good/,dataengineering
"Barbenheimer, Data Engineering edition",1553rxb,Top-Substance2185,1689888296.0,,False,False,Meme,18,False,407,,False,0.96,https://i.redd.it/tw33gopht6db1.png,dataengineering
Automating a Password Reset System - Is this a good approach? (AWS),155rdr0,Funny_Duck2429,1689953654.0,,False,True,Help,7,False,5,"I've started working as a DE in a new project automating some DBA tasks. 

I've been tasked with implementing a 180 day password reset policy for users on AWS Redshift. When there password reaches 180 days old, it should be reset automatically. It should be a python script with is ran every day. 

My current implementation features two tables: 

* a **password\_log** which contains two columns - username and a date which the password is reset. Every time a password is reset an entry is put into this column with today's date. 
* a **password\_update** table which contains two columns - username and date which the password should reset

The python script contains the following logic: 

1. Sort the password\_log table to find the last date that the password was reset. If this is greater than or equal to 173 days - it then checks to see if the user already has an entry in password\_update. If not it should create an entry in password\_update with a reset date which is 7 days  + today's date. 
2. Sort through the password\_update table to see if there are any users with an update date = today's date. If yes, create a password and send email to user. It should then remove the record for this user from the password\_update table. 

This implementation works when testing, but seems a bit clunky due to the constant inserting and deleting of records from the password\_update table.

Does anyone see any problems with this implementation? Any improvements? Or have you created something similar in the past? ",False,0.86,https://www.reddit.com/r/dataengineering/comments/155rdr0/automating_a_password_reset_system_is_this_a_good/,dataengineering
How do you integrate code into Airflow that comes from different repositories within your organization?,155pmu7,panymermelada,1689949788.0,,False,True,Help,6,False,5,"Hey fellow data engineers,

I'm currently working on a project that involves collaborating between the data engineering and data science teams. Our goal is to automate certain tasks and leverage the code created by the data science team. However, the data science team follows a unique approach of having separate repositories for each project.

I've been exploring ways to handle Airflow when it needs to use code from these different repositories. One approach I came across is used by the [Data Team at Gitlab](https://gitlab.com/gitlab-data/analytics/-/blob/master/dags/data_science/ds_propensity_to_contract.py) , where they clone the data science team's repository and execute the required code. But considering our team's setup, having multiple repositories for each project, I'm wondering if it would be better to propose consolidating the data science team's code into a single repository.

I'm eager to hear about your experiences and insights on this matter. Have any of you faced a similar situation? How have you tackled the challenge of integrating Airflow with code from different repositories in your data engineering projects? Are there any potential tradeoffs to consider?

I'm a junior in a relatively small team, so dealing with git submodules might be a bit overwhelming. Additionally, I've been contemplating another option of installing Python packages on Airflow's host machine and then mounting volumes into the container. However, as our number of projects grows, I worry this approach could become difficult to manage.

At this point, I'm leaning towards the Gitlab Data Team's style, but I'd love to learn about alternative approaches or best practices that have worked well for you.

Looking forward to hearing your thoughts and experiences! Thanks in advance for your valuable input.

&#x200B;",False,0.79,https://www.reddit.com/r/dataengineering/comments/155pmu7/how_do_you_integrate_code_into_airflow_that_comes/,dataengineering
Is there a book about patterns of batch data processing?,155kvkz,shaunyip,1689937697.0,,1689944167.0,True,Help,6,False,9,"I asked the same question for stream processing in this sub, and I got what I wanted.  (The link is [here](https://www.reddit.com/r/dataengineering/comments/10oqttx/is_there_a_book_about_data_streaming_patterns))

Is there also a good book about batch processing?

Or if you think patterns of it are too few to form a book, please also point it out.

Thank you.",False,0.92,https://www.reddit.com/r/dataengineering/comments/155kvkz/is_there_a_book_about_patterns_of_batch_data/,dataengineering
data visualisation using pygwalker,1562hy6,DaliCodes,1689978905.0,,False,True,Blog,0,False,1,[https://www.youtube.com/watch?v=8LIkQb5yEQs&ab\_channel=DaliCodes](https://www.youtube.com/watch?v=8LIkQb5yEQs&ab_channel=DaliCodes),False,0.67,https://www.reddit.com/r/dataengineering/comments/1562hy6/data_visualisation_using_pygwalker/,dataengineering
DE by core. Forced into Mgmt & admin work - Mid career crisis.. Perils of working for non-tech manager,155lu0b,ElectricalCold4537,1689940396.0,,False,True,Career,5,False,7,"I have build excellent quality DE & frameworks processing petabyte of data in last 5 yrs. Started my career on Hortonworks then cloudera & now on Azure. 
Got promoted, a good hike and now expected to drop out of core coding to just a dummy lead responsible for admin work.

Sparked no interest in even logging in for the job.
Best & optimistic opinion & experiences welcomed. 

Sarcastic ones @ keep it yourself 😏",False,0.74,https://www.reddit.com/r/dataengineering/comments/155lu0b/de_by_core_forced_into_mgmt_admin_work_mid_career/,dataengineering
What do I say in this situation?,155w3vt,lmao_unemployment,1689964331.0,,False,True,Help,5,False,2,"I have a teammate (named K) that my manager asked me to delegate a task to create an STM so I can focus on the development and deployment of a new table for a project. 

Problem is, person K hardly ever responds to my messages and even ignores messages from our boss whenever my boss checks in on them. Ive offered to help person K and they just tell me “I’ll let you know when I need help”. 

Now it’s been weeks and person K hasn’t even so much as informed myself nor my boss on how far they’ve gotten on the STM. My boss is out of town for next week and her boss, the director (called R) scheduled a one-off meeting with the team to discuss what we’ve been working on Monday first thing in the morning. 

I caught wind today from one of the leads that said meeting is going to be used to figure out who to layoff and who to reassign to other teams. And one of the things that will be asked for my project is the STM. 

What should I say here? I don’t want to be a jerk and throw K under the bus but my boss specifically assigned the task to K, K ignores our boss and hasn’t shown any progress and now I’m going to have to answer for it to R. 

Thank you!",False,1.0,https://www.reddit.com/r/dataengineering/comments/155w3vt/what_do_i_say_in_this_situation/,dataengineering
System Design Questions,155oyn3,pauloj1,1689948214.0,,False,True,Interview,2,False,2,"Hello everyone,

As many of us are in the process of job hunting or preparing for interviews, it would be extremely helpful to gain insights into the types of questions being asked in recent system design interviews.
Please include

Company:
Topic:

Your contributions are much appreciated and I hope that we can learn a lot from each other's experiences.",False,0.6,https://www.reddit.com/r/dataengineering/comments/155oyn3/system_design_questions/,dataengineering
2023 Women's World Cup Predictive Analysis,1560kjy,monimiller,1689974393.0,,False,True,Personal Project Showcase,2,False,0,"In honor of the Women's World Cup that started yesterday, I decided to push myself out of my comfort zone and create my first analysis using a machine learning model where I use CatBoost to predict the outcome of the Women's World Cup. Major thank you to Gustavo Santos who's [initial work](https://github.com/gurezende/World_Cup_2022/blob/main/Results/FIFA_WorldCup2022.png) for the men's world cup last year allowed me to streamline my own analysis. To make things fun, I decided to play against the model in an installment I like to call [Monica vs the Machine](https://dev.to/monimiller/monica-vs-the-machine-womens-2023-world-cup-analysis-kf8).  View my [Github repo here](https://github.com/monimiller/womens_wc_23). Let me know what you all think!  
",False,0.33,https://www.reddit.com/r/dataengineering/comments/1560kjy/2023_womens_world_cup_predictive_analysis/,dataengineering
Storing solar API data,155kcm2,tandem_biscuit,1689936144.0,,False,True,Help,21,False,6,"Hi r/dataengineering

I have a personal project that I'd like a bit of guidance on. I'm not a DE (which will become more apparent as you read my post), but I'm keen on learning some stuff.

In short - I have rooftop PV solar system, with a Fronius inverter. I want to gather data from the inverter and have it stored in a local database. I'll then analyse the data or pop it into a power BI dashboard or something, i'm not sure yet.

Currently, I have a scheduled report that emails me the data once a month, and I have written some basic python to read/format the data and upload it to a local PostgreSQL database. So each month, I download the CSV file, save it and then run my python script that does the rest. Which works fine - but I'm keen to learn more about DE so i'm looking to automate this as much as possible.

So - my solar inverter has a ""Push Service"", which seems to be an API that can either send the data via a) FTTP or b) HTTP to a web address. So my next question is - what is the best way to do this? Do I just tell my solar inverter to FTTP the data to my NAS (or home server), and write a python script to run every day (or whatever) to read the JSON data and send it to Postres, or is there a better way to do this via HTTP using a local web server? This is where I'm a bit lost. I'd like the data in 5 minute intervals (so 288 JSON files p/day).

I have a home server running proxmox, with multiple VMs/LXCs running 24/7 - including a PostgreSQL server, Linux VM with docker, and various other services. Suffice to say i'm not afraid of getting my hands dirty. I honestly want to make this as automated as possible, with somewhat close to ""real-world"" implementation to learn some DE techniques. Also, because this is a personal project - free or close to free is ideal.

So my question - how should i go about this? What would be considered ""best practice""?",False,0.88,https://www.reddit.com/r/dataengineering/comments/155kcm2/storing_solar_api_data/,dataengineering
Os change on n2200 thecus nas,155vn7b,SGTARM87398,1689963240.0,,False,True,Discussion,0,False,0,"I need help I have a thecus n2200 nas and want to change is os since the n2200 is not supported anymore.
But I can't find a single documentation about it

Do anyone as an idea in how to do it",False,0.5,https://www.reddit.com/r/dataengineering/comments/155vn7b/os_change_on_n2200_thecus_nas/,dataengineering
Help breaking into Python work,155kbjm,Separate-Rhubarb,1689936055.0,,False,True,Help,6,False,4,"My company's stack is focused on SQL based development (dbt, Redshift, low code sql based tool for integrations). I feel fairly proficient in SQL. 

I am a beginner in python. I have gone through automate the boring stuff which was great, and have taken odd web courses for things like pandas and api calls. However I'm having a really hard time applying this at work, partly because our stack seems to cover this all off with SQL. 

Really need some practical applications to get more practised and confident. I feel like having python in my tool box would really expand my capabilities. 

Any advice, suggestions etc? I feel like I struggle to come up with practice projects, and there's not much in this job to automate because its a consultancy firm and projects seem to be pretty different from one to the next.",False,0.76,https://www.reddit.com/r/dataengineering/comments/155kbjm/help_breaking_into_python_work/,dataengineering
What is the most impressive thing you’ve ever done as a data engineer?,154zbbu,SeriouslySally36,1689878337.0,,False,True,Discussion,110,False,74,"Did you perhaps singlehanded do [data engineering task]?

Or did you solve [difficult data engineering problem]?",False,0.97,https://www.reddit.com/r/dataengineering/comments/154zbbu/what_is_the_most_impressive_thing_youve_ever_done/,dataengineering
How can I go about solving this task?,155o5c1,International-Shirt5,1689946290.0,,False,True,Help,4,False,2," 

We have a data collection application. The product is used for gathering metadata. My task is to conduct research and prepare a preliminary solution: Add a chat feature to our application that will be able to answer questions about the data collected in our production. Write what tools and methods we could use.

How can I approach this problem? I'm more inclined towards typical data engineering, and here I have to do something like this. I don't know where to start. No additional requirements or product information.",False,0.75,https://www.reddit.com/r/dataengineering/comments/155o5c1/how_can_i_go_about_solving_this_task/,dataengineering
"Optimizing Data Modeling for the Data-First Stack - Existing Challenges, Solutions, and Proposed Data Flow.",155gzrz,growth_man,1689925321.0,,False,False,Blog,6,False,5,,False,0.78,https://moderndata101.substack.com/p/optimizing-data-modeling-for-the,dataengineering
Do you use Databricks SQL with DBT?,155rtng,sync_jeff,1689954649.0,,False,True,Discussion,0,False,0,"I know DBT + Snowflake is a very popular combination.  I am curious if people use DBT with Databricks SQL?

[View Poll](https://www.reddit.com/poll/155rtng)",False,0.5,https://www.reddit.com/r/dataengineering/comments/155rtng/do_you_use_databricks_sql_with_dbt/,dataengineering
Modern Data Stack: What Data Ingestion tool should you pick?,155pzn4,MisterHide,1689950586.0,,False,False,Blog,9,False,1,,False,0.67,https://bitestreams.com/blog/modern_data_stack__what_ingestion_tool_should_you_pick/,dataengineering
What do you guys think is the best to get UCC Data?,155pwmw,Pr0d1gy63,1689950408.0,,False,True,Help,1,False,1,Doing some research on UCCs.,False,1.0,https://www.reddit.com/r/dataengineering/comments/155pwmw/what_do_you_guys_think_is_the_best_to_get_ucc_data/,dataengineering
Replace airflow to aws step functions,155g7xw,yeager_doug,1689922892.0,,False,True,Discussion,25,False,1,"Hi there 
My company is trying to move in a path where all solutions need to be 100% AWS , and we (data team) was asked to replace airflow to SF , how do you guys see it? Would SF replace all features from airflow?",False,0.57,https://www.reddit.com/r/dataengineering/comments/155g7xw/replace_airflow_to_aws_step_functions/,dataengineering
“Technical” PO driving me and my team nuts,154qdwf,rudboi12,1689858267.0,,False,True,Discussion,42,False,48,"Just a quick rant. Thankfully I don’t have to deal with this woman anymore since I’m changing companies soon. 

We had a legacy process of sql queries running on notebooks done by non-engineers. 

Now, the company decided that because this product brings a lot of money, it should be maintained by actual engineers. My team started working on this “migration” and now that almost everything is done, this PO wants to have access to change code, etc. She is questioning every engineering decision Ive made like if she knew. Aside from not knowing what git is, I got a funny (not really funny) passive aggressive question “why do you have everything starting with feature on github, better rename it to something we all know what it means”. This is just the tip of the iceberg lol.

I honestly feel sorry for my teammates that will have to deal with her after I’m gone. 

Hope no one in this sub is going through this lol.",False,0.89,https://www.reddit.com/r/dataengineering/comments/154qdwf/technical_po_driving_me_and_my_team_nuts/,dataengineering
Relational or Non-Relational Database for Data Warehouse,15579su,Sweet-Butterscotch11,1689896664.0,,False,True,Discussion,19,False,6,What is the point of spending effort modeling a highly structured Relational Database when it comes to a Data Warehouse that has as main function collect data from different sources - most cases systems that has its own relational database for reliability proposes - and provide Data Assets to be consumed by Dashboards/Reporting tools?,False,0.88,https://www.reddit.com/r/dataengineering/comments/15579su/relational_or_nonrelational_database_for_data/,dataengineering
"i have encountered the weirdest process of de interviews ever, and is there anybody want to team up and mock?",1554o9y,ebink0010,1689890397.0,,False,True,Interview,28,False,8,"so i applied to a data engineer job and by far i have passed 3 rounds (phone screen, 1st vo, 2nd vo), now they invite me to attend on site next week, to go through another 4 rounds of interviews.....

i feel like they want to kill me

so by far i havent been tested a single question of python or sql, and its a very surprising thing to me

they tailing me about pipeline design, system design and api design, yes, you are seeing this right, api design, as if its interview for sde not de

so now im trying to prepare for the next, and also the final round. i thought if anybody is interested or going through similar preparation process, maybe we can do a mock for each other. i mainly want to look for buddies who wants to do prep on design and not the sql/python codings.

add me on discord if you wanna team: Elaina#5305 ",False,0.72,https://www.reddit.com/r/dataengineering/comments/1554o9y/i_have_encountered_the_weirdest_process_of_de/,dataengineering
Databricks down?,1556asw,britishbanana,1689894285.0,,False,True,Discussion,5,False,5,The status page is all green but everyone on my team got kicked out and can't log back in. API access doesn't seem to be working either. Anyone else having issues?,False,0.86,https://www.reddit.com/r/dataengineering/comments/1556asw/databricks_down/,dataengineering
"Purposeful networking sessions, *therapy sessions*, coaching/mentoring conversations with data folks [Free]",15532vo,drc1728,1689886740.0,,False,True,Discussion,4,False,7,"Came to Reddit to scroll for a bit. Scroll stopped at the first 2 posts on r/dataengineering.

Top 2 posts are associated with skill gaps in product management and data engineering. Have an idea and I want to validate if it makes sense. So here goes...

I am considering **offering my time for 3 free sessions of up to 60 minutes every week to discuss data careers, data architecture, data products, learning resources etc. with data practitioners**.

I am also happy to record these conversations and share the recordings with you, or as podcast episodes to share and contribute to building your brand. (Flexible on this point)

If you are raising your eyebrows at 'free sessions'... Kudos to you for thinking critically. Here is my reasoning.

**Why do I want to do this? What's in it for me?** *(Discovery and research as always)*

* I want to prioritize learning from actual experiences over scrolling through social media and internet newsletters.
* I work for a company building a composable data platform and iterating through a data product management mastermind with a handful of my past coworkers.
* I am already talking to our current users and my existing network and I want to broaden the scope of my research beyond my echo chamber.

**Is my time valuable to you?** *(Maybe... you decide)*

* I am currently a Head of product at a sufficiently funded startup working to fix a few basic broken building blocks of data pipelines.
* Prior to this I have led data and platform products in eCommerce, automotive manufacturing, surveillance, healthcare and life sciences.
* I worked as a hands on data engineer from 2009 to 2014 and a software engineer from 2006 to 2009.
* Through the last 10 years I have:
   * Mentored several (100s) of data engineers, architects, product managers over the last 10 years.
   * Managed teams of 12 to 60 people.
   * Been in interview panels for data roles.
   * Been in the instructor panel and advisory groups for technical training companies.

**Who is this for?**

* Senior data engineers/architects with experience in scalable data pipelines and data products.
* Have experience with deploying data pipelines using tools like Kafka, Kinesis, Data Flow, Beam, Pulsar, K-Streams, Spark, Flink etc. (I am interested in diving deeper into this ecosystem right now)

If this resonates with you - **Book some time on** [**my calendar**](https://app.reclaim.ai/m/drc/high-priority-meeting)

If you have any questions or feedback, I will look for them in the comments.

That's it! My 1/10 \*anonymous\* self promotional content. Fingers crossed.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15532vo/purposeful_networking_sessions_therapy_sessions/,dataengineering
"A Composable Customer Data Platform (CDP) for the combination of software and tools for data collection, storage & modeling, and activation",155oawg,DataSentics,1689946660.0,,False,True,Blog,0,False,0,"Unlike traditional all-in-one CDPs, a composable CDP is like Lego building blocks — you pick the best components to build what you want. To personalize customer experience, to boost automation and to power up marketing.

**Traditional vs. Composable**

Classic CDPs integrate different needs into a single streamlined product. Such a platform creates **a unified customer database and offers various functionalities** (e.g. data collection) that are quickly accessible by other systems.

A composable CDP, on the other hand, utilizes the best-in-class components for every step using your preferred components. Data collection and data creation systems of your choice, a data platform to store and process the data, and components to activate the insights in CRM, marketing or self-service analytics.

&#x200B;

[Key terms explained](https://preview.redd.it/zd842jy7nbdb1.png?width=1497&format=png&auto=webp&s=cc37695479e6654ca35d0d50fafbcca03a780967)

**Layers of composable CDP. How does it work?**

The idea behind composable CDP is simple – using top-notch software to **collect, unify, understand and activate various data.** Composable CDP consists of the same parts as traditional CDP:

* **Data collection** from different sources of customer interaction (including metadata) provides context of customers’ specific digital interactions. CDP follows up on it and integrates this data. Traditional institutions have a lot of unused and locked data which aren’t used at all.
* **Data storage,** **unification and analyses** in a unified database that provides a 360-degree view of each customer. Using these unique insights powers the organization’s personalization, facilitates marketing cross-selling/upselling opportunities, and more.

&#x200B;

https://preview.redd.it/rl2hksr9nbdb1.png?width=1548&format=png&auto=webp&s=242854644b90628ecdec91c0257b1df6fa93bcc6

**5 key benefits of composable CDP architecture for your personalization efforts**

Using composable CDP brings you multiple benefits:

1. **Best-in-class solutions.** Make no compromises, keep using your favorite software and tools, avoid double costs, and leverage your investments.
2. A composable CDP is **no black box**, the data is transparent, and can be extended, checked and integrated into various tools and systems.
3. **Maximized use of your customer data.** By capturing all data, you get to understand your customers’ behavior and make the most of these rich insights for personalization, advertisement targeting, marketing and more
4. **Future-proof**, easily adaptable to your future needs and changing use cases. You can add (and remove) solutions as you wish, so they always serve you well.
5. **Fast implementation and easy maintenance.** Implementing an all-in-one CDP is challenging, especially at an enterprise level. As it revolves around existing customer data, data integration must be implemented across different sources — this often takes months. A composable CDP is much quicker to implement and easy to maintain as it can be implemented module by module, step by step.

Author: Matei Matoulek

PS: This is an informative post based on the Research and Experience of Experts from DataSentics. Visit our website  [DataSentics, an Eviden business](https://datasentics.com/)  for more information.",False,0.25,https://www.reddit.com/r/dataengineering/comments/155oawg/a_composable_customer_data_platform_cdp_for_the/,dataengineering
What is the best approach to handle missing data or late data coming from source systems?,15520f1,cyoogler,1689884382.0,,False,True,Discussion,5,False,6,"If you have a source system that typically sends data into your data lake every hour, how can you handle delayed data or missing data altogether? what if the source system sends incomplete data?

I'd imagine there are a few options:

\- your pipeline can be event-driven (lambda, CF) so lateness doesn't matter, whatever arrives is processed accordingly

\- your pipeline has sensors (a la airflow) that wait for data to arrive and fail or timeout after a given window

\- your pipeline has the ability to keep track of data that is ""supposed"" to be there and queues it up for a later data pull / streaming window.

\- your pipeline uses data quality checks (Deequ, GX, etc.) that determine if a batch of data fails to meet a certain requirement and then retries (?)

&#x200B;

How do you handle this scenario at your companies? What services do you use?",False,0.8,https://www.reddit.com/r/dataengineering/comments/15520f1/what_is_the_best_approach_to_handle_missing_data/,dataengineering
Data as Code - Git like history for Databases?,154wh3y,stringofsense,1689872108.0,,False,True,Discussion,16,False,8,"Hello fellow DEs!

I've been exploring the idea of seeing tables as mathematical functions, where each column is a lookup function of the table's index:

Math: `f(x)=y` Table: `column(index)=value`

Which means, theoretically, changes to the values in a table could be considered alterations to the table's ""code"". I come from a software engineering background, so my immediate reaction to this thought is that changes to code should always be tracked in a version control system so that:

1. Differences between old and new code can be identified
2. Code changes can be staged for testing compatibility
3. We can track change metadata about who made the changes and when they were made
4. Changes can be reverted to an old version

I've seen hints here and there of people trying to solve these problems. For each of the above version control attributes, below are the current technologies that come to mind to solve the same problem (keep in mind I am fairly new to the industry and haven't used any of these)

1. DataFold's data-diff tool
2. dbt-tests
3. change data capture
4. database time travel

So I'm thinking that the data engineering field will eventually develop its own version of git for databases, but I would love to hear other people's opinions on whether there is a need or if it is even feasible.",False,0.85,https://www.reddit.com/r/dataengineering/comments/154wh3y/data_as_code_git_like_history_for_databases/,dataengineering
How many data engineers are on your team?,1551x6d,specificanaldolphin,1689884169.0,,False,True,Discussion,21,False,5,"After university I got hired as the only engineer to a data team serving dozens of companies (within the company). Since then, Ive done everything from setting up data loaders,  pipelines, and self hosted airflow myself. Recently got a SQL/Python developer helping me do dbt migrations. Feeling pretty bummed out that I never had a full team around me to bounce ideas off of and I need to learn new things by myself. The people I help love me but I was wondering what size everybodys data engineering team is? Any advice for someone in my position to stay mentally strong?

[View Poll](https://www.reddit.com/poll/1551x6d)",False,1.0,https://www.reddit.com/r/dataengineering/comments/1551x6d/how_many_data_engineers_are_on_your_team/,dataengineering
Anybody in here experienced with web design and development?,1555dw9,suitupyo,1689892071.0,,False,True,Discussion,6,False,3,"Can anyone recommend any good crash course-style resources on JavaScript and basic web dev for someone with a background in data engineering?

I’m pretty familiar with sql, python and data modeling, but I’m trying to get into web development as a hobby. I’m a huge noob on that frontier.

I’d like to build a basic website with embedded videos, photos and a mailing list, but I’ve never touched front end web design, apart from a very short html class in college. Is there any good primer that could outline essential concepts and tools that are fundament to web design?

I thought I’d ask here because maybe someone else found resources that were well suited for data engineers, as we probably don’t need as much help with backend design.",False,0.72,https://www.reddit.com/r/dataengineering/comments/1555dw9/anybody_in_here_experienced_with_web_design_and/,dataengineering
How did you get zero down time in your streaming data pipelines?,154v5vj,data_is_great,1689869211.0,,False,True,Help,17,False,9,"We are working on a streaming spark data pipeline that reads events from multiple Kafka topics, perform some stateful operations on the data and outputs it to another Kafka topic.
Today when we want to deploy a new version, we need to stop the currently running jobs, upload the jars to synapse (yes we are using synapse), and then run it again, picking up from where the checkpoint left off. checkpoint is crucial here btw because it stores data about state which helps with the stateful operations.

Has anyone who's doing similar stuff achieved zero down time deployments without losing data and with minimum side effects?
I would love to hear about your techniques.

Thanks!",False,0.81,https://www.reddit.com/r/dataengineering/comments/154v5vj/how_did_you_get_zero_down_time_in_your_streaming/,dataengineering
Is it normal for data engineers to be lacking basic technical skills?,1549emd,Techthrowaway2222888,1689806565.0,,False,True,Discussion,159,False,215,"I've been at my new company for about 4 months.  I have 2 years of CRUD backend experience and I was hired to replace a senior DE (but not as a senior myself) on a data warehouse team.  This engineer managed a few python applications and Spark + API ingestion processes for the DE team.  

I am hired and first tasked to put these codebases in github, setup CI/CD processes, and help upskill the team in development of this side of our data stack.  It turns out the previous dev just did all of his development on production directly with no testing processes or documentation.  Okay, no big deal.  I'm able to get the code into our remote repos, build CI/CD pipeline with Jenkins (with the help of an adjacent devops team), and overall get the codebase updated to a more mature standing.  I've also worked with the devops team to build out docker images for each of the applications we manage so that we can have proper development environments. Now we have visibility, proper practices in place, and it's starting to look like actual engineering.

Now comes the part where everything starts crashing down.  Since we have a more organized development practices, our new manager starts assigning tasks within these platforms to other engineers.  I come to find out that the senior engineer I replaced was the only data engineer who had touched these processes within the last year.  I also learn that none of the other DE's (including 4 senior DE's) have any experience with programming outside of SQL.  

Here's a list of some of the issues I've run into:  
Engineer wants me to give him prod access so he can do his development there instead of locally.

Senior engineers don't know how to navigate a CLI.

Engineers have no idea how to use git, and I am there personal git encyclopedia.

Engineers breaking stuff with a git GUI, requiring me to fix it.

Engineers pushing back on git usage entirely.

Senior engineer with 12 years at the company does not know what a for-loop is.

Complaints about me requiring unit testing and some form of documentation that the code works before pushing to production.

Some engineers simply cannot comprehend how Docker works, and want my help to configure their windows laptop into a development environment (I am not helping you stand up a Postgres instance directly on your Windows OS).

I am at my wits end.  I've essentially been designated as a mentor for the side of the DE house that I work in.  That's fine, but I was not hired as a senior, and it is really demotivating mentoring the people who I thought should be mentoring me.  I really do want to see the team succeed, but there has been so much pushback on following best-practices and learning new skills.  Is this common in the DE field?

&#x200B;",False,0.95,https://www.reddit.com/r/dataengineering/comments/1549emd/is_it_normal_for_data_engineers_to_be_lacking/,dataengineering
Need Help with Automating Daily REST API Calls and File Storage using Python and Azure,154yptu,Next_Sink9778,1689877025.0,,False,True,Help,2,False,4,"Hi r/dataengineering,

I'm working on a project and need your guidance. I'm attempting to build a solution that daily downloads a file from a REST API, makes another call to the same API to indicate I want another measurement for the day (to be retrieved ""tomorrow""), and then stores this file in a specific format: id-date.json. This process is repeated for 3 different IDs, resulting in 3 files per day.

I have a background in Python and am using Azure. My current approach uses Azure Functions, with Python v2 selected for the runtime. The documentation suggested using the `func.write_blob` decorator, but I couldn't get it to work, so I switched to the `azure.storage` module. While I'm able to run it locally, I'm facing issues with deployment.

My questions for the community:

1. Am I on the right track, or should I consider a different tool or methodology for this task?
2. For those of you who've done something similar, how did you tackle this problem? 

This seems like a common workflow, yet I've struggled to find a simple, straightforward example. Any tips, suggestions, or pointers to resources would be greatly appreciated!

Thanks in advance!",False,0.84,https://www.reddit.com/r/dataengineering/comments/154yptu/need_help_with_automating_daily_rest_api_calls/,dataengineering
Maximizing available tools,155b975,ViolinistAcademic916,1689907626.0,,False,True,Help,0,False,1,"Sorry, long read but I need help!

I just got a new job where data is saved on-prem. My team receives a daily report from Oracle in csv format, this report is saved in a drive, pulled by VBA and then processed using Power Query. The records are filtered by Sales Rep and then VBA appends applicable records to existing excel files owned by each sales rep and saved in another drive. This process takes about 45-60 minutes with someone clicking buttons. I have been tasked with automating this process.

I want a completely handsfree process and would also want to expand the use our data, rather than for just data entry. The sales spreadsheets are about 110 columns long. The. tools available are SharePoint, Dataflows in Power BI & Power Apps, Power Automate, Microsoft Access and Excel.  Unfortunately, i will not be given a data gateway to use Power Automate or Power BI desktop, hence files will still be manually retrieved. I also want to create a canvas app in Power Apps linked to a SharePoint List so the sales reps can stop using the long Excel files. I do not know VBA programming.

Any suggestions on a good architecture for this? Sorry for the long draft.

&#x200B;

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/155b975/maximizing_available_tools/,dataengineering
What does your permissions approval process look like?,154ufoc,icysandstone,1689867669.0,,False,True,Discussion,23,False,6,"For audit purposes, how do you keep track of what database or other resources permissions have been granted, who has approved, etc.?

Are you managing this with spreadsheets, or a dedicated tool?",False,0.88,https://www.reddit.com/r/dataengineering/comments/154ufoc/what_does_your_permissions_approval_process_look/,dataengineering
Datalake file structure design for easy deletion of specific user data,15510hy,Longjumping-Nail-250,1689882120.0,,False,True,Help,11,False,3,"Hi!

I am doing a redesign of our datalake (Azure) structure to support incremental loads.

It needs to support easy deletion of data from specific users. User data is in jsonl format.

Would it make sense to just dump new data into json files, one new file per patient with new data? And then tag or prefix the filename with user ids?

Or does it make more sense to use parquet files and partition by date or similar? I am not aware of any easy way to remove specific rows from parquet files.",False,1.0,https://www.reddit.com/r/dataengineering/comments/15510hy/datalake_file_structure_design_for_easy_deletion/,dataengineering
I would like some project suggestions involving Spark.,154pq2q,Kratos_1412,1689856510.0,,False,True,Help,9,False,9," I need some suggestions for projects involving Spark or Spark Streaming with Kafka and ELK, as I have searched but haven't found anything yet. I am just a little bit confused. Can anyone please help me? ",False,0.91,https://www.reddit.com/r/dataengineering/comments/154pq2q/i_would_like_some_project_suggestions_involving/,dataengineering
Athena Provisioned Capacity Review,154szgo,kevins8,1689864471.0,,False,False,Blog,3,False,3,,False,0.72,https://bit.kevinslin.com/p/athena-provisioned-capacity,dataengineering
Is your data team using real-time streaming analytics?,154xf4x,arthur-dataland-io,1689874189.0,,1689883822.0,True,Discussion,12,False,3,"The evolution to the ""live data stack"" is getting easier with more options for streaming transformations + analytics (Flink, Druid, Materialize, Clickhouse, etc.), but I'm curious for a rough breakdown of data teams using it.

For folks using it in production - I'd also love to know the stack.

EDIT: Typo, or → for

[View Poll](https://www.reddit.com/poll/154xf4x)",False,1.0,https://www.reddit.com/r/dataengineering/comments/154xf4x/is_your_data_team_using_realtime_streaming/,dataengineering
"I crafted a personal learning project, does this data/analytic pipeline make sense?",154wam4,Guyserbun007,1689871699.0,,False,True,Discussion,3,False,3,"I have a quite a bit of experience in data science, but want to get some real hands-on experience with data engineering. I plan to do this workflow, if anyone who can comment on whether this makes sense or if there is any room for improvement, it will be much appreciated!

The overall goal for this project is to get a 5-min interval price data for BTC and ETH, create a simple time-series ML prediction for their price for the next 5 min, then display the historical price and predicted price on a streamlit app.

Here is my proposed data/analytic pipeline:

1. Data source: use a local python script to extract prices of BTC and ETH from APIs such as coingecko or flipside, every 5 min. I will likely also extract tweets on these cryptocurrency so I can later conduct a sentiment analysis as a predictor for their price forecast.
2. The raw, extracted data will be stored in the local postgresql database
3. Data transformation will be done in dbt
4. Sentiment analysis (for tweets) will be performed with a pretrained model (i.e., VADER sentiment classifier), I will either do it locally or via pySpark (like to hear some thoughts from others)
5. Once the data is transformed and sentiment and price prediction is generated, it will be loaded into a snowflake data warehouse
6. I will then create a streamlit app by accessing the snowflake's analytic layer data
7. The orchestration will be done via dagster",False,1.0,https://www.reddit.com/r/dataengineering/comments/154wam4/i_crafted_a_personal_learning_project_does_this/,dataengineering
Data engineering and Kubernetes?,154yvxw,knowledgeMeUp,1689877384.0,,False,True,Help,10,False,2,"Hi everyone,

I'm slowly learning more and more about data pipelines and dats engineering. I get how data engineering and pipeline creation works but every time I hear kubernetes it's like a black box to me. 

What do you do with kubernetes in data engineering. I know airflow is usually set up with kubernetes but idk what that generally means. 

Any resources you can provide or direction you can point me in would be helpful. Just trying to fill this gap in the data engineering life cycle that I don't understand.",False,1.0,https://www.reddit.com/r/dataengineering/comments/154yvxw/data_engineering_and_kubernetes/,dataengineering
Delta tables: Binary vs String data types,154tym1,RandomWalk55,1689866625.0,,1689867895.0,True,Discussion,0,False,3,"I get what the binary data type is, but am unsure on when I should be leaving a field as binary and when I should be storing it as a string.

Is the sole difference that it's not being represented in a specific string format like UTF-8?

Is there performance overhead on casting that to a string on reads that would make it beneficial to cast to string during an initial table write?

Another way to ask the question: Should only binary data be stored as binary or is there an advantage to storing text/string/json data as binary?",False,1.0,https://www.reddit.com/r/dataengineering/comments/154tym1/delta_tables_binary_vs_string_data_types/,dataengineering
Dashboarding as DE,154x3o4,sakeoyakudon,1689873482.0,,False,True,Discussion,5,False,2,"Hey everyone, I’m currently interning as a Data Engineer and my project is solely a building a dashboard. I was looking forward to working on pipelines, data architecture, that sort of thing. 

My question is, is building dashboards usually part of the job description of Data Engineers? I don’t want to sound like I’m complaining - building dashboards is a new challenge for me but I guess I was really excited going into the role and was slightly disappointed when I was assigned my project. 


Also to note: I recognize the fact that they probably wont give an intern any business critical work - just wanted to get your guys’ experience on what Data Engineers should expect their day to day to look like.",False,0.76,https://www.reddit.com/r/dataengineering/comments/154x3o4/dashboarding_as_de/,dataengineering
Alternatives for mapping documentation,154tnf1,cami100fuegos,1689865947.0,,False,True,Help,1,False,2,"Hello, I am working at a bank, in the data warehouse department. The ETL part uses DataStage to load data into Teradata. The current mapping of all transformations is done in a shared Excel file via SharePoint. The information in this Excel file includes: Source Table/File (with the details of each field) -> Transformation -> Data Warehouse Table (with the details of the fields)

The Excel file has become huge, and it's challenging to work with. I was wondering if there are any tools that could help me replace it. Do you have any recommendations?

Thank you!",False,1.0,https://www.reddit.com/r/dataengineering/comments/154tnf1/alternatives_for_mapping_documentation/,dataengineering
Need advice in deciding DE Career path,154jdep,raj_gd,1689835969.0,,False,True,Career,26,False,11,"Hi, I am (Data) engineer with near to 5 years of experience in Data. 

My experience consists of working majorly on SQL (have got intermediate level expertise) and in past 1 year am working on Snowflake sql and ETL tools (Datastage & Snaplogic - but not too deep).. Apart from these know some basics on Power BI & recently started on Python).. 

I am STUCK as I feel like more of Support Engineer than Dev & have no idea on what to do next as DE is becoming code heavy & I don’t like  advance BI tool work as it’s slow, frustrating with those DAX & M language. 

So should I continue in DE (by learning any visual ETL tool like ADF) OR
 become Data Analyst (but BI tools are must here which I am not comfortable due to above)
OR 
is becoming Data Scientist possible? 

REALLY NEED HELP 🙏🏻",False,0.82,https://www.reddit.com/r/dataengineering/comments/154jdep/need_advice_in_deciding_de_career_path/,dataengineering
Leveraging Object Storage for Enterprise Legacy Data,154wo99,swodtke,1689872530.0,,False,False,Blog,0,False,1,,False,1.0,https://blog.min.io/object-storage-for-legacy-data/,dataengineering
Recommendations for Automating a Databricks Production Workspace,154vr72,RichHomieCole,1689870512.0,,False,True,Discussion,2,False,1,"I don’t have much of a devops background. I’ve worked some with Jenkins deployments to airflow in the past, but that’s about it. Looking to expand my knowledge and capabilities.

At my current job, I’m looking to automate our production databricks workspace so that it essentially only runs jobs. I’m curious what type of tooling everyone recommends for this. We’ve used Azure Data Factory in the past to trigger jobs, but we’ve never had a tool for actually deploying everything full scale. 

Ideally the work stream for users would be something like this:
1. Engineer develops and confirms code passes tests in dev.
2. Engineer pushes pipeline and job code to respective folders in main branch
3. Possibly GitHub action triggers a deployment when main is updated? This is where my knowledge drops off. When I worked with Jenkins, this deployment step was manual

With repos, we really don’t need to create any notebooks, as my thought has been that all we really need is something to deploy json that will specify compute, schedule, and path to notebook in github. 

How are you guys doing this? Any learning resources or suggestions are much appreciated.",False,1.0,https://www.reddit.com/r/dataengineering/comments/154vr72/recommendations_for_automating_a_databricks/,dataengineering
What is it like working as Data Engineer at EY?,154pa7h,ryandane123,1689855301.0,,False,True,Career,6,False,2,"First of, what kind of a DE is the DE in EY? Is it actually more on building pipelines or more on the BI side?

Second, how are the projects? Do you work more closely with modern tech or legacy ones?

Third, what does a Senior DE typically do? ",False,0.67,https://www.reddit.com/r/dataengineering/comments/154pa7h/what_is_it_like_working_as_data_engineer_at_ey/,dataengineering
If you have 100 different data sources and each one needs to have a different config file. What's the best way to design this process?,154g5w3,epictaco,1689825254.0,,False,True,Interview,6,False,10,"Had a systems design interview that I failed because I wasn't sure how to answer this question.

My naive ass said I would store it all on an in-mem db like redis and set the params there and just call the process that way.

Not sure if there's a better way",False,0.86,https://www.reddit.com/r/dataengineering/comments/154g5w3/if_you_have_100_different_data_sources_and_each/,dataengineering
Share System Design interview topic,154i6yc,pauloj1,1689831838.0,,False,True,Interview,1,False,4,"Hello everyone,

As many of us are in the process of job hunting or preparing for interviews, it would be extremely helpful to gain insights into the types of questions being asked in recent system design interviews.
Please include

Company:
Topic:

Your contributions are much appreciated and I hope that we can learn a lot from each other's experiences.

 #Software #SystemDesign #System #Intervie",False,0.83,https://www.reddit.com/r/dataengineering/comments/154i6yc/share_system_design_interview_topic/,dataengineering
Fact,153o48v,Sailja_Jain,1689752236.0,,False,False,Meme,10,False,224,,False,0.96,https://i.redd.it/glkf1eivkvcb1.jpg,dataengineering
Data Engineering Struggles - My boss thinks Azure is not for internal data processing needs but only for client facing apps. What to do?,1542hgz,Glittering_Role_8051,1689790400.0,,False,True,Discussion,38,False,22,"Hi all, some context - I work in a medium size North American consulting company mostly comprising of civil/mechanical type engineers and an IT team mostly of software engineers and some cybersecurity/devops people. We are a Microsoft shop only, so Azure and Power Platform (relevant info). I am one of those “analyst” fulfilling data analyst/data scientist/data engineer roles for client projects but truly mostly doing data engineering work and I am the only one doing this stuff. Regardless to say, there is no formal recognition of any data-specific role at my company. I’ve never heard “data engineer” or even “data scientist” words from my bosses. It’s always me trying to include data type of lingo to plant a seed in their heads that there is a whole data world out there that the company is even doing client projects in but it all becomes ”app development” in managers’ eyes but in reality we are doing things like moving GBs of data (thank god its not in TBs yet) in Azure SQL/blob, processing it with python/R (sometimes Azure Data Factory, if IT approves, who are also new to the data world) , database schemas, and developing PBI dashboards. 

Any Azure resource I need I have to explain to my main boss who is a civil engineer (not in IT but does .NET development, doesnt deal with the amount of data processing I have to do) and then that goes to IT. But he only understands Azure (or any cloud for that matter) from the lens of his “app development” work. And without his support IT can’t approve because IT also dont know much abt pipelines/star schema/distributed computing/Spark and all this and there is no dedicated data engineer in IT to understand my pains and even my work….

Anyways, today I was sharing how I plan to process this new set of files (about 60 GB, so not bad in terms of big data) for a project and this is what my boss said among other things, “Keep the solution simple, avoid Azure if you can. Don’t forget, Azure is really there for client facing apps, not for internal data processing.”……………. bruh… :( 

And I have many managers like this who are civil/mechanical engineers doing .Net stuff and think every data pipeline is an “app”, every dashboard is an “app”, all database schemas shd be transactional and so even some IT people start making .NET compiled apps to clean up column names and do basic data transformations and math. And there was a time in the past where I was even being discouraged to use Python because it is not Microsoft. Their push - .NET - to do develop apps? No, to do ETL/statistics and other automating data workflows. Thankfully we are over that phase (I think) and I can use python, thanks to some senior machine learning experts who also use python.

But yes anyways, I am just so tired I don’t know what to do. I have tried giving presentations on analytical vs transactional data needs, written emails of explaination regarding certain technical decisions of why we need for example Azure Data Factory and not Power Automate/Power Apps/Power \_\_\_ for large/complex data transformation needs (oh yes they are easier on Power platform compared to anything Azure because Power “Apps” and because a non data higher up boss who leads the “digital” practices of our company uses Power platform/share\[oint for his projects. And it works well for his document processing type work which is fine, but not when you are feeding 500 million rows for a realtime dashboard…). I have explained where our competitors are in terms of their data abilities, I have shared where the world is in terms of data technology and everything but it is becoming growingly taxing to keep trying to convince non-data/software developers of why we need certain data tools/processes for data engineering/data science workflows (yes I m sorry, I group anything data all together for them). 

The company is def not bad in terms of work life balance and they are all nice respectful people and with months and months of convincing sometimes they do approve on using certain data-specific tech but they keep going back…just their lack of understanding of data world and yet their confidence that they know everything has been very tiring… but after 2 years of this may be I should stop caring and just give up….

Thank you for listening, I just wanted to vent it out since I dont have any friends in the data field. Have a wonderful rest of your week :)

&#x200B;",False,0.97,https://www.reddit.com/r/dataengineering/comments/1542hgz/data_engineering_struggles_my_boss_thinks_azure/,dataengineering
Is Dataform relevant?,154h3p2,Itchy_Advantage_6267,1689828170.0,,False,True,Help,11,False,2,"I have been exploring open-sourced tools to ensure data quality and implement an alerting system based on SQL-based validation rules. During my research, I came across Dataform. I would appreciate your opinion on whether it is the most suitable tool for my use case.

I have tried Great Expectations but have come to the opinion that it is not well-suited for SQL-based validation.",False,0.76,https://www.reddit.com/r/dataengineering/comments/154h3p2/is_dataform_relevant/,dataengineering
Any one interviewed with City of Toronto,154flci,EmergencyVanilla2870,1689823480.0,,False,True,Interview,0,False,2,"Hi,   
I was wondering if anyone had prior experience with being interviewed for a data engineering role in the city of Toronto. If so, Can you please share your experience?",False,0.76,https://www.reddit.com/r/dataengineering/comments/154flci/any_one_interviewed_with_city_of_toronto/,dataengineering
Bridging the gap between IaC and Schema Management | Atlas | Open-source database schema management tool,153w8gb,rotemtam,1689776056.0,,False,False,Blog,3,False,23,,False,0.97,https://atlasgo.io/blog/2023/07/19/bridging-the-gap-between-iac-and-schema-management,dataengineering
"Implementing microservices, API in DE context",154exvt,cyamnihc,1689821595.0,,False,True,Help,1,False,2,"Working as the sole DE on a non tech team. I am free to use whichever tool/tech I want. As I want to transition to backend dev, I want to learn and implement microservices, rest apis, lambdas. I am looking for suggestions on how and where I can use this in my role. P.S: We use GUI ETL tools to get data from external databases, use python to get data from external apis and push all to redshift. All jobs are on AWS instance. The data in redshift is the source for tableau dashboards",False,1.0,https://www.reddit.com/r/dataengineering/comments/154exvt/implementing_microservices_api_in_de_context/,dataengineering
Github tips/suggestions for small team,1548dlo,xxEiGhTyxx,1689804108.0,,False,True,Help,3,False,4," 

Hello there,

Posted this on the Github subreddit without much response and I feel this somewhat more tailored towards this sub anyway.

I work for a fairly large company that already uses GH, however the team I work with and others surrounding it do not (I am fairly new and the teams never bothered with it I guess)! Our manager wants us to start utilizing GH.

Does anyone have any tips/suggestions about where to start and which/what features are super helpful to teams? Anything like a checklist of ""must-haves""?

Some things to note - we are primarily backend devs/data engineers/data scientists. The overwhelming majority of our code is in T-SQL or Python/Jupyter. We also have A LOT of SSIS packages and some things in Databricks.

I have a couple years experience working with it (creating/submitting PR's, squashing, basic things).

Some early questions I have been trying to answer is if Code Spaces or Actions would be useful - how are your experiences with these? Does it make sense in a team of \~15-20 people? And what kind of structure makes sense? Should we be operating out of one repo or many?

Thank you!",False,0.84,https://www.reddit.com/r/dataengineering/comments/1548dlo/github_tipssuggestions_for_small_team/,dataengineering
Is GitHub enough to show projects?,1548nku,Neat_Historian9740,1689804746.0,,False,True,Career,4,False,3,"I’m looking to start applying for my first DE job. I was thinking about making a portfolio website, but then I wasn’t sure if it would be a good idea to delay job applying because of that. I’m not from CS/data background, so I felt like I’d really have to present my projects well, including the web development skills by building a portfolio web, in order to pass the interview screenings. I will eventually make one in the future, but since I want to get out of my current job so bad and since it is not a remote, I’m leaning towards not making one right now. 
And also, how important is LinkedIn when searching for jobs? My current LI acc is restricted for some reason, and it’s taking forever for them to resolve it..(apparently many people are experiencing this issue)

Thanks for any advice.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1548nku/is_github_enough_to_show_projects/,dataengineering
Bugs Bunny + data jokes + data,1540ti9,Top-Substance2185,1689786510.0,,False,False,Meme,0,False,5,,False,0.7,https://i.redd.it/6lpo2jnreycb1.png,dataengineering
Manager wants me to sync two different code bases to the same Git repo. Is it a good idea?,1546cnh,rotterdamn8,1689799401.0,,False,True,Help,7,False,3,"I maintain code in Linux and ""git push"" changes to the origin, can view in browser. 

Separately I work on Databricks notebooks - and manager asked me to sync to the same repo.

I did that today and it's causing trouble when trying to git push/pull from Linux (branches diverged and other errors).

Is this common practice? I see some possible solutions (git pull with --allow-unrelated-histories) but I wonder if it's a good idea. Will I see other problems like this? 

There's no technical reason to use the same repo, it's just that they are related to the same project. 

Based on your experience, I want to talk to the manager (he's technical and reasonable). TIA! ",False,1.0,https://www.reddit.com/r/dataengineering/comments/1546cnh/manager_wants_me_to_sync_two_different_code_bases/,dataengineering
Schema Migration for Delta Lake on Databricks,153xtyq,geeeffwhy,1689779717.0,,False,True,Discussion,14,False,6,"I’m trying to understand how to think about and architect the schema (ddl) and data migration strategy for some fairly complex data pipelines in databricks. i’m really puzzled because i’m coming from an applications (backend) background, largely with postgres/mysql. so for these, database migration is pretty mature and used everywhere. but when i search this up for delta lake/databricks, there’s basically nothing. 

so maybe i’m just not thinking about this in the data engineering way, or maybe there’s some other terms i need to be using? how should i think about supporting structured, version-controlled changes to the table structures and the data they contain? 

am i way out of line to be thinking of implementing sqlalchemy alembic for delta lake? is there _any_ tool or even just discussion of how this is done? 

i can’t possibly be the first person to have this this thought, but when i asked my databricks rep, i spent half an hour trying to explain what a schema migration even was, so clearly there’s a disconnect somewhere.",False,0.88,https://www.reddit.com/r/dataengineering/comments/153xtyq/schema_migration_for_delta_lake_on_databricks/,dataengineering
Most Data Engineers are Mid,154towa,philonoist,1689866036.0,,False,False,Blog,11,False,0,,False,0.21,https://dataengineeringcentral.substack.com/p/most-data-engineers-are-mid,dataengineering
Infrastructure/Modeling Questions about Best Practices.,153z4uf,GreenMellowphant,1689782654.0,,False,True,Help,3,False,4," 

My (quant) teammate (Data Visualization Specialist) is working with a couple of proficient Data Engineers and a DBA. We're in the middle of transitioning from a solely SQL Server and Power BI environment to incorporating Azure Data Factory (for data ingestion), Data Bricks (for storage and transformation), and maintaining Power BI for self-service reporting. The data sources are scattered, and the data literacy is in the basement. ""We're pretty much burning it to the ground and starting over"" as my teammate (DE) puts it.

As we gear up for this bonfire, a few critical **questions pertaining to best practices** are surfacing, particularly surrounding the housing and management of our data models. I am hoping to tap into this community's wisdom and gain some insights into the following:

1. **Location of Data Models:** What is the best practice in terms of where to place our data models? Is Azure Analysis Services (AAS) a feasible choice to hold enterprise-grade data models in the cloud, or should we lean towards Power BI's built-in datasets?
2. **Exposing Data Models to Users:** What are the most effective ways to expose our data models to the business users who need them? We plan to leverage Power BI's roles and permissions features but are there other recommended strategies? We've set up security groups and spoken about the level of access during each stage prior to publishing.
3. **Model Complexity and Size:** How can we accommodate larger datasets in the future, considering the model size limitations in Power BI? Does Azure Analysis Services or another platform provide a better solution for larger data models?

Our goal is to provide each business group a dedicated PBI Workspace (in addition to the default individual workspaces). As expected, all reporting that comes out of these distributed development spaces will go through a pipeline from Development to Testing to Production, overseen by the Data Team before reaching the production space. 

This business has grown by 400% in the last 5 years and has no real experience with proper data governance or infrastructure. Thankfully, we don't have a ton of data, only about 400GB. Help has arrived, but your insights and experiences regarding these questions would be greatly appreciated and would go a long way in helping us make informed decisions quickly (obviously we'll research and validate any advice given).

Lastly, if it isn't apparent, I'm unfamiliar with many of these matters (fortunately not as unfamiliar as most of our users), so please be as explicit as is convenient. Thank you in advance for your help, and I look forward to reading your responses.",False,1.0,https://www.reddit.com/r/dataengineering/comments/153z4uf/infrastructuremodeling_questions_about_best/,dataengineering
Free 30-minute one-on-one Mock Interview,154hqea,pauloj1,1689830234.0,,False,True,Career,2,False,0,"Dear All,

&#x200B;

I am pleased to offer you a free 30-minute one-on-one mock interview to help you enhance your SQL, Python, Big Data, AWS, and Databricks skills. You will be able to practice industry-focused questions, take on real-world scenarios, and build your confidence. I will provide direct feedback to help you improve. This is a completely free and unsponsored opportunity.

This opportunity is only available to candidates with 2-5 years of experience, not senior roles. Interviews will be scheduled on Pacific Time.

&#x200B;

Please sign up today! The deadline to submit the form is July 31st, 2023.

&#x200B;

[https://airtable.com/appTeodcm1TJD9UoH/shrExsaVBax7xSWB5](https://airtable.com/appTeodcm1TJD9UoH/shrExsaVBax7xSWB5)",False,0.29,https://www.reddit.com/r/dataengineering/comments/154hqea/free_30minute_oneonone_mock_interview/,dataengineering
How to identify which all tables are used/queried frequently in databricks.,1540pec,fusebox12345,1689786252.0,,False,True,Help,1,False,3,"Hi all, is there a way to identify which all tables are being used or queried frequently in databricks.
Note: no access to unity catalogue.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1540pec/how_to_identify_which_all_tables_are_usedqueried/,dataengineering
How important is database knowledge beyond querying? At what point are returns diminishing?,153qe9h,Subject_Fix2471,1689760055.0,,1689762841.0,True,Discussion,10,False,8,"In your opinion - what' s the threshold of diminishing returns as far as postgres/database knowledge goes?

For context Im currently working as a data engineer and I've ended up doing most of the work on the database (it's a startup...). This is giving me experience with data modelling, schema design, writing methods on the database, performance monitoring and so on. 

I'm on the fence as to how useful this experience is, on the one hand I feel it's pretty valuable and can be hard to get  (most people _probably_ wouldn't have wanted me managing their prod db...). On the other - I'm _not_ doing things such as dataflows, pyspark and so on. My python is fairly solid mid level (package publishing, reasonable code/data patterns/structures, testing etc), so I'm not lacking ability to write python - I just haven't been working with cloud / distributed stuff.  

Obviously data engineering is a very broad brush - but I'm wondering what sort of database experience / knowledge people typically have here _beyond_ querying effectively and how valuable they feel it is. Do you have experience setting up postgres on a server from scratch, create and test backup&restore, handle load balancing, bastion server etc etc... if not, do you care?

So - In your opinion - what' s the threshold of diminishing returns as far as postgres/databases goes?  

Thanks! :)",False,1.0,https://www.reddit.com/r/dataengineering/comments/153qe9h/how_important_is_database_knowledge_beyond/,dataengineering
Job application for a lead role “Please expand more on your experience with SQL”,153llcj,k_dani_b,1689744004.0,,False,True,Interview,24,False,20,Ummm weird question. At this point I dream in SQL sometimes and think I might think in it as much as I think in English. I think most lead data engineers know it super well.,False,0.81,https://www.reddit.com/r/dataengineering/comments/153llcj/job_application_for_a_lead_role_please_expand/,dataengineering
Create index (reference) table for parquet tables?,153zndo,_unwin,1689783825.0,,1689789345.0,True,Help,2,False,2,"So we have a data lake on AWS, data goes with its original format (or csv if extracted from a ddbb) into the landing zone, then gets converted into parquet and carried through the next zones.

The DA at my team came up with this idea of creating a reference table that would work as an index.

So for example, given a users table, create a reference table with only the id and the relevant timestamp column (for eg.: created\_at, updated\_at), add some fields like 'valid\_from', 'valid\_to' and 'is\_current\_row' and then join this reference table with the main one.On the next executions, the new records should be inserted into the main table and then, both the main and the reference tables should update values for 'valid\*' and 'is\_current\_row'.

So the purpose of this is to use the index table for speeding up some queries since it holds less columns, and also be able to tell in which point in time the row was inserted/updated.

Finally, all this data will be loaded into redshift.

My questions are, is this index table necessary since parquet is columnar and supports page-level indexes? Wouldn't be more convenient to use apache iceberg for handling the 'time travel'? It also provides more features like compacting files that otherwise would have to be done with sepparate script.

&#x200B;

Edit: so i got a metting with the DA. She stated that the reason parquet would be less performant than using the index table (***id where current\_row = true*** to get the max date value, and then use that value to load the corresponding partition) is more efficient than relying on parquet looking through metadata for each partition and checking the values until you find the current\_row = true, since it will lead to multiple reads that will result in the current\_row being false, until it finds the true one. So even though it will skip row groups, it will have many false positives. Is this correct? The only way to figure it out is by testing it, or are there some good practices i'm not following? ",False,0.76,https://www.reddit.com/r/dataengineering/comments/153zndo/create_index_reference_table_for_parquet_tables/,dataengineering
the devs chose mongo again smh,1531jz7,itty-bitty-birdy-tb,1689694124.0,,False,False,Meme,39,False,197,,False,0.98,https://i.redd.it/ux9wsli3sqcb1.png,dataengineering
Apache Doris/Starrocks Architecture,153u367,Public_Fart42069,1689770815.0,,False,True,Discussion,8,False,3,"Started researching these open source data warehouses. One thing I'm confused about is, if using this type of tech, where do you do your transformation jobs? If I have a bunch of csv or parquet files in blob storage, if I load them into Doris/Starrocks, does it support transformation or should your data be cleaned and ready to go before loading it in these types of warehouses?  

If you're using either of these, what's your overall architecture look like?",False,1.0,https://www.reddit.com/r/dataengineering/comments/153u367/apache_dorisstarrocks_architecture/,dataengineering
"Database ""Reconstruction"" Project Feedback",1542sl3,mjnr_19,1689791114.0,,False,True,Blog,0,False,1,"I just completed my first project and I have gotten some opinions from those outside the field for my English, now I'd like some technical review on maybe how to better present my article on how I dealt with Text Qualifiers: [The Rookie’s Mistake](https://medium.com/@olufeoluwamac/the-rookies-mistake-d2b1238fde83)

Also does it pass as data reconstruction or is cleaning a safe umbrella term?",False,1.0,https://www.reddit.com/r/dataengineering/comments/1542sl3/database_reconstruction_project_feedback/,dataengineering
S3 -> RDS Question,1542n4f,INeedLegalHelp69,1689790761.0,,False,True,Help,6,False,1,"I have pretty simple requirement. Read small .csv files in S3, perform light transformations, and load into an RDS Postgres table. Data is light enough where it makes sense (for me at least) to do the transformations in lambda with Pandas. My question is now that I have a dataframe in the lambda with the transformed data, what is the best approach from here to get that data in RDS? 

(Also open to overall architecture suggestions if what I’m doing is bad)",False,1.0,https://www.reddit.com/r/dataengineering/comments/1542n4f/s3_rds_question/,dataengineering
Spark jar dependencies directory deleted,154252k,SmileHardy_,1689789621.0,,False,True,Help,2,False,1,"Hello all!

I need help!
We expanded the size of the yvi directory partition and as a result, all JAR dependencies got deleted :(

Now, when we run a Spark job, Spark attempts to download each JAR file for every user.

How can we fix this issue?

We are using Spark on AWS EKS.

Thanks for your assistance.",False,1.0,https://www.reddit.com/r/dataengineering/comments/154252k/spark_jar_dependencies_directory_deleted/,dataengineering
Preparing Advice for DP-203: Data Engineering on Microsoft Azure,153va24,keyboard1ish,1689773784.0,,False,True,Career,5,False,2,"I currently work as a Data Engineer with 5 months experience in AWS. I have proficient knowledge in Python and SQL.

I want to take the exam to learn about Microsoft Azure and all cloud related topics in Data Engineering.  I have never used Microsoft Azure and have no idea even where to start to learn. I am familiar with some of AWS tools like Glue, Athena, S3, Lamda, EC2. 

I am planning to take a week off work to revise and 2 hours a day for another week. 

My question is,

1. Can I pass this exam by learning from [Azure Data Engineer Associate](https://learn.microsoft.com/en-us/certifications/azure-data-engineer/?tab=tab-learning-paths#certification-exams)
2. Is there any other website that can help prepare me for the exam. 
3. Is there a course or free learning that I can follow with hands on experience. ( I am guessing Azure has a free trial like AWS but I would need a guide to help me through) 

Thanks  ",False,1.0,https://www.reddit.com/r/dataengineering/comments/153va24/preparing_advice_for_dp203_data_engineering_on/,dataengineering
Pipeline for Ml system metrics improvement.,153ywpb,BetterPhotograph585,1689782161.0,,False,True,Discussion,0,False,1," 

Hi,

I'm currently a part of a developing a CV based system, which runs multiple ML models. We have a service that combines all the inference information from the models and applies some algorithms and postprocessing to get end result. We would like to get metrics related how overall system works for predictions, and compare it to ground truth data. Currently the system takes around 7 seconds per sample to get the end result (mainly due to service that encapsulates everything and does postprocessing) called M service. And if we have 20000 samples it takes considerable amount of time to get the end result how overall system works for predictions.

We are trying to figure out how we could make it significantly faster to get the metrics of overall system.

We are thinking to do it as a batch process and run it within some scheduled times, for example, every 4 hours as a workflow:

Data are in S3 and Amazon RDS, and we plan to create a workflow in AWS Glue, that gets the data from the S3 and Amazon RDS -> send each sample to the M service through API -> gets the results back -> aggragates results -> calculates metrics -> stores the metrics.

We have automated scaling in kubernetes for M service on test environment and models are managed in sagemaker which has also scaling rules.

Does this make sense to do it In terms of the idea? Are the any better alternatives? And would it be viable to use AWS Glue for this purpose?

 ",False,1.0,https://www.reddit.com/r/dataengineering/comments/153ywpb/pipeline_for_ml_system_metrics_improvement/,dataengineering
How do you handle comma character while parsing csv files in dataframe based frameworks like pandas or spark?,153yehi,BiggESmalls96,1689781013.0,,False,True,Discussion,4,False,1,"I was trying to parse comma separated files in pandas but was facing issues due to there being commas in the data fields itself.

I tried some approaches on stack overflow that needed the string character to be quoted but the csv file in question does not have quotes.

I also saw some mentions of using a pre-defined schema but that would be a no-go as we are trying to build a generic function to parse and load csv to delta tables for further processing.

I was curious as to how other have handled this issue as I felt this might be very common occurrence for DE's.",False,0.67,https://www.reddit.com/r/dataengineering/comments/153yehi/how_do_you_handle_comma_character_while_parsing/,dataengineering
Excel to SharePoint,153x1xk,shanke_y8,1689777959.0,,False,True,Discussion,1,False,1,"Hello, 

I have been tasked to transfer excel files (.xlsx) to SharePoint library, I am having issues with the linked contents (numerical values) within the files, so one file is linked to another excel file and another one is linked with 3 other files. 
I am not sure how to process so many files, as when I upload the files on SharePoint their links get changed to ""='https://seai.sharepoint.com/"". 
All the linked contents from these files have now broken as the code changed to ='https://seai.sharepoint.com/, is there a way to fix this?

We will also be operating and working with the data from SharePoint going forward, I am not sure how I can make Excel files work in SharePoint. Is there a lead or a tutorial I can sign up for to learn about handling excel files in SharePoint? Or is there a different method to fix this.  

Thanks,",False,1.0,https://www.reddit.com/r/dataengineering/comments/153x1xk/excel_to_sharepoint/,dataengineering
"Free copy of ""Fundamentals of Data Engineering"" to learn DE",152yp2h,TemporaryPoorFrench,1689687366.0,,False,False,Career,15,False,100,,False,0.98,https://go.redpanda.com/fundamentals-of-data-engineering,dataengineering
"Real-time Data Processing Pipeline With MongoDB, Kafka, Debezium And RisingWave",153m5jl,bumurzokov,1689745793.0,,False,False,Blog,0,False,3,,False,0.81,https://boburadvocate.hashnode.dev/real-time-data-processing-pipeline-with-mongodb-kafka-debezium-and-risingwave,dataengineering
For every Data Professional mini-interview we'll donate $5 to dogs or code.org,1537puk,sprintymcsprintface,1689708241.0,,1689731948.0,True,Discussion,18,False,26,"TLDR - you mini-interview, we donate: [VideoAsk Mini-Interview](https://www.videoask.com/fisb43vgs) 

I'm a founder at a small data startup, and we are looking for input from Data Professionals related to the problem our tool is solving; I am a DE myself and am too close to our solution to be objective. 

**We decided instead of paying a marketing research group, we would take a shot with the communities we are part of (like this one) and donate our small research budget to good causes instead.** (mini-interview Link is above, here it is again):

[VideoAsk Mini-Interview](https://www.videoask.com/fisb43vgs) 

For every completed (applicable) mini-interview we'll donate $5 to either [code.org](https://code.org) or [phillyPAWS](https://phillypaws.org/) (our local rescue here in Philadelphia) until we run out of $$ (we will shut the survey down before then, no free lunches on our part).   
by ""applicable"" we mean the respondent is either a data or software professional or works directly with data/software professionals.",False,0.91,https://www.reddit.com/r/dataengineering/comments/1537puk/for_every_data_professional_miniinterview_well/,dataengineering
Prefect vs dbt for creating warehouse transform DAG,153sl5w,Longjumping-Nail-250,1689766757.0,,False,True,Help,2,False,1,"I have been using Prefect + pandas for ETL pipelines, but currently looking into options for doing the transformation part in the warehouse (ELT).
Dbt seems to be the most common tool to define the transformations, but I am assuming a set of prefect tasks that query the warehouse (executed in the correct order) would achieve a similar thing?

I am aware that they are completely different tools, so what would be important features of dbt that would be tricky to replicate with a prefect flow?

The reason I would want to stick to prefect is to be able to completely customize things like logging/notifications, and not having to add another tool unless strongly motivated. 

I am currently doing all the data engineering/data science for a scale-up, probably analyst(s) would be hired soon, and it might be easier for them to contribute to a dbt project than a python project?",False,1.0,https://www.reddit.com/r/dataengineering/comments/153sl5w/prefect_vs_dbt_for_creating_warehouse_transform/,dataengineering
Does Data Engineering ever stop running into issues with the cloud service provider?,153i3me,LeftShark,1689733696.0,,1689734057.0,True,Discussion,8,False,5,"I've done 3 different personal projects this month, and each one I run into thousands of problems with AWS, Azure, or GCP. I'm not trying to be a DevOps person but with how many cloud issues I've resolved, maybe I should be. Does anyone else feel these limitations? I just wanna get to the point on projects where I can utilize my SQL or Python without wading through 4 hours of cloud initialization

It feels like on all these projects, I spend 95% of the time working out issues with the cloud development and getting that running, and 5% on the actual transformations and orchestrations between the services once they're running.",False,0.73,https://www.reddit.com/r/dataengineering/comments/153i3me/does_data_engineering_ever_stop_running_into/,dataengineering
"Need help configuring hadoop, spark, hive and derby",153pp6c,data_extractor,1689757687.0,,False,True,Help,0,False,1,"Hey guys, I wanted to learn about the working and integration of hadoop, spark, hive and derby.

So far i have created a cluster of 3 nodes using dell optiplex thin client core i5 and 32gb each.
I have successfully installed hadoop, spark, hive and derby.

I am able to access and create files in hdfs, run spark on the master node, but struggling with connecting derby with hive, hive with spark and connecting to spark remotely.

Version used

- Hadoop 3.3.1
- Spark 3.4.1
- Hive 3.1.3
- Derby 10.14
- Java 1.8.0_362",False,1.0,https://www.reddit.com/r/dataengineering/comments/153pp6c/need_help_configuring_hadoop_spark_hive_and_derby/,dataengineering
Live Q&A Session on DE Careers,153om3v,Acrobatic-Bass-5873,1689753942.0,,False,True,Career,0,False,0,"Join us on 25th July at 8 PM IST for a LIVE Q&A session on ""Exploring Data Engineering Careers""! 🚀

🔍 Are you curious about Data Engineering roles?

🤔 Wondering how to kickstart your career in this booming field?

📺 Tune in to our YouTube channel for valuable insights from an industry expert! Don't miss this chance to get your questions answered and gain valuable career tips! 💼

Register here: [https://forms.gle/bB64meGrsowSC5PX9](https://forms.gle/bB64meGrsowSC5PX9)

Save the date and set a reminder! See you there! 👋 

https://preview.redd.it/l3v99enypvcb1.png?width=720&format=png&auto=webp&s=dfdd65a9cdac63549146da4cf93036a774cd883e",False,0.5,https://www.reddit.com/r/dataengineering/comments/153om3v/live_qa_session_on_de_careers/,dataengineering
"Comparing Data Parallel, Task Parallel, and Agent Actor Architectures",1539nyl,semicausal,1689712664.0,,False,False,Blog,0,False,8,,False,1.0,https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures,dataengineering
"Free, Real-time Flight Status Pipeline with Kafka, Schemas Registry, Avro, GraphQL, Postgres, and React",153jimw,Jealous_Ad6059,1689737731.0,,False,True,Personal Project Showcase,0,False,2,"[https://medium.com/@stefentaime\_10958/free-real-time-flight-status-pipeline-with-kafka-schemas-registry-avro-graphql-postgres-and-7ac59b63ea99](https://medium.com/@stefentaime_10958/free-real-time-flight-status-pipeline-with-kafka-schemas-registry-avro-graphql-postgres-and-7ac59b63ea99)

&#x200B;

https://preview.redd.it/6v00mbnrducb1.png?width=1767&format=png&auto=webp&s=45eff0b11287df6931a7b80c55859fa36b6ebc32",False,1.0,https://www.reddit.com/r/dataengineering/comments/153jimw/free_realtime_flight_status_pipeline_with_kafka/,dataengineering
Microservices and DE,153clpb,ubiquae,1689719449.0,,False,True,Discussion,5,False,4,"How are you solving data analytics and data management considering a decentralized microservices ecosystem for operational data

Are you using any tool for centralized data governance on the operational side?",False,0.84,https://www.reddit.com/r/dataengineering/comments/153clpb/microservices_and_de/,dataengineering
Unleashing the power of Data : Storage,153nmsw,RemarkableAttempt311,1689750632.0,,False,False,Blog,0,False,1,,False,1.0,https://waruithemystery.hashnode.dev/unleashing-the-power-of-data-storage,dataengineering
Managing JSONs as raw data in CDP data lake,15396op,TheSoulHokib,1689711563.0,,False,True,Discussion,3,False,8,"Hi,

I'm currently managing a data swamp, and working to make it clean and reliable.

We have a lot of raw data as JSONs, that we need to store and be able to manipulate on our Cloudera Data Platform (on top of HDFS).

What would be the best way to store them on our cluster, in order to access them afterwards in our data preparation ?

Atm, we are inserting them transformed in Apache Hive, but we really want to store them raw and manipulate them from a more appropriate DBMS / Document Manager",False,1.0,https://www.reddit.com/r/dataengineering/comments/15396op/managing_jsons_as_raw_data_in_cdp_data_lake/,dataengineering
Is there a ROI for the data engineering endeavor?,153ioma,afivegallonbucket,1689735343.0,,False,True,Discussion,7,False,2,"Curious how often other engineers see any results of the effort and money sunk into your or your customer’s company in the developing of their data estate. Pipelines are built, infrastructure stood up, dev ops incorporated, warehousing, reports, etc….what is the benefit? I love engineering but sometimes I feel bad for the those flipping the bill. There is this idea(on every major data products website) that there will be n-zettabytes of data worldwide by 202X and we can unlock all “this” insight from the data, etc., etc. How often is a tangible report delivered that actually produces insight or generates ROI? I see very basic deliverables, but rarely a payoff that equates to the amount of resources sunk into the endeavor.",False,1.0,https://www.reddit.com/r/dataengineering/comments/153ioma/is_there_a_roi_for_the_data_engineering_endeavor/,dataengineering
Seeking Advice: Navigating the Field of Data Engineering after the completion of my Master's Degree,153ds3e,arcofiero,1689722224.0,,False,True,Career,0,False,2,"Hello Everyone,

I am a recent graduate with a passion for working in the field of data engineering. After earning a Master's degree in Data Analytics Engineering and a Bachelor's in Management in Information Systems, I've managed to gain experience and develop key skills in this domain.

My proficiency lies in designing scalable data solutions, optimizing data infrastructure and pipelines, and managing large datasets. I am proficient in Python, R, SQL, C++, and Java, and have considerable experience with big data technologies such as Apache Spark, Apache Hadoop, Kafka, PySpark, and Snowflake. I've gained a good understanding of cloud tech involving AWS, DB2, and Oracle SQL. 

Over the past few years, I have held positions (as an intern) Senior backend developer in a renowned tech company and as a data engineer at a startup. My roles included developing real-time data ingestion pipelines, managing data warehouses, optimizing SQL queries, integrating RESTful APIs into database solutions, and ensuring data integrity, security, and compliance across all operations.

Despite my experience, navigating the job search post-graduation has proven to be quite a challenge. I am reaching out to this community seeking advice:

* What companies should a fresh graduate in data engineering consider for a good start?
* Do you have any suggestions on how to stand out among a pool of candidates?
* How do I strategize my job search? - Up until now, I've usually relied on LinkedIn, so should I broaden my search to other Job Portals?   


I sincerely appreciate any advice or insights this community can offer. Here's to a shared passion for data engineering and the exciting journey it entails!

Best Regards,   
A Passionate Data Engineer",False,0.67,https://www.reddit.com/r/dataengineering/comments/153ds3e/seeking_advice_navigating_the_field_of_data/,dataengineering
Data management challenges in M&A,1539422,Competitive_Speech36,1689711385.0,,False,True,Blog,3,False,5,"My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.  
Mergers and acquisitions (M&A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.

  
Mergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&A, and provide tools for addressing them effectively.  
During and after M&A, two companies’ tech stacks essentially collide, and IT management has to deal with a number of challenges:

# 1. Integrating disparate systems and platforms

Integrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.

Here are some of the key issues that arise during this integration process:

* Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;
* Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;
* Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;
* Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;
* User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;
* System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.

# 2. Reconciling different data formats and standards

When two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.

# 3. Significant differences in the technological maturity between the two companies

If one of the companies has a significant technological advantage over the other, several issues may arise:

* Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;
* Operational Disruptions: if one company’s systems are automated and the other’s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;
* Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It’s crucial to assess and address these risks as part of the integration process.

# 4. Considering the human element

This is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&A.  
The problem is, employees are conservative and don’t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.  
And it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things – they need to start profiting from the merger or acquisition as soon as possible.

  
Merging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal’s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.

  
In fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&As), as they help increase acceptance among employees and minimize potential disruption. Here’s how these strategies can be applied:

1. Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company’s situation;
2. Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;
3. Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;
4. Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive ‘us versus them’ mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.

Other issues can be solved via the right tool that addresses every issue mentioned.

# How to effectively address data-related M&A challenges?

To effectively address the challenges posed by M&A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.  
A central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.  
By transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.  
Moreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.

# No-Code Data Solutions

No-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&A right away.

Furthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.

In addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&A process.

The rest of my research can be found here: [https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm\_source=linkedin&utm\_medium=social&utm\_campaign=statistics&utm\_content=migrations\_acquistions&utm\_term=ITarchitecture](https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&utm_medium=social&utm_campaign=statistics&utm_content=migrations_acquistions&utm_term=ITarchitecture)",False,1.0,https://www.reddit.com/r/dataengineering/comments/1539422/data_management_challenges_in_ma/,dataengineering
Seeking a Reliable Python Library for Text-to-SQL Conversion,153kurs,Sweaty_Ad_1093,1689741722.0,,False,True,Help,6,False,0,"I am currently in search of a reliable Python package or library that would basically do text-to-SQL conversion. My goal is to generate SQL queries that I can run on BigQuery. Although I had tried OpenAI API and it works great, I am unable to utilize it at my current company due to internal reasons.  I have explored several Python libraries available on GitHub for text-to-SQL conversion, but I have encountered various issues such as bugs and missing files. If anyone has successfully used a Python library for text-to-SQL conversion and can vouch for its reliability, I would greatly appreciate your insights and recommendations. Any suggestions, tips, or advice related to this topic would be immensely helpful to me.  Thank you all in advance for your time and assistance!",False,0.33,https://www.reddit.com/r/dataengineering/comments/153kurs/seeking_a_reliable_python_library_for_texttosql/,dataengineering
I created a great expectations wrapper so that even my mum could validate her data,153bm5a,whatisthisdataman,1689717195.0,,False,True,Open Source,2,False,5,"**TLDR**: I created [this package](https://github.com/Elsayed91/easy_ge) so you can use Great Expectations without knowing much about it or if you have simple use cases and would rather an option with better readability and easier implementation.

If you'd rather watch than read, you can check [this low-budget demo](https://www.youtube.com/watch?v=9v8mlDb2oRo). a mm a mmm a. 

This is my first package ever. I called it ""Easy G.E"", pretty cringe, but at least it rhymes. Now I am going to present it as if I'm selling you a real enterprise-grade solution.

**Features**:

1. Provides a low-code approach for validating files stored on various filesystems (local, GCS, S3) or in-memory dataframes.
2. Works with both Pandas and Spark engines.
3. Reduces configuration complexity to just four schema-enforced fields in a YAML file.
4. Allows dynamic variable definition/substitution at runtime, eliminating the need for any hardcoded values in the configuration YAML and facilitating the utilization as part of dynamic data pipelines.

**How it works**:

1. Create an expectation suite for your data that you want to validate. This is straightforward, and a guide is available in the repository.
2. Fill out a `YAML` file with the dataframe name/file path, destination to save results, and the expectation suite file name.
3. Place the expectation suite in a folder named ""expectations"" in the destination you've defined in the YAML file.
4. Import the package and execute `results = easy_validation('/path/to/yaml_config.yaml')`   
That's it! You're done!   
Alternatively, you can use the docker image provided to validate files on the fly. It also integrates well with  `Kubernetes` and `Airflow` when using Kubernetes Jobs for tasks.

**Motivation**:

* When I first tried using Great Expectations, I became so frustrated that I decided to include DBT in my project just to leverage the easier-to-implement DBT Expectations.
* When you are not using `great_expectations.yaml` you might have to write \~ 100 lines of cryptic code to run the validation.
* The documentation is hard to navigate, at times difficult to understand, undergoes significant changes between major releases. Moreover, the examples are riddled with [irrelevant context ](https://github.com/great-expectations/great_expectations/blob/develop/tests/integration/docusaurus/connecting_to_your_data/cloud/gcs/pandas/inferred_and_runtime_python_example.py)for users, mainly because the examples serve as tests.
* Another technology to add to the learning list.

**Target Users**:

|Who Could Benefit from the package?|Problems Addressed by the package:|
|:-|:-|
|Individuals who are unfamiliar with `Great Expectations`  but wish to implement data quality checks and documentation.|Overcoming Great Expectation's learning curve required to run basic use-cases.|
|Individuals experienced with `Great Expectations`|Providing a low-code solution without the need for extensive module creation, documentation, and unit testing.|
|Users seeking a solution that is easily readable by peers unfamiliar with the tool.|Enhancing readability for individuals who are unfamiliar with Great Expectations, ensuring ease of understanding and collaboration.|

&#x200B;

Disclaimer #1:  *It ain't much but it is honest work.*

This package is rudimentary but will carry out the advertised functionalities. However, as I'm no Great Expectations power user, I've only implemented what I believe to be common use cases. Nevertheless, I've put in my best effort to design the package to be extensible.

Disclaimer #2

I have approximately 6 months of data engineering learning under my belt, and a total of 10 months in the programming ecosystem. Zero real world experience, which makes me feel insanely embarrassed posting this here (imposter syndrome intensifies), but YOLO. Receiving feedback could greatly benefit my ongoing job hunt and help me grow. Also considering how useful I'd have found this package myself some months back, it could be of some use to others, if the whole concept is not a disaster that I am unaware of, that is.",False,0.85,https://www.reddit.com/r/dataengineering/comments/153bm5a/i_created_a_great_expectations_wrapper_so_that/,dataengineering
Passing parameters from table to view in SQL Sever?,153jtf7,suitupyo,1689738618.0,,False,True,Help,3,False,1,"For work, I have several views pertaining to different elements of tracked analytics. The views contain a goalmet column with a bit data type to denote whether the goal was met or not. This column is dependent upon logic built into the view. Unfortunately, the comparisons can change as business goals change. 

Is there a way to make this more scalable? Our management routinely changes its goals. Rather than alter the view and pass in a new target value, I’d rather just compare the value against a target value in another table and have it cascade to each dependent view when it updates. 

This is kind of a silly question, but can I do this by just selecting the target goal in the with clause and joining it to the result table? Or would it be more performative to just stick the target values into a DAX function that underlies the dashboards?",False,1.0,https://www.reddit.com/r/dataengineering/comments/153jtf7/passing_parameters_from_table_to_view_in_sql_sever/,dataengineering
Merge multiple parquet files into a single table in database,153arfs,pussydestroyerSPY,1689715203.0,,False,True,Help,12,False,2,"Hi, I have several parquet files (around 100 files), all have the same format, the only difference is that each file is the historical data of an specific date. Currently I have all the files stored in AWS S3, but i need to clean, add columns, and manipulate some columns. After that, i will need to use these data for analytics and to train a ML model.  WHAT WOULD BE THE BEST APPROACH FOR ALL OF THIS? Should i merge all the files into a database (all files have the same format and columns) and that would be easier to use and would increase the performance of the data cleaning and the analytics? HOW CAN I DO IT? WHICH SERVICES DO YOU RECOMMEND (redshift, glue databrew, etc??)?

Btw, Im new on this stuff. Thanks

&#x200B;",False,0.67,https://www.reddit.com/r/dataengineering/comments/153arfs/merge_multiple_parquet_files_into_a_single_table/,dataengineering
"I am a new grad “data engineer” who barely writes SQL, how will my career be impacted?",152own3,aacreans,1689656717.0,,1689657321.0,True,Career,36,False,56,"I work at a big tech company as a new grad and my title is technically data engineer, however much of my work is building distributed Python applications for ML and data replication, spark, Java, devops pipelines and infrastructure automation (ansible). If I write sql, it’s always within a Python application and is nothing more than an insert, table creation or simple select query. With my experience, would I be more suited to go into software engineer roles moving forward? Scared of applying to companies that use modern data stack or no-code tools and I am stuck having to re-learn SQL or failing an interview.

Edit: I’d consider myself average at SQL, I’ve passed all-SQL interviews, just worried about my skills atrophying as it’s not something I really enjoy to use and my role doesn’t require it",False,0.91,https://www.reddit.com/r/dataengineering/comments/152own3/i_am_a_new_grad_data_engineer_who_barely_writes/,dataengineering
Databricks Monitoring,1537ll0,va1kyrja-kara,1689707981.0,,False,True,Help,0,False,4,"What do you all use to monitor your ingestion pipelines in databricks?

We currently use Azure Log Analytics/Azure monitor but the SparkListener library for Log Analytics sends Spark performance metrics. 

Been using databricks notebooks that do some aggregations and push data through to Log Analytics as a custom SDK rig using log analytics and data collector API's. 

Not great, logs are slow to arrive in LA and the authorization headers and JSON responses get fiddly when data structures differ.",False,0.83,https://www.reddit.com/r/dataengineering/comments/1537ll0/databricks_monitoring/,dataengineering
Loading Data from a Source with a lot of Business Logic,153d0bs,yoquierodata,1689720406.0,,False,True,Discussion,5,False,2,"I’m working to stand up my company’s first enterprise data warehouse, and the biggest and most valuable source system is our flagship product: a CRM. This CRM has an RDBMS back-end on top of which sits an API layer with a ton of business logic. None of that logic is consistently documented, so when I think of integrating that database into our EDW, I’m not sure if there’s any non time-consuming way to discover that logic from the code and incorporate it in our data pipelines. 

Am I missing something here? I don’t think the answer can be “go through the API layer,” can it? 

Thanks, braintrust.",False,1.0,https://www.reddit.com/r/dataengineering/comments/153d0bs/loading_data_from_a_source_with_a_lot_of_business/,dataengineering
Seeking the Best Apache Spark Video Tutorial - Recommendations?,1535mkd,Loki_029,1689703458.0,,False,True,Help,1,False,3," 

Hello to all Data Engineers !

I'm diving into the exciting world of Apache Spark and I'm looking for the perfect video tutorial that provides a comprehensive understanding of Spark's architecture, concepts, and practical usage. With so many options out there, both free and paid, I thought it would be great to tap into the knowledge and experiences of this community.

So, my question is: Which video tutorial would you recommend as the best resource for learning Apache Spark? It could be a free tutorial available on YouTube or other platforms, or even a paid course that you found to be incredibly valuable.

I'm particularly interested in a tutorial that covers the following aspects:

* Spark architecture and core concepts
* Hands-on examples and real-world use cases
* Detailed explanations of Spark transformations and actions
* Spark SQL, data frames, and dataset APIs
* Integration with popular big data tools and frameworks

Your insights and recommendations would be highly appreciated. Please feel free to share any personal experiences, tutorial names, links, or even the instructors who made a significant impact on your Spark learning journey.

Looking forward to your valuable suggestions and thank you in advance for your help!",False,0.72,https://www.reddit.com/r/dataengineering/comments/1535mkd/seeking_the_best_apache_spark_video_tutorial/,dataengineering
What makes a good data warehouse platform?,1532ylc,doubleblair,1689697307.0,,False,True,Discussion,4,False,5,"When you picked your data warehouse platform or if you would pick again, what are the most important themes for you in how you make that selection?For example:

* Installation Simplicity
* Simplcity of operations
* Security
* Query performance
* Cost
* Integrations
* Migration effort
* Cloud vs on-prem
* anything else ???

What was at the top of your list? Is there anything you regret about the data warehouse technology you ended up with? What questions would you have asked before making the decision in hindsight?",False,0.86,https://www.reddit.com/r/dataengineering/comments/1532ylc/what_makes_a_good_data_warehouse_platform/,dataengineering
Airflow Builds Breaking,1532mzq,AnySherbet,1689696579.0,,False,True,Discussion,0,False,3,"Hey ya'll, Airflow builds are breaking today. Yesterday Cython jumped to version 3.0, which caused \`pymssql\` to break. That breakage is causing Airflow to break as well.

My company doesn't use Airflow, but it bit us because we use pymssql in our application. We had to fork pymssql and pin to an earlier version of Cython.

Just hoping to save someone a couple hours of investigation",False,0.81,https://www.reddit.com/r/dataengineering/comments/1532mzq/airflow_builds_breaking/,dataengineering
Building the gold container in the medallion architecture,1537lly,Specific-Passage,1689707982.0,,False,True,Help,4,False,2,"We are just about to build the gold layer in our data lake but unsure of how to organize the directories. We aim to have a kimball model for each project, but unsure where to store the dimension tables (are in delta format). Should we create directories inside the gold container for each model and duplicate shared dimension tables? How do you guys set this up in your orgs? We have the facts and dims in their respective delta tables.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1537lly/building_the_gold_container_in_the_medallion/,dataengineering
What is my role?,152z2bq,1_0-0_1,1689688281.0,,1689689718.0,True,Career,5,False,5,"Two years ago I transitioned from Systems Analyst (app dev) to Data Engineer.  I am the only Data Engineer in our company.  When I transitioned, not only did I continue maintaining all of my legacy projects, but I have since taken over all cloud app development in Azure, designed and maintained our API Management Gateways for third-party devs and vendors (multiple environments, dozens of API products), created numerous backend APIs, implemented most file/data exchange processes between our company and third-parties, created numerous ETL processes which includes many replication jobs from third-parties apps, etc.  I did much of this as a level 1 DE (I am now level 2).  I am trying to understand how much of this is actual Data Engineering and how much of it is something else (Cloud App Development?).  I also am curious what level I should be.

Additional context:  5+ years with the company.",False,0.86,https://www.reddit.com/r/dataengineering/comments/152z2bq/what_is_my_role/,dataengineering
Data Load Patterns 101: Full Refresh and Incremental,153ckcj,ryan_CritHits,1689719368.0,,False,False,Blog,0,False,0,,False,0.5,https://tobikodata.com/data_load_patterns_101.html,dataengineering
"Data Scientists -- Ok, now I get it.",151xsis,tarzanboy76,1689588707.0,,False,True,Discussion,218,False,546,"A few days ago, our data scientist gave me some of his SQL code to put in production.  I quickly glanced over it -- it was neat and tidy, well formatted, organised and filled with helpful comments.  ""Shouldn't be a big deal.  Give me a few days to review it.""

I looked at it more closely today and realised it's actually mess.  Badly written, inefficient, breaks the DRY approach repeatedly, doesn't comply with naming standards, the lot.  I get back to the guy and ask him to explain some of the SQL statements because they're confusing to me. He admits that a lot of it was actually written by ChatGPT and he isn't sure what it's doing.  But he politely asks me when will his code be put into production, thank you very much?

You have code that you can't actually explain *as the developer yourself* but you still expect it to be put in production?

I tell him a need a bit more time.  And he cheerfully tells me that he'll be happy to do it himself *if I'd just give him admin access to production.*",False,0.96,https://www.reddit.com/r/dataengineering/comments/151xsis/data_scientists_ok_now_i_get_it/,dataengineering
Cleared Azure Cloud Certifications. Wondering what is next. Need help !,1532rt2,h_buddana,1689696882.0,,False,True,Career,2,False,2,"I’m a new grad and have cleared AZ-900,DP-900 and DP-203 certifications and currently working on my portfolio projects. Finding it difficult to find End-to-End Data engineering projects on Azure. But I can see there are so many such projects executed and well documented on AWS and GCP. Decided to learn AWS and GCP on side. So wondering what is the best way to learn AWS and GCP after learning Azure. And if you have any crazy data engineering projects on Azure, kindly let me know.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1532rt2/cleared_azure_cloud_certifications_wondering_what/,dataengineering
Seeking Advice from Experts on Electronic Health Record Integration,1531r59,i_am_baldilocks,1689694564.0,,False,True,Discussion,7,False,2,"Hey guys,

I'm working at a company that needs to take data from health care records and put it into a data warehouse for machine learning and analysis.  I've never done anything like this before, and was wondering if there was anyone in the community who has accomplished a similar task and was willing to chat.

We're using Google Cloud for our data services right now.  I'm interested in using their Healthcare API as it looks promising, although I'm open to other solutions: [https://cloud.google.com/healthcare-api](https://cloud.google.com/healthcare-api).  Curious to hear what other peoples' thoughts are.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1531r59/seeking_advice_from_experts_on_electronic_health/,dataengineering
Why not Flink?,1537lsw,mwylde_,1689707994.0,,False,False,Open Source,0,False,1,,False,1.0,https://www.arroyo.dev/blog/why-not-flink,dataengineering
From data specific consultant to DE path,152y27j,jokerxbr,1689685752.0,,False,True,Career,0,False,3,"Hey folks, I'd like to share my current situation to understand better where to go.

I started in data as a ETL developer since 2011, I did projects using DataStream (old Cognos), PowerCenter, SSIS, Talend, SAS and even VBA up to 2016 when I joined a data visualization tool company as consultant mainly focused to create conceptual DB models and reports/dashboards with this tool. However since 2018 composed by 2 periods (2016-2021 and from 2022- up-to-date)   most of my projects were related to upgrade servers to newer software versions, configuration etc. There were a couple projects here and there that were related to predictive analytics with python and/or R, NoSQL, Hadoop, Query performance, data ingestion, but summing all of this it is no more than 10% of my time.

Within 2021 I left this job for a ecommerce company which I had to work with python, Big Query, Teradata, Presto etc. But unfortunately I had to leave it to immigrate which was a bigger dream. That's why I re-joined the previously mentioned company.

I feel that I have the requirements, I never stopped to study, I'm pretty good in query performance, data modeling, user specification, also know python, Scala, AWS, data bricks. a bunch of databases (SQL Server, Oracle, Teradata, MySQL, Big Query, Redshift, Snowflake, PostgreSQL) and I also know a little docker due to home projects, yet I've applied for almost 50 openings and didn't got any replies/interviews even diposed to earn a little less than I'm currently making if necessary. Sometimes I fell that the extended period working for the data viz company kinda put a stamp on me saying that I'm not capable of doing a DE job.

My question is, is it possible to go to a DE path?",False,1.0,https://www.reddit.com/r/dataengineering/comments/152y27j/from_data_specific_consultant_to_de_path/,dataengineering
Looking for warehouse options to replace pandas ETL,152vv4s,Longjumping-Nail-250,1689679785.0,,1689762128.0,True,Help,7,False,2,"I was hired as a DS at a scale-up company, and have implemented all data engineering pipelines using the following stack: Prefect + pandas, with .parquet files on azure datalake as final storage.

I am running into some performance issues, and would possibly want to separate the orchestration and compute., using some warehouse option for the compute. Additionally, the final storage should probably also be a warehouse and not a datalake, as we are hiring analysts that would probably expect some SQL-queryable storage.

I am basically looking into options for warehouses that can do the whole ELT process. Would something like SnowFlake be a good option, or is it overkill for small sizes of data?Does it handle extract/normalization of fairly complex json files, or should I use some other tool for the extract part?

(edit) Some extra info on the performance issues:

* Mainly some specific transformations that take a long time, like creating links between fact tables based on some timestamp business logic.  I would assume a warehouse could do this faster.
* Out-of-memory issues, mainly when ""big"" dataframes are passed between Prefect tasks. I would say this is an antipattern that could probably be coded around easily.
* Reports (PowerBI) are import parqute files directly and are performing well",False,1.0,https://www.reddit.com/r/dataengineering/comments/152vv4s/looking_for_warehouse_options_to_replace_pandas/,dataengineering
"What's the best ""free"" set-up to learn or try to build a data pipeline on my own?",152c49r,Guyserbun007,1689623684.0,,False,True,Discussion,21,False,36,"I am new to data engineering, and I like to learn by trying/building. I am very unclear what's the best way to go about it?

My ultimate goal is to be able to build my own website with a data pipeline in the backend and front end with data analytics visualization etc. Should I go directly with free cloud resources (like AWS), or should I try to manage a local data pipeline such as postgresql, and once I hit the bottleneck, I migrate to better scaling solutions?",False,0.95,https://www.reddit.com/r/dataengineering/comments/152c49r/whats_the_best_free_setup_to_learn_or_try_to/,dataengineering
Working from EU on US,152y6r5,AdClean1116,1689686076.0,,False,True,Career,4,False,0,"Hi guys, I am really sorry if this is the wrong thread to ask. Is there a chance to work on US from EU? I see your salaries and they are just 2-5 times bigger that same positions in EU. If it’s possible, what are chances to get the job for DE with 2 years of experience? Or these opportunities only for senior positions?

Thank you a lot for your help colleagues.",False,0.5,https://www.reddit.com/r/dataengineering/comments/152y6r5/working_from_eu_on_us/,dataengineering
"As a student data analyst in apprenticeship, what can I do to improve the data exploitation process?",152xs6i,Zuzukxd,1689685051.0,,False,True,Help,3,False,1,"I've been working for less than a year in a small IT services company as a data analyst. I'm the only one working in the data field, so I'm totally autonomous.  My job is pretty straightforward and basic: I create dashboards on Power Bi using data from the company's activities, such as accounting or human resources. The data is stored in SQL server databases on local servers.

Here's my workflow, I create sql views in order to extract and process as much of the upstream data as possible directly in sql server and then import it into power bi. Thanks to a data gateway I've been able to automate the updating of the data on the dashboards that are put online. There is no real complexity or intelligence in this process and looking at this reddit every day I wondered if my practice was correct and optimal in my case or if I could optimise it with ETLs or tools/services like Databricks dbt ect.

Sorry for my poor English or if what I'm writing doesn't make much sense, I'm still a data science student with poor knowledge in data engineering.",False,1.0,https://www.reddit.com/r/dataengineering/comments/152xs6i/as_a_student_data_analyst_in_apprenticeship_what/,dataengineering
Real-time data streaming with Debezium and Postgres,152ws1a,Euphoric-Let-8960,1689682327.0,,False,True,Discussion,0,False,1,"Max Kremer outlines real-time data streaming with Debezium and Postgres. Gone through the process before? What has your experience been?

[https://lassoo.io/blog/2023/07/17/postgres-real-time-data-streaming-debezium/](https://lassoo.io/blog/2023/07/17/postgres-real-time-data-streaming-debezium/)

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/152ws1a/realtime_data_streaming_with_debezium_and_postgres/,dataengineering
Transition from cloud architect into data role,152rnc1,Substantial-Figure88,1689665871.0,,False,True,Career,4,False,2,"Hi all

I'm exploring the transition into a data role as the next phase of my career. Uncovering data engineering concepts and architectures make my juices flow again after a few years of stagnation in my current infrastructure solution architect role. Data science and analysis isn't my bag, I'm focused on the engineering track.

My background: I've worked in public and private cloud (primarily Azure infrastructure and micro-services) and data management (primarily protection and disaster recovery) for c. 13 years, the last 5 at a senior architect level. 

I've seen a few posts on 'transitioning' roles and currently looking through the wiki to understand skillsets but would loveto hear from those who have experienced a similar transition from cloud solution architect into a data role. 

 I'm working my way through DP-203 and I envisage the most difficult aspects will include coding. However, that's something I'll just have to work on.

Did you work from a data engineering starting point up to data architect? Did you face any significant hurdles or were there any skills that you found invaluable in your journey?",False,1.0,https://www.reddit.com/r/dataengineering/comments/152rnc1/transition_from_cloud_architect_into_data_role/,dataengineering
What data lakehouse table format does your team plan to use by the end of 2023?,152rgcu,brrdprrsn,1689665188.0,,False,True,Discussion,0,False,2,"

[View Poll](https://www.reddit.com/poll/152rgcu)",False,0.75,https://www.reddit.com/r/dataengineering/comments/152rgcu/what_data_lakehouse_table_format_does_your_team/,dataengineering
Interview preparation help - Python,152ny8o,table_data,1689653788.0,,False,True,Help,0,False,2,"Hello everyone, I browsed through the community posts and understood that [neetcode.io](https://neetcode.io) is by far the best and the most recommended platform for practising Python for DE interviews.

I work as a Data Engineer at a small startup right now, but never in my life I have done LeetCode or even touched DSA, hence requesting for help here.

In the [neetcode.io](https://neetcode.io) platform, in the practice section, if we sort by group, the questions are grouped by topic so that you can tackle them accordingly.

Based on your interview experience, can you please help me prioritise the topics? I am guessing many of these topics are for SDE interviews and not all might be relevant to DE interviews, and even if they are relevant, some must be more important than others.

The topics are:

1. Arrays and Hashing
2. Two Pointers
3. Sliding Window
4. Stack
5. Binary Search
6. Linked List
7. Trees
8. Tries
9. Heap / Priority Queue
10. Backtracking
11. Graphs
12. Advanced Graphs
13. 1-D dynamic programming
14. 2-D dynamic programming
15. Greedy
16. Intervals
17. Math & Geometry
18. Bit Manipulation

The reasons I have to prioritise/filter this is because I have limited time left in the interview prep hence want the important things 1st, and also because I am not specifically targeting MAANG.

&#x200B;

Thank you for the help!!",False,1.0,https://www.reddit.com/r/dataengineering/comments/152ny8o/interview_preparation_help_python/,dataengineering
Unleashing the power of Data: Choosing the right Storage System,152rbb9,RemarkableAttempt311,1689664709.0,,False,False,Blog,0,False,1,,False,1.0,https://waruithemystery.hashnode.dev/unleashing-the-power-of-data-choosing-the-right-storage-system,dataengineering
Flink Batch Job needs to run based on a trigger,152m3hr,Salekeen01,1689648465.0,,False,True,Help,1,False,2,"Hey, I need some output regarding this situation.  
I have a flink batch job that needs to run based on a trigger. By trigger I meant suppose there is a FASTAPI endpoint exposed for clients to post ""submit"", as soon as that happens, the job needs to be submitted!  
We are using Flink-Kubernetes-Operator and its on Session Cluster mode. I can't really use cron, because the its aperiodic as you can see. What will be the flink way to achieve this? Has anyone solved this kind of use cases? If so how?  
Love to know your thoughts!  
Thanks",False,1.0,https://www.reddit.com/r/dataengineering/comments/152m3hr/flink_batch_job_needs_to_run_based_on_a_trigger/,dataengineering
What are the best no-code platforms you have come across?,151yrep,EmoryCadet,1689591732.0,,False,True,Discussion,48,False,28,"My company is interested in having a no code platform so that we can democratize data access and mitigate shitty practices like shared drive hoarding. We are looking at Palantir but it's absurdly expensive. I've never really dabbled with ""tools"" like that because I've never come across something truly no code that worked well.

Curious if anyone here has used Palantir's Foundry or other tools that give non technical users a place to search and see data exists, do their data manipulation, exploration and then visualization on both tabular and time series data.",False,0.95,https://www.reddit.com/r/dataengineering/comments/151yrep/what_are_the_best_nocode_platforms_you_have_come/,dataengineering
What steps to take after first role,1524trd,gators939,1689607060.0,,False,True,Career,5,False,10,"I finally got a role as a Senior Data Engineer after having a background of SQL and taking about 6 months to learn Python. I’ve been here almost a year and my days are spent building pipelines using Pandas. My team though is an analytics team and I am the only data engineer on it, so I’m kind of just learning on my own and doing the same tasks repeatedly. While I have plenty more to learn about Python, I am starting to wonder what other steps I should be taking to move into the next salary bracket. I’m around 120k now and don’t think I have the skill set to move into the 180+ range yet at all, but my team isn’t giving me a ton in terms of project variety so I’m not sure what would be the most effective tool/language/software to spend time learning on my own time. I like my current company but even when they ask me what my goals are/what I want to be doing I’m not sure exactly what to tell them. Any advice?",False,0.92,https://www.reddit.com/r/dataengineering/comments/1524trd/what_steps_to_take_after_first_role/,dataengineering
"Seeking Advice on Securing Data Engineering, Data Science, or Data Analysis Jobs as a Fresh Graduate Based in Asia, Preferably with Overseas Opportunities and Dollar-Based Compensation",152qsh4,Informal-Actuary8833,1689663007.0,,False,True,Career,0,False,0,"Any additional advice, tips, or personal experiences you can share would be immensely helpful in my career journey. I am eager to grow professionally and enhance my skills in these exciting fields. ",False,0.29,https://www.reddit.com/r/dataengineering/comments/152qsh4/seeking_advice_on_securing_data_engineering_data/,dataengineering
Does anyone have experience self-hosting Airbyte?,152lvk5,kdamica,1689647841.0,,False,True,Discussion,6,False,1,"I’m curious about how well the open source self-hosted version works, and how it compares feature-wise to the cloud version. Thanks!",False,0.67,https://www.reddit.com/r/dataengineering/comments/152lvk5/does_anyone_have_experience_selfhosting_airbyte/,dataengineering
Job Title?,152k2qc,Upgrayyedd43,1689642897.0,,False,True,Career,1,False,1,"I currently work as an R&D Test  Engineer in the Microelectronics industry and I'm looking for a title of  some type of Software/Data engineering position to search for.

A little bit of background -- my current role is to process all of our data taken from our test equipment while characterizing our integrated  circuits during on-wafer probe testing. I manipulate all of our test  data into usable format where I then plot the information according to internal/external customers wants and needs.

From here, I normalize all of our test data taken from the lab and import everything through a data pipeline via the libraries and packages below. Next, I try to find a correlation between the process control monitors (PCM) and our test data taken from the lab. Process control monitors are random data points on the wafer that gives us information on wafer oxidation thickness, capacitance values, leakage values and so on that are distributed to us from the manufacturer. I'll use different statistical methods such as Bi-Serial Point Methods, Linear Regression and the use of histograms for example. If I find something of high correlation between the PCM data our test data, I'll dive deeper and analyze it. The whole point of this is to predict wafer yield based on a PCM data range and lab collected test data.

Lastly, we use machine learning models to predict our next lot's yield based off the high correlation PCM data/test data. I am new to building machine learning models and machine learning as a whole, but I've enjoyed this process so much I'm thinking about getting a Masters in either Machine Learning, Applied Data Science or something along those lines.

**The libraries and packages I use are:** Python, NumPy, Pandas, Matplotlib, Pytorch, Keras, SciPy, Scikit learn and not an exhaustive list.

I may be looking for a new job soon if I am switched into another role in the company that I don't want to do, which I wont get into here.

Anyway, if I wanted to dive deeper into this type of role or job, what job title should search for?

Thanks all",False,1.0,https://www.reddit.com/r/dataengineering/comments/152k2qc/job_title/,dataengineering
How to move semi-structured data to LLMs?,1526j16,shrifbot,1689610913.0,,False,True,Help,11,False,6,"I’ve been tinkering with setting up a data pipeline to make a bunch of semi-structured sources in my org (documentation website, github issues, slack messages, …) useful in a [retrieval-augmented generation](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) use case (a.k.a. chat with your data).

&#x200B;

My current approach of gluing together python clients for APIs of various services (openai for embeddings, pinecone for the vector db) feels kind of clunky, are there “proper” ways of handling that? I’ve looked into e2e solutions like [https://www.chaindesk.ai/](https://www.chaindesk.ai/) but they feel a bit too limiting for my use case.",False,1.0,https://www.reddit.com/r/dataengineering/comments/1526j16/how_to_move_semistructured_data_to_llms/,dataengineering
Best way to execute Python scripts on a schedule?,151npp7,BestTomatillo6197,1689556868.0,,False,True,Discussion,48,False,78,"Our data warehouse is a SQL Server. We’ve been using Python to do a lot of scheduled ETL tasks. Currently I’m executing the tasks on a schedule (10 minutes) using Windows Task Scheduler and batch files on the same Windows server as the SQL Server. 

Is there a better way to do this? I’ve read that you can use stored procedures or scheduled events, but is that going to be faster? 

Currently 85% of memory is allocated to SQL Server.

Any pros or cons to consider?",False,1.0,https://www.reddit.com/r/dataengineering/comments/151npp7/best_way_to_execute_python_scripts_on_a_schedule/,dataengineering
LLM based pipelines with PostgresML and dbt,1525yyl,something_cleverer,1689609669.0,,False,False,Blog,0,False,4,,False,1.0,https://postgresml.org/blog/llm-based-pipelines-with-postgresml-and-dbt,dataengineering
Is there SQL questions practice website with specific topics?,15226ql,Laurence-Lin,1689600967.0,,False,True,Help,12,False,5,"Would like to find some questions practicing for DE interview. I mostly use leetcode to practice DSA, and leetcode have multiple tags like 'array', 'binary tree' when I want to practice specific topics.

However, there seems no similar tag for SQL problems on leetcode. I believe SQL have various type of questions, and I want to practice specific topics (join, aggregation, window function...etc) not wondering around and look at all kinds of problems.

Is there any website that may have specific topics to practice SQL questions?  
Or more specifically, SQL practicing websites for DE interview?  


Thanks a lot!",False,1.0,https://www.reddit.com/r/dataengineering/comments/15226ql/is_there_sql_questions_practice_website_with/,dataengineering
META: What's with the doomposting?,151uv83,FlowOfAir,1689578738.0,,False,True,Discussion,12,False,14,"I've noticed a scary trend lately in this sub where every now and then someone pops out fearing for the future of this career or how we'll be replaced by AI. I know anecdotal evidence is crap, but at least I haven't seen a lower demand of dataengs, I can still find a decent number of job postings requesting dataengs for companies. Am I missing something? What's going on? I can understand that companies are demanding experienced dataengs, but for real, this field was never meant for newly grads.",False,0.86,https://www.reddit.com/r/dataengineering/comments/151uv83/meta_whats_with_the_doomposting/,dataengineering
Real-time Data and AI using open source tools Hackathon!,1520dzi,Glittering_Bug105,1689596371.0,,False,True,Blog,0,False,5,"Hey everyone inviting you to join our Hackathon!!  
In this hackathon, you are going to create a wildfire early warning system for the fictional island nation of Zakar. Zakar has been struggling with wildfires in the last few years. 

The best project will get the perfect gaming package!

for more info - [Save Zakar Hackathon](https://medium.com/memphis-dev/you-have-a-chance-to-save-the-world-d789dcd55f87)

[Sign up](https://www.hackathon.memphis.dev/)",False,1.0,https://www.reddit.com/r/dataengineering/comments/1520dzi/realtime_data_and_ai_using_open_source_tools/,dataengineering
Unified pipeline monitoring tool / all-in-one pipeline status page,152cldb,WEI3M,1689624752.0,,False,True,Discussion,0,False,1,"I've been thinking of building such a tool that can

\- Aggregate all pipeline status on one page

\- Manage notifications for different systems

\- Unify incident reports

I did some research only to find a 3 yo post. [https://www.reddit.com/r/dataengineering/comments/jkzxfy/pipeline\_monitoring\_tool/](https://www.reddit.com/r/dataengineering/comments/jkzxfy/pipeline_monitoring_tool/)

I'm wondering if there's existing tools doing this / if not, anyone has the need?",False,1.0,https://www.reddit.com/r/dataengineering/comments/152cldb/unified_pipeline_monitoring_tool_allinone/,dataengineering
How to not hate your job when you’re the only DE supporting a non-tech company?,151svfk,jadedorca,1689572320.0,,False,True,Career,10,False,11,"I know that it’s just a job and I should just take the pay check and do my 9-5, but I’m starting to despise my non technical teammates and managers. I’ve had to make all design decisions and train new hires when it comes to data. My manager does not have the technical knowledge to offer any advice or plan any forward looking strategy, when there are tons of opportunities to modernize our stack.

Looking for advice from folks who’s been in similar situations on ways to navigate being the only technical person supporting the entire data infrastructure of the company. How do I make the best out of my situation until I leave? Thanks!",False,0.93,https://www.reddit.com/r/dataengineering/comments/151svfk/how_to_not_hate_your_job_when_youre_the_only_de/,dataengineering
Is web scraping even an authentic data source to move forward with?or buying the data/API worth it?,1520xun,Grand-Knowledge-4044,1689597849.0,,False,True,Discussion,13,False,3,"So my company which is a startup is hiring web scrapers to scrape data(financial data) from the websites, and load into S3 and as usual it goes to the database.
Will this even going to work?",False,1.0,https://www.reddit.com/r/dataengineering/comments/1520xun/is_web_scraping_even_an_authentic_data_source_to/,dataengineering
Not able to import bacpac files,1527uzc,Ecstatic-Cat-5830,1689613964.0,,False,False,Help,3,False,0,,False,0.5,https://i.redd.it/6uixe56s5kcb1.jpg,dataengineering
The simplest way to orchestrate SQL data pipelines,152d4j2,WildShallot,1689625944.0,,False,True,Blog,1,False,0,"I'm one of the co-founders of Vertis.

In my previous role as a data engineer, I saw the daily challenges our analytics teams were facing in orchestrating, deploying and monitoring SQL pipelines.

Most existing tools have a steep learning curve and often require ongoing engineering support. That's why we built Vertis. Our solution allows you to orchestrate SQL pipelines in a simple, intuitive way - all without any extra configuration, no special syntax to learn, and no infrastructure to manage.

Vertis offers an all-in-one platform, with features such as:

GitHub Integration, logging and alerting, interactive dependency graph, and more!

You can learn more about Vertis and request early access here: [https://www.vertis.app](https://www.vertis.app/)

We're excited to share Vertis with you today, and we would appreciate your thoughts and feedback. I'll be here to answer any questions you might have.",False,0.42,https://www.reddit.com/r/dataengineering/comments/152d4j2/the_simplest_way_to_orchestrate_sql_data_pipelines/,dataengineering
Do you really need exactly-once delivery?,15270yt,sap1enz,1689612063.0,,False,False,Blog,2,False,1,,False,1.0,https://streamingdata.substack.com/p/do-you-really-need-exactly-once-delivery,dataengineering
Differences between Feature Store and Data Catalog in Databricks,151v5uw,north_pr,1689579758.0,,False,True,Discussion,2,False,7,"Hi everyone,

Our small team has just finished the data preparation phase of our project and started data analysis in Databricks. As we go deeper into this field, we're trying to understand the distinctions and appropriate uses for a Feature Store versus a Data Catalog. On the surface, it seems to me that both are ways to manage and use tables of data.

For example, let's consider a churn prediction. We need to create a table (or several) for churn prediction and join later, which would include multiple features that could potentially influence a user's possibility to churn. It appears to me that we could just create a new table in a Data Catalog with this information. However, I am not sure if this is the right approach or if a Feature Store could be more suitable.

Here are my questions

1. In what situations would we prefer to use a Feature Store over a Data Catalog, or vice versa?
2. In the context of our Churn Prediction example, would it be more beneficial to use a Feature Store? If so, why?
3. If we were to use a Feature Store in this scenario, what would that look like in terms of setup and workflow?

Really appreciate every help, ideally, any examples would be great. I'm very noob using Databricks, and a real-world approach would be very very helpful.

Thanks for the help in advance.",False,1.0,https://www.reddit.com/r/dataengineering/comments/151v5uw/differences_between_feature_store_and_data/,dataengineering
Lakehouse - event data,1525pib,romanzdk,1689609082.0,,False,True,Discussion,5,False,1,How is event data supposed to be handled in the lakehouse architecture? It is not efficient to save lots of small events into the object storage so maybe I save them into some relational database first and then create a parquet dump or something like that?,False,1.0,https://www.reddit.com/r/dataengineering/comments/1525pib/lakehouse_event_data/,dataengineering
Considering doing a part time online masters in Data Science or AI,1525dkl,LaidbackLuke77,1689608331.0,,False,True,Career,0,False,0,"I have a bachelor’s degree in aerospace engineering and I’ve been working as a Data Engineer for 2 years.

I’m interested in pursuing a master’s degree in AI or data science, but I don’t want to quit my job or relocate. I’ve been looking at some online programs that offer part-time options, but I’m not sure if they are worth the time and money.

What are the pros and cons of doing a part-time online master’s degree in AI or data science?

How will it affect my career prospects and salary potential?

Is a full time program valued more?

I would appreciate any advice or insights from people who have done or are doing a part-time online master’s degree in AI or data science. Thanks!",False,0.5,https://www.reddit.com/r/dataengineering/comments/1525dkl/considering_doing_a_part_time_online_masters_in/,dataengineering
Anyone know spark projects to work with > 400 gb datasets and performance tuning?,151nfbf,OptimistCherry,1689556076.0,,False,True,Help,3,False,15,"Hi, want to learn spark in a bit more detail with large datasets, till now I dabbled in few mb projects doing some exploratory analysis on community edition of databricks, looking for any project with huge dataset to actually learn what skewness in data mean and how to tune performance, any practical videos you know? Also anyway where I can play with huge datasets for free? AWS EMR and databricks seems to cost a lot for this.",False,0.94,https://www.reddit.com/r/dataengineering/comments/151nfbf/anyone_know_spark_projects_to_work_with_400_gb/,dataengineering
Best way to surface dbt docs?,151utkk,Wide-Pop6050,1689578575.0,,False,True,Discussion,4,False,4,"My team uses dbt and would like to share the documentation created with other teams in our company. Those teams don't have access to our dbt. 

I'm trying to figure out a way to do this that is private - only people with the link can access it. I looked into github pages but did not like that a github page is pretty easy to guess the url for. We don't have enterprise Github. 

Would the best option be to pay for Netlify?",False,1.0,https://www.reddit.com/r/dataengineering/comments/151utkk/best_way_to_surface_dbt_docs/,dataengineering
I made a Stock Market Dashboard,151edrn,hieuimba,1689534039.0,,1689554881.0,True,Personal Project Showcase,14,False,38,"Coming from a finance background, I've always been interested in trading & investing.As I switch to tech and data for my career, I wanted to create my very first DE project that combines these two interests of mine:[https://github.com/hieuimba/stock-mkt-dashboard](https://github.com/hieuimba/stock-mkt-dashboard)

I'm proud of how it turned out and I would appreciate any feedback & improvement ideas!Also, where do I go from here? I want to get my hands on larger datasets and work with more complex tools so how do I expand given my existing stack?",False,0.9,https://www.reddit.com/r/dataengineering/comments/151edrn/i_made_a_stock_market_dashboard/,dataengineering
"Advice on Building an ETL Pipeline with AWS S3, Postgres, and Pyspark",151yksz,Data_is_fuel,1689591190.0,,False,True,Help,5,False,2," Hello, everyone!

I am currently working on a project to build an ETL (Extract, Transform, Load) pipeline, and I would greatly appreciate some advice and guidance from the Reddit community.

Here's a brief overview of the project: I am tasked with downloading data from an AWS S3 bucket, performing transformations using Pyspark, and loading the transformed data into a Postgres database. This is my first project of this nature, and I need to schedule this task to run automatically on a weekly basis.

One specific challenge I am facing is the need to compare the last six months of data in the S3 bucket with the data already present in the Postgres database. If any changes are detected in the S3 data, I need to overwrite the corresponding data in the Postgres database. The amount of data can range between 15-20 GB.

I am currently using Pyspark for data transformations, but I would appreciate any recommendations or best practices on efficiently comparing and updating the data between S3 and Postgres.

Additionally, I am in the process of selecting a suitable tool for scheduling and managing ETL tasks. I have set up Airflow, but I am uncertain whether it is the best fit for this project. I would love to hear your thoughts and experiences with scheduling ETL tasks, particularly for a scenario like mine.

Thank you in advance for any advice, suggestions, or insights you can provide. I am eager to learn from your expertise and make this ETL pipeline project a success!",False,1.0,https://www.reddit.com/r/dataengineering/comments/151yksz/advice_on_building_an_etl_pipeline_with_aws_s3/,dataengineering
What is best way to change existing schema in delta[Databricks],151voz1,mannu_11,1689581588.0,,False,True,Help,1,False,3,"hey guys,
We have a table in delta format where we have enabled schema evolution. 
Lets consider a table having column X which is struct and have multi level nested keys. 

E.g. - X.Y.Z.A

Now, Databricks is trying to convert Z to struct, I want it to convert it to String
Z has 10k keys. 

How do we solve for this?

We’re following this approach:
1. get ddl for table 
2. recreate the table with schema of Z as string.",False,1.0,https://www.reddit.com/r/dataengineering/comments/151voz1/what_is_best_way_to_change_existing_schema_in/,dataengineering
snowflake employment help for my wife,1520p8f,Pbd1194,1689597208.0,,False,True,Career,9,False,1,"I am looking to help my wife switch her domains. she is currently a FTE at infosys with \~6 years experience in the IT Services domain. She spent 5 years at IBM before joining Infy in same project.

She has extensive onsite experience where she worked with IBM Australia with Westpac back.

As expected, she has stagnated massively and will become completely disconnected with tech if she continues in these companies. One rather seamless switch that we feel she can pursue is moving to Snowflake platform and pursuing a career in either dev or as an analyst.

She has been following tuts from multiple resources and aiming a certification as well, but lack of enough hands-on experience is making her nervous and preventing her from focussing on studies.

Given the current market situation, job opportunities are not easy either.

How can she explore internal switch within Infy to pursue Snowflake? Otherwise, how can she overall pursue this switch? We are f9 with her taking up contract positions as well as long as she gets to work on Snowflake.",False,0.55,https://www.reddit.com/r/dataengineering/comments/1520p8f/snowflake_employment_help_for_my_wife/,dataengineering
Microsoft Purview not showing complete lineage,151s5fb,Dev-98,1689570006.0,,False,True,Help,1,False,3,We have connected our ADF pipelines with Purview but the complete lineage is not being shown in Purview. We are able to see the flow from Stage to Interim but not from Interim to Target.,False,1.0,https://www.reddit.com/r/dataengineering/comments/151s5fb/microsoft_purview_not_showing_complete_lineage/,dataengineering
finqual: Python project to simplify fundamental financial research with SEC data,151ly9j,Myztika,1689552187.0,,1689552764.0,True,Open Source,7,False,7,"Hey, Reddit!

I wanted to share my Python package called finqual that I've been working on for the past few months. It's designed to simplify your financial analysis by providing easy access to income statements, balance sheets, and cash flow information for the majority of ticker's listed on the NASDAQ or NYSE by using the SEC's data.

**Features:**

* Call income statements, balance sheets, or cash flow statements for the majority of companies
* Retrieve both annual and quarterly financial statements for a specified period
* Easily see essential financial ratios for a chosen ticker, enabling you to assess liquidity, profitability, and valuation metrics with ease.
* Retrieve comparable companies for a chosen ticker based on SIC codes
* Tailored balance sheet specifically for banks and other financial services firms
* Fast calls of up to 10 requests per second
* No call restrictions whatsoever

You can find my PyPi package here which contains more information on how to use it: [https://pypi.org/project/finqual/](https://pypi.org/project/finqual/)

And install it with:

    pip install finqual

**Why have I made this?**

As someone who's interested in financial analysis and Python programming, I was interested in collating fundamental data for stocks and doing analysis on them. However, I found that the majority of free providers have a limited rate call, or an upper limit call amount for a certain time frame (usually a day).

**Disclaimer**

This is my first Python project and my first time using PyPI, and it is **still very much in development**! Some of the data won't be entirely accurate, this is due to the way that the SEC's data is set-up and how each company has their own individual taxonomy. I have done my best over the past few months to create a hierarchical tree that can generalize most companies well, but this is by no means perfect.

There is definitely still work to be done, and I will be making updates when I have the time.

It would be great to get your feedback and thoughts on this!

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/151ly9j/finqual_python_project_to_simplify_fundamental/,dataengineering
[Interview] Data pipeline design round,151s5ki,Delicious_Attempt_99,1689570019.0,,False,True,Interview,6,False,2,"Hi All,

As you read it from title, I have an interview round ( which is 2nd round ) on designing the data pipelines. The interviewer told me, there wont be any live coding round, but we would design a data pipeline. Can you please help with your experience on what all should we be prepared? Any resources will help me a lot

Thanks in advance :)",False,1.0,https://www.reddit.com/r/dataengineering/comments/151s5ki/interview_data_pipeline_design_round/,dataengineering
"Is this fear-mongering, or is this actually truthful?",150qcx2,Analyst2163,1689463491.0,,False,False,Discussion,132,False,250,,False,0.97,https://i.redd.it/poz94nkcq7cb1.png,dataengineering
Anyone want to partake in making a cracked Parquet PoC?,1518w59,mwlon,1689520768.0,,False,True,Discussion,8,False,12,"Parquet is nice, but it does a bad job at compressing columns of numerical data. It's a real shame, since exabytes of data are written in Parquet and similar formats. I made a codec (https://github.com/mwlon/pcodec) that can be wrapped in as an ""encoder"" for better compression ratio at no cost to compression/decompression speed on average (compared to .zstd.parquet).

I'm planning to test out a forked parquet using pcodec. Let me know if you're interested in collaborating.",False,0.94,https://www.reddit.com/r/dataengineering/comments/1518w59/anyone_want_to_partake_in_making_a_cracked/,dataengineering
People who went to Snowflake Summit or Databricks Summit: what did you learn and Why did you pick one vs the other?,151qzhy,haragoshi,1689566417.0,,False,True,Discussion,2,False,1,"Two of the top Data warehousing vendors held summits. Both these summits happened at the same time a few weeks ago. 

Who went?  Why did you pick one vs the other?  What were the highlights to you?

https://www.databricks.com/dataaisummit/

https://www.snowflake.com/summit/livestream/",False,0.67,https://www.reddit.com/r/dataengineering/comments/151qzhy/people_who_went_to_snowflake_summit_or_databricks/,dataengineering
Implementing date dimensions in DW?,151he4d,jazzopardi203,1689541032.0,,False,True,Help,8,False,3,"Hi everyone,

Working on my first data warehouse project and learning online on the way using tutorials, articles, youtube videos etc.

I'm using data meant to mimic an OLTP database for a dvd rental company, and I'm at the stage of implementing my dimension and fact tables.

The fact table I'm currently building out is an inventory table - I want it to be a snapshot every month of a dvd's stock, including how many dvd were lost that month,  and some other aggregates.

One thing I can't wrap my head around though is how to implement a date dimension - currently none of my other attributes capture a timestamp that can facilitate this.

and tutorials online don't really go into the details of how to build a snapshot fact table so that i can define the grain as 'week', 'month' etc.

I don't understand how my fact table can make a reference to a date dimension if my other dimensions don't contain a date attribute?

I'm attaching a photo of my model below in case it helps paint a better picture.

https://preview.redd.it/f957h9t15ecb1.png?width=1376&format=png&auto=webp&s=17ffdf934ba9650196211d46ee299cf8ac86d18b",False,1.0,https://www.reddit.com/r/dataengineering/comments/151he4d/implementing_date_dimensions_in_dw/,dataengineering
There should be data engineering courses in colleges,150yzmo,shaunyip,1689489867.0,,False,True,Discussion,11,False,29,"Current software engineering education in colleges still targets building up OLTP systems.As a graduate you may know ACID, OS details, remote RPC really well, but after working 10 years, there is still a chance that you don't know what ""tumbling window"" is.

Why not just learn data engineering in your own time, you may ask.  The problem is they are ""unknown unknowns"".  You don't know you need to think of solutions from data engineer point of view.

Most software engineers only know that DE is just used for data science, is about building data pipelines for them. They don't know that DE patterns can actually help in SE domain.

One example is a project I took part in. We were supposed to build a real time trading risk monitoring system. Nobody was aware of Kafka Streams so we just handled the incoming data using plain Java code, stored the time window in a nosql db and used a single processing application, no cluster. 

Ironically the DE team was part of the project. And we just treated them as a downstream and sent some audit information to generate reports.

That's typical in the industry. There is a big gap between SE and DE.  Why? Because lots of people are not trained in school for the DE solutions and not lucky enough to meet colleagues who are aware of this.",False,0.82,https://www.reddit.com/r/dataengineering/comments/150yzmo/there_should_be_data_engineering_courses_in/,dataengineering
Why do data engineers have so much to learn?,150mfrd,Hankaul,1689453652.0,,1689469567.0,True,Discussion,94,False,118,"I am a student preparing to get a job as a data engineer.

1. linux, Python, SQL, JAVA or Scala
2. Cloud Knowledge
3. Docker, Kubernetes
4. Server security + data security/quality
5. Database (""Cassandra"", ""Mongo"", ""Mysql"", ""postgres"", ""redis"")
6. ELK or (fluentd, Opensearch)
7. Kafka
8. Spark Stream or Flink
9. Spark or Trino
10. table format ( Iceberg/deltalake/ Hudi)
11. snowflake/ Redshfit / Bigquery   
12.  OLAP Data Modeling  
13. Airflow

  
I only wrote down the essentials.  
There are many good tools like dbt, lakeFS, etcBut in addition  
Companies may still require Hadoop Echo System (Hive/HDFS/Hbase).  
They might ask us to build a dashboard with javascript or python.  
They can also ask us to create the web.  


From number 1 to 13, each one has a very difficult and difficult concept to master.What I'm really curious about is how much I need to know and how much I need to master them to apply for a company.

I'm sorry. Actually, I was whining because I got hit with reality while studying.",False,0.84,https://www.reddit.com/r/dataengineering/comments/150mfrd/why_do_data_engineers_have_so_much_to_learn/,dataengineering
Airbyte in Production - Robust?,1510pua,No-Dress-3160,1689495814.0,,False,True,Discussion,27,False,9,"Hi everybody,

I’m making a POC  using Airbyte as a spun a VM in GCP.

I noticed a lot of connectors were alpha or beta. How robust is the tool? Has anyone been using it in a Fortune 500 company?",False,0.86,https://www.reddit.com/r/dataengineering/comments/1510pua/airbyte_in_production_robust/,dataengineering
Key/value store design for integer keys,151d8m7,chris_ochs,1689531306.0,,False,True,Help,0,False,1,"Thought I'd post this here for review to see if others see obvious flaws or better approaches.

The larger context is this store is internal to another more complex store that has specific needs for it's permanent storage.

Keys are always integers.  Values are fixed length byte arrays.  Efficiency matters but volume is fairly low 10k reads/writes second  at most.  This would be using SSD volumes dedicated for this data.

Other off the shelf stores tend to have caveats.  LSM tends to scale poorly with larger data sets, and most other stores just have a lot of overhead and complexity around stuff I don't need.

The basic file format is a file is assigned a key range.  And the start of the file contains an array covering the entire range.  Each array index contains a 1 byte flag and a 4 byte value containing the block index into the actual data.

The data area starts immediately after the index array and is variable.  It's comprised of fixed length blocks that are sized in a multiple of the disk block size.

Inserts are just appended into the data area.  Compaction is fairly straight forward move active blocks from the end to unused blocks. Implementation probably not important so won't cover that.

Allocating the index up front will always have wasted space in the file with the highest key range.  But it's a flexible design that allows for arbitrary ranges and merging files if needed.  I'm ok with the amount of wasted space here since it's relatively small.

An insert will append to the data area and return the write offset.  That is then written into the index.  An update will seek/read the index and then seek/write into the value area.

Reading is two seeks/reads.",False,1.0,https://www.reddit.com/r/dataengineering/comments/151d8m7/keyvalue_store_design_for_integer_keys/,dataengineering
Data Modeling Snowflake,150x8zd,Used_Ad_2628,1689484069.0,,False,True,Help,11,False,10,I am joining a company that uses Snowflake. My past experiences have been with Teradata and Redshift. Any advice on the best way to model the data for performance and cost savings? It will be used for self service reporting with BI tools. Star schema or wide table? Any indexes or partition strategy?,False,0.87,https://www.reddit.com/r/dataengineering/comments/150x8zd/data_modeling_snowflake/,dataengineering
Implementing a functionality similar to haveibeenpwned.com,151bfxc,leShawarmaMan,1689527042.0,,False,True,Help,4,False,1," I'm working on a project that involves storing a large combolist in a database and implementing efficient search functionality for user email addresses within the combolist. The combolist can scale to tens of gigabytes and include billions of entries, and I want to achieve the lowest search time possible.

A combolist in cybersecurity terms is a collection of leaked credentials that comes in email,password pairs and it comes in this form:

\`email:password\`

In short; I'm trying to implement a functionality similar to websites like [haveibeenpwned.com](https://haveibeenpwned.com/) and [dehashed.com](https://dehashed.com/) where a user can lookup if their email has been leaked in a data breach.

What is the approach that I should follow here?",False,1.0,https://www.reddit.com/r/dataengineering/comments/151bfxc/implementing_a_functionality_similar_to/,dataengineering
"Why use ""GROUP BY 1""?",150korq,aria_____51,1689449247.0,,1689548067.0,True,Discussion,81,False,44,"I'm going through some of the dbt training courses on their website. Across multiple videos and presenters, they seem to use the syntax ""GROUP BY 1"" in their SQL code. I honestly had to Google wtf that meant lol.

Please correct me if I'm overgeneralizing, but it seems like in almost every case, you should just use the column name in the group by clause. 

I'm very new to dbt, so please let me know if there's a good reason to use GROUP BY 1 rather than the column name.

Edit: Appreciate everyone's responses! As I suspected, there's a lot of reasons one would do it that I hadn't thought of. Really interesting to get everyone's thoughts. Great subreddit!!",False,0.89,https://www.reddit.com/r/dataengineering/comments/150korq/why_use_group_by_1/,dataengineering
Neeed help with a project pls !!!!,1519m26,sam-sinister,1689522572.0,,False,True,Help,0,False,0,"It's a simple project I have a data as hudi table in s3 partitioned on year month day 
Want to create a iceberg table of out of it partitioned on field capturedate Nd save it on another s3 bucket 
I'm using aws glue 

I have tried multiple ways even last resort chatgpt also failed 
I donnot have the permission to use the iceberg connector from aws market place pls suggest how to solve and an example links anything is welcomed need to submit tomorrow my job is a stake, I'm new to DE so encountering problems 

If push come to shove I'll get aws connector bit if you can suggest all the possible ways with or without connector as well is welcomed !!!! 
Pls help 

Thanks in advance 
I'll ans if anyone has any queries 


Thanks !!!!",False,0.33,https://www.reddit.com/r/dataengineering/comments/1519m26/neeed_help_with_a_project_pls/,dataengineering
No more data breaches with VulcanSQL!,150yt6l,cyyeh,1689489251.0,,False,True,Open Source,5,False,4,"Hello, friends on reddit!

I would like to share with you a demo using VulcanSQL + LangChain + Cohere + Streamlit to demonstrate how VulcanSQL can easily create & share secure data APIs.

VulcanSQL has built-in data privacy mechanisms such as dynamic data masking, row/column level security, etc., to ensure your sensitive data stays where it belongs to you.

VulcanSQL: [https://vulcansql.com/](https://vulcansql.com/)  
VulcanSQL data privacy docs: [https://vulcansql.com/docs/data-privacy/overview](https://vulcansql.com/docs/data-privacy/overview)  
Demo: [https://yelp-dataset-demo.fly.dev/](https://yelp-dataset-demo.fly.dev/)  
Source code: [https://github.com/Canner/vulcan-sql-examples/tree/main/yelp-dataset-api](https://github.com/Canner/vulcan-sql-examples/tree/main/yelp-dataset-api)

The attached image is the application flow diagram of the demo  


https://preview.redd.it/z8zgv1edu9cb1.jpg?width=1460&format=pjpg&auto=webp&s=5c74cfbff7f2c100d3c017fe4e670ff64aa7fed5",False,0.75,https://www.reddit.com/r/dataengineering/comments/150yt6l/no_more_data_breaches_with_vulcansql/,dataengineering
Recommendations for computer to learn Data Engineering?,150yoxn,Weary-Individual-309,1689488823.0,,False,True,Help,14,False,4,"I’m about to embark on my self taught DE journey and I just realized I haven’t had my own personal computer since college.

I am having a hard time between navigating between Mac’s, Dells etc and was wondering if anyone could recommend a laptop (preferably less than 1K) which would make my journey seamless (I’ve heard certain laptops or operating systems make installing some DE tools/environments a nightmare).


Again sincerely appreciate!",False,0.83,https://www.reddit.com/r/dataengineering/comments/150yoxn/recommendations_for_computer_to_learn_data/,dataengineering
Which Profession Is Celebrated More By Society? A Data-Driven Dive into Madame Tussaud’s,1516mcp,44za12,1689514940.0,,False,False,Blog,0,False,0,,False,0.33,https://aazar.me/post/which-profession-is-celebrated-more-by-society,dataengineering
"Just got ""promoted"" from Sr DE to Sr DA ¿?",150knwq,erwingm10,1689449186.0,,False,True,Discussion,16,False,25,"This is something. I'm feeling bad since it happened yesterday.

Somehow I got promoted from a Sr Data Engineer role to a Sr Data Analyst with a salary increase. 

I mean it's good but since I do both roles it's awkward since the title should be something like analytics engineer, which is more attractive for any job seeking in the future.

I have a lot of confidence in my boss. I plan to tell him to reconsider the title based on all my functions.

What do you think about this guys? am I wrong?",False,0.97,https://www.reddit.com/r/dataengineering/comments/150knwq/just_got_promoted_from_sr_de_to_sr_da/,dataengineering
What should a DE really know in SQL to succeed in an entry level job?,150e59x,NoChemical1223,1689433102.0,,False,True,Discussion,32,False,39,"I used SQL my whole life and I don't have issues with data modeling or querying in general. But when I see jobs asking for a good level in SQL, I wonder what does good mean ? What are the items that I should really know to qualify as ready for an SQL DE job ?",False,0.92,https://www.reddit.com/r/dataengineering/comments/150e59x/what_should_a_de_really_know_in_sql_to_succeed_in/,dataengineering
Pyspark VS SSIS,1509lyr,Legitimate_Finish673,1689420528.0,,False,True,Help,36,False,32,"Hi,

I currently do my etl task with spark. It is simple ETL jobs. The amount of data is small to medium. I essentially work with on-prem data, but some may be stored in S3 buckets or Redshift.

I have good knowledge of SQL.

Since I am looking for a new job, I wonder if I should learn SSIS to do ETL, since a lot of companies use it, or is AWS Glue and Pyspark is sufficient to tackle ETL tasks.

Is it worth to learning SSIS ? What are the pros and cons ?

Thank you.",False,0.95,https://www.reddit.com/r/dataengineering/comments/1509lyr/pyspark_vs_ssis/,dataengineering
Should I skim cs50 or do all the work?,150gzsj,Icy-Big2472,1689440176.0,,False,True,Career,7,False,11,"Currently learning data engineering and taking cs50, should I just watch all the videos? Or should I go through all the practice problems? I'm currently on week 4 and have been going through the practice problems, but that's moving pretty slowly since I'm also reading a textbook on data warehousing. Would it be better to just watch all the videos on cs50 so I can move through it quicker?

For some background info, I'm a data analyst with no degree. I no longer do any type of data analysis, instead I'm doing ETL, OLAP cube building, and automating for my departments reports. I'll also start focusing on pre-processing data on clients where our current processes can't handle the data quantity, and I'm helping come up with some data pipelines to automate some manual processes. None of this is focused on super high volume data.

I want to build a strong foundation in computer science since I have no degree, but I also want to learn at a decent rate since I'm already doing some data engineering to an extent.

tl;dr - If you were learning data engineering again and taking cs50, would you just watch the lectures or do all the work?",False,0.72,https://www.reddit.com/r/dataengineering/comments/150gzsj/should_i_skim_cs50_or_do_all_the_work/,dataengineering
Where data is processed when using PythonOperator?,150j1ge,NoobAllTheWay,1689445198.0,,False,True,Help,5,False,5,"I am currently trying to learn Apache Airflow and I have read and heard many times that Apache Airflow is an orchestration tool and should not be used to process data.

However, some things are not clear to me:

1. Does this mean I should not use the `PythonOperator` to transform my data but instead use something like the `SparkOperator`?
2. If I can use the `PythonOperator` to transform my data, where exactly is the process done? On a worker (if I use Celery Executor) or on a POD (if I use Kubernetes Executor)? How is that different from using Apache Airflow as a processing tool?
3. Also, If I can use the `PythonOperator` to transform my data, then how exactly does one use Apache Airflow to process data? 

&#x200B;",False,0.86,https://www.reddit.com/r/dataengineering/comments/150j1ge/where_data_is_processed_when_using_pythonoperator/,dataengineering
What Master program do you recommend I take?,150qtj0,shouldawouldacoulda0,1689464702.0,,False,True,Help,2,False,2,"Im a recent grad from the class of 2023 with a degree in Business and Marketing education. I want to pursue a career in data and fell in love with the concept of data engineering. I’ve done a Google data analytics course back in 2021 so I have familiarity with SQL, Tableau, Power BI, Excel. Im really thinking of pursing a masters in DS or in statistics but not sure what programs I should pursue to help me land my first job in data engineering. Any master program recommendations or programs you recommend would be great.",False,1.0,https://www.reddit.com/r/dataengineering/comments/150qtj0/what_master_program_do_you_recommend_i_take/,dataengineering
DBMS options for a small clinic,150jm2q,juiceleft88,1689446581.0,,False,True,Help,6,False,4,"Hi everyone, 

So I work at a small clinic and have recently been charged with consolidating our various data sources into one single database so that analytics can be performed in a fast, easier way. Our sources include excel sheets and csv files created in-house, and reports downloaded in these formats from medical web-portals. 

What would be the best way (and technology) to go about creating a single database system where I can eventually create automated systems for all these different data sources to be consolidated in one database system? 

&#x200B;

Thank you

(P.S. I do have a computer science degree, but I haven't worked too much with architecting database systems/infrastructure, mostly running SQL queries, and ML algorithms with existing datasets)",False,1.0,https://www.reddit.com/r/dataengineering/comments/150jm2q/dbms_options_for_a_small_clinic/,dataengineering
Cloud data Engineering or on-prem Business Analyst,150o5ey,TProfessional,1689457915.0,,False,True,Career,1,False,2,"I am a data engineer with 2 years of experience in Azure cloud. 

I have basic knowledge of Power BI, SQL Server, SSIS, and SSAS, I got an offer in business analysis using these on-prem tools and it's a much better salary.  

My worry is that I will switch to technologies that are getting old and cloud data engineer looks pretty prosperous in the future. 

Any suggestions

&#x200B;

[View Poll](https://www.reddit.com/poll/150o5ey)",False,0.75,https://www.reddit.com/r/dataengineering/comments/150o5ey/cloud_data_engineering_or_onprem_business_analyst/,dataengineering
Example of Incremental Sensor Project,150i6w3,No_Cover_Undercover,1689443087.0,,False,True,Discussion,4,False,5,I was curious if there are examples of projects for pulling in machine sensor data from different locations via api? Looking to do a project and specifically how to handle outages and incremental jobs for a location or certain machines that may go offline while the rest are running smoothly.,False,1.0,https://www.reddit.com/r/dataengineering/comments/150i6w3/example_of_incremental_sensor_project/,dataengineering
Small Data processing with Polars @AWS lambda,150fgvc,gabbom_XCII,1689436394.0,,False,True,Discussion,7,False,4,"So, In the company I work for we use PySpark on EMR or Glue Jobs for Big Data processing.

For small data cases we use AWS SDK for Pandas (awswrangler) on Lambda.

Is there anyway to use polars in this manner while also reading and writing to Glue Catalog tables?",False,0.84,https://www.reddit.com/r/dataengineering/comments/150fgvc/small_data_processing_with_polars_aws_lambda/,dataengineering
full stack DE,150cc7j,puzzled-cognition,1689428407.0,,False,True,Discussion,13,False,7,"Hello, 

I recently joined a company where I am given the role of a data engineer. They have plans of converting it to a full-stack data engineer role (not really sure if that's a thing). This decision was made entirely by them without any prior discussion. Was eventually let to know but I wasn't told this at the time of joining. I have mainly worked in backend development and data engineering space before. I am worried they would move me completely to frontend which I neither have prior experience in nor the inclination. Their explanation is that it would give me an end-to-end picture of the product and also reduce dependency on FE team for backend features we have worked on. But I am not particularly interested in adding on that skill set and instead focus on learning what else there is in the area I have been working so far. But the following thoughts are worrying me:  
1. Will saying no be taken as ""I don't wish to grow and learn""   
2. I am okay with maybe ramping up to resolve bugs and add small features but being part of a complete design story or overhaul is not something I am interested in but the discussion indicated it might be the case in the future if it comes to that.  


If given a choice I would avoid frontend work. But wanted to know what the general opinion is around this and what would be suggested. 

TIA",False,1.0,https://www.reddit.com/r/dataengineering/comments/150cc7j/full_stack_de/,dataengineering
Simplest solution for self managed spark cluster on Azure,150budt,Pancakeman123000,1689427048.0,,False,True,Help,9,False,8,"My team have some large workloads which we are currently running on Azure Databricks. On an ad-hoc hasis, we occasionally do some large simulations for analytics purposes over all of our historic data which require large costly clusters. It works ok, but since we use cheap Spot instances, 80% of the resulting bill is for the Databricks DBU cost rather than the underlying infrastructure. 

Since the jobs don't rely on any of the Databricks closed-source tech, and they're ad-hoc so don't form part of any other integrated workflows, I'd like to see if there's a good option for running these jobs on self managed clusters so that I can cut down the bill.

Having done some research, there was historically a library called [AZTK](https://github.com/Azure/aztk) (Azure Distributed Data Engineering Toolkit) which provided a simple CLI for creating Spark Clusters on Azure Batch. However, it's no longer being maintained.

Does anyone have any other recommendations?",False,1.0,https://www.reddit.com/r/dataengineering/comments/150budt/simplest_solution_for_self_managed_spark_cluster/,dataengineering
Jobs to just coast,14zzmfq,TAno15,1689388368.0,,False,True,Career,28,False,47,"I legit don't care for the hustle and bustle of promotions and improving shit anymore. I'm super content just being a worker bee.

Any industries or employers where DEs can just coast that pay well?",False,0.94,https://www.reddit.com/r/dataengineering/comments/14zzmfq/jobs_to_just_coast/,dataengineering
"It's not a glamorous life, but we all know who really drives the bus",14zh0hc,itty-bitty-birdy-tb,1689342659.0,,False,False,Meme,51,False,352,,False,0.97,https://i.redd.it/s3fg4dwiqxbb1.png,dataengineering
Do you backup your S3 data?,14zpfq8,Affectionate-Day-240,1689362383.0,,False,False,Meme,12,False,94,,False,0.95,https://i.redd.it/om6ea88jdzbb1.jpg,dataengineering
Syndigo MDM,150f1yp,Federal_Ad179,1689435350.0,,False,True,Career,2,False,2,"Does anyone here familiar working with Syndigo PXM or MDM solution? 

I’m starting thinking if is this really the future career? 

I need your advise what are the trends for DE role. 😊",False,1.0,https://www.reddit.com/r/dataengineering/comments/150f1yp/syndigo_mdm/,dataengineering
"I’m the de facto data engineer at a small startup (50 employees), but I don’t feel like I could be competitive for a “real” data engineering job elsewhere.",14zn3b0,Reasonable_Tooth_501,1689356825.0,,1689366724.0,True,Discussion,36,False,65,"Was hired as a many-hats data analyst.

For the last few years, I’ve:
-Worked with SQL/Spark/Python
-Built/managed countless ELTs in Databricks
-Designed architecture and built tables for BI and ML purposes
-Managed AWS Glue/S3 pipeline that replicates our prod database (eventually into Databricks)
-Built pipelines that read/write to multiple tools (eg Salesforce, Marketo, Google Sheets) via APIs

But the areas I’m lacking are:
-Terabytes of data. At this point Ive only worked with gbs.
-Best practices. My team was laid off so I no longer get feedback when I could be doing stuff better/more optimally. Not to mention data governance and all that jazz. 

But yeah, I’m wondering how much of my concern is legitimate and how much is imposter syndrome? Current title isn’t an issue—I could change that if I want. But when were you actually “ready” and competitive enough to go for data engineer gigs?",False,0.97,https://www.reddit.com/r/dataengineering/comments/14zn3b0/im_the_de_facto_data_engineer_at_a_small_startup/,dataengineering
Unit Testing for data engineers,150dzb6,philonoist,1689432670.0,,False,False,Blog,2,False,0,,False,0.43,https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers,dataengineering
Understanding fail-safe in Snowflake,15052dq,inglocines,1689405440.0,,False,True,Discussion,8,False,2,"I am new to Snowflake and I was trying to read about different types of tables. I understood how fail-safe is an internal mechanism in Snowflake that is kept for operational purposes.

 But the fail-safe is for 7 days beyond the retention period is what Snowflake docs mention. So in that case, if my retention period is 7 days, then fail-safe will be for another 7 days, which means I have to pay storage costs for 14 days of data for each table right? 

Why can't fail-safe be just as retention period and not on top of retention period? This will save costs considerably if it is a large table. 

PS: I know about fail-safe not being a option that can be leveraged by developers. In case of operational failures, Snowflake support team can help recovering the data and that's the reason for fail-safe. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/15052dq/understanding_failsafe_in_snowflake/,dataengineering
Chat-GPT plugins and Data Engineering,150dwlj,philonoist,1689432477.0,,False,False,Blog,0,False,0,,False,0.43,https://juhache.substack.com/p/chat-gpt-plugins-and-data-engineering,dataengineering
M1 vs M2 MBA for tasks relating to PySpark or Apache Airflow?,15082zu,Prudent-Writing-5724,1689415630.0,,False,True,Help,17,False,1,"I want to purchase a MacBook for my basic tasks relating to data engineering. For instance, running Apache Spark Server and executing PySpark jobs and/or running Apache Airflow Server and executing the code for workflow orchestration.

Currently, I am using a virtual machine inside my Windows Laptop. So, I am sure that any MacBook Air will outperform deadly with respect to the setup that I am currently using. Plus, I have had enough of reading reviews and watching comparisons between M1 Air and M2 Air.

This I am posting because I want to get to know the experience from the personal user instead.

I am confused about purchasing which MacBook Air only for personal use considering I will not be replacing it in a year or two.",False,0.57,https://www.reddit.com/r/dataengineering/comments/15082zu/m1_vs_m2_mba_for_tasks_relating_to_pyspark_or/,dataengineering
Anyone using ByteBase for schema management?,14zob6a,generic-d-engineer,1689359742.0,,False,True,Open Source,1,False,9,"This looks pretty good. It’s CI/CD for Database schema management. Was looking at it after searching for a change management platform supporting MongoDB.

https://www.bytebase.com/docs/introduction/what-is-bytebase/

Also it seems to understand the entire lifecycle from a data professional’s perspective, has a read only table viewer, so that can help with not having to provision so many different client tools.

I couldn’t find any feedback on Reddit about this product. Seems most people are using stuff like Liquibase or Flyway.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zob6a/anyone_using_bytebase_for_schema_management/,dataengineering
Job Title Question,14zzq93,WorldlyDirt5024,1689388679.0,,False,True,Career,4,False,2,"Hi everyone,

I’ve been seeing quite often how much variety Data Engineer responsibilities can be. Some can be more of an Analytics Engineer who is focused on metrics and dashboards, while also building pipelines. While others are more of a SWE focused on data. 

My question comes with the SWE Data Engineer. Obviously SQL and Python are very important, but are Data Engineers who have more SWE responsibilities, more Python heavy, or SQL heavy, or both equally? I understand this varies by company, but I’m curious.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zzq93/job_title_question/,dataengineering
SQL only - Work frustration,14z8hhn,xchgre,1689315999.0,,False,True,Discussion,79,False,77,"A few months ago I changed from data scientist to engineer, since in my previous company I did everything myself, but being a CS guy I pref the data engineering part.

In my current company, a multinational telecommunications, I only do SQL queries and stupid dashboards for marketing and business, I feel frustrated because anyone can do this…  And it has little to do with the Data Engineer role that I was offered.

How much do you use SQL in your daily work ?",False,0.91,https://www.reddit.com/r/dataengineering/comments/14z8hhn/sql_only_work_frustration/,dataengineering
Are My Skills Too Far Gone?,14zgfh5,Alternative-Engine77,1689341155.0,,1689381854.0,True,Career,26,False,14,"I was a Business Intelligence Analyst/SQL developer from 2016-2021. I was working mostly with SSIS and Microsoft SQL Server. Cron + Powershell were my best frenemies. I have a fundamental understanding of data structures, patterns, etc; know SQL very well and have a cursory understanding of Python—I’ve done some personal projects but never needed it for work. I’m also pretty familiar with Git and general SDLC best practices. 

I worked at two “tech enabled” companies where their primary business was selling a service that was “data-driven” and backed with “cutting edge tech” but in reality the tech was a giant spaghetti mess behind the scenes and probably 7+ years behind current day at all times. These companies had terrible on-call practices and that plus spaghetti architecture, I was getting constantly pinged after hours for this or that broken thing—a lot of times late at night. In short, I got fully burnt out. 

Fast forward to today, I ended up landing a TPM role at a company whose tech stack is current day. They have their problems like anywhere else but from what I can tell, the Eng teams have mature on call procedures that actually split up the work. I like this company but despite “technical” in the name, the TPM role here is not at all technical. Now I’m working through whether or not it would even be possible to transition over to their DW team and be in more of an Eng role again.

I know I’ve got some skills gaps to fill.  Airflow, Big Query and Python are going to be key for this role and I’ve never worked with them day to day. In reality, if that team even wants me I’d probably need to step down in seniority a level or two and I’d be fine with that. But what I’m trying to work out is, with these more modern tools, is the job really going to be THAT different? Do I need to set the expectation for myself that if I pursue this, I am essentially starting from scratch? I think what really worries me is that I do not have a formal CS background—in my prior life I was totally self taught, so I fear there might be some knowledge gaps I don’t even know I have.

EDIT: Thank you for all of the replies! There’s some really good advice here. Imposter syndrome is so real and can sneak up on you even if you’re aware. This thread actually reminded me that I bought the “100 Days of Python” course from Udemy last year during a flash sale. I started it but then had a bunch of life events happen back to back and it got dropped in the priority list. I’ll pick that back up to start and see where it takes me.",False,0.86,https://www.reddit.com/r/dataengineering/comments/14zgfh5/are_my_skills_too_far_gone/,dataengineering
Overwhelmed with such detailed job descriptions,14zhtx1,frustratedhu,1689344668.0,,1689347877.0,True,Help,19,False,9,"I am currently a data analyst with 2 YOE. I have worked on SQL, python (for retrieving and transformaing the data for analysis),excel and Tableau. I feel that i would be able to do better as a data engineer. So I taught myself spark, hive, azure data factory and databricks (fundamental). Now when I tried to analyse the market requirements for data engineer, I am overwhelmed with the different requirements they have (airflow, Kafka, warehousing , DBT and a lot). I feel like I would have to spend another year to catch up with the requirements. 
How should I plan? I feel like I am lost. 
Please help",False,0.92,https://www.reddit.com/r/dataengineering/comments/14zhtx1/overwhelmed_with_such_detailed_job_descriptions/,dataengineering
"After balking for a long time, tried out dbt for the first time and now am a full convert",14z9id2,Lockonon3,1689319474.0,,False,True,Discussion,14,False,27,"I love cbt. I finally get the fuss now.

Back in the olden days, I'd just dump all of my pandas/polars tasks in one compact jupyter notebook, download it all as a .py file, and call it a day. I genuinely thought that was better and more compact.

But now after using dbt for the first time, I can sort of see its usefulness. I rewrote the pandas stuff as a chain of CTE statements and breaking them up into different, consecutive SQL files. It really forces you to view things more modular, and while SQL can't do as much stuff as python, I gotta agree that it's overall cleaner to read, even as someone who was taught Python before SQL and thus feels biased for Python.

The only thing I don't know how to exactly do yet is interrupt the dbt midway to do something only python can do, or rather something SQL can't do (like adding columns that are the output of a machine learning method such as k-means, tf-idf, or PCA), and then resuming the dbt.",False,0.94,https://www.reddit.com/r/dataengineering/comments/14z9id2/after_balking_for_a_long_time_tried_out_dbt_for/,dataengineering
"Am I reading it correctly, self paced DE course costs $998.",14zutl9,rrahul42177,1689375241.0,,False,False,Discussion,0,False,1,,False,1.0,https://i.redd.it/r65gidrxf0cb1.jpg,dataengineering
Data Engineering Headaches,14z3dvp,jayking51,1689300344.0,,False,True,Discussion,75,False,44,"As a Data Engineer what’s your biggest headache, frustration, time suck?",False,0.98,https://www.reddit.com/r/dataengineering/comments/14z3dvp/data_engineering_headaches/,dataengineering
"If you saw this and actually looked through it, what would you think",14z582e,big_lazerz,1689305728.0,,False,True,Personal Project Showcase,26,False,27,"Facing a potential layoff soon, so have started applying to some data engineer, jr data engineer and analytics engineer positions. I thought I'd put a project up on github so any HM could see a bit of my skills. If you saw this and actually looked through it, what would you think?

[https://github.com/jrey999/mlb](https://github.com/jrey999/mlb)",False,0.97,https://www.reddit.com/r/dataengineering/comments/14z582e/if_you_saw_this_and_actually_looked_through_it/,dataengineering
How do you build an effective development experience?,14zrhee,drrednirgskizif,1689367241.0,,False,True,Discussion,0,False,1,"There are a lot of options now in terms of data pipelines, tools, and paradigms. Databricks, airflow, trino, dbt, snowflake, medallion , lake house, mesh, fabric…. 

I’m interested in a discussion around how people have set up their environment to do effective development. I am especially interested in ways to enable local development that can then be promoted to a prod data pipeline. 

For instance, let’s say I have airflow executing tasks on a simple job or spark cluster cluster. These jobs live in a VPC that is not accessible to the internet. I want to develop my jobs in some test environment, preferably locally, then push these changes to a CI/CD pipeline to hook them in to prod. How do you go about solving this challenge? Do you replicate any source or target destinations locally? 

Another example is if I have databricks running my pipeline, I can set up a duplicate test environment or database to development or test my code. Although it is easy to have a dev/test pipeline then promote these jobs with a global variable, this can get quite costly if one has a duplicate everything, not to mention devising unit tests in databricks can be a hassle. 

Another hassle I have run across is using lambda functions as part of a data processing pipeline. If one uses SNS to trigger the lambda function, do you use localstack to recreate the AWS infra?

Sorry for sort of rambling off for a while, but TLDR;

How do you set up your environment to develop data pipelines locally and what challenges have you overcome to accomplish this?",False,0.67,https://www.reddit.com/r/dataengineering/comments/14zrhee/how_do_you_build_an_effective_development/,dataengineering
TCA Data Modelling!,14zqyw6,11noskcire,1689366021.0,,1689366259.0,True,Discussion,0,False,1,"any thoughts about this data modelling approach in this article? this gotta the best one at structuring your data model in data lakehouse IMHO!

&#x200B;

[TCA Data Modelling](https://medium.com/pier-stories/how-to-organize-your-data-lake-warehouse-miniseries-553348df043d)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zqyw6/tca_data_modelling/,dataengineering
Starting a new venture. What steps would you all suggest I take to make sure my data is “clean”? What would the first data you would look to collect?,14zpqdw,5olArchitect,1689363082.0,,False,True,Discussion,3,False,1,"I just set up my web app with error/performance analytics for engineering and it got me thinking about what marketing/user data I should be collecting. I was listening to an episode of software daily (Making Data Driven Decisions with Soumyadeb Mitra) and one thing the guest (Soumyadeb) mentioned was that many companies he’d worked with didn’t have clean data, so they couldn’t do much with it.

I am a software engineer, so I do understand some aspects of clean data, for instance, typing and null extraction. 

But, is there anything else I should consider?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zpqdw/starting_a_new_venture_what_steps_would_you_all/,dataengineering
Postgres as a protocol,14zh9r4,Euphoric-Let-8960,1689343304.0,,False,True,Blog,1,False,2,"Interesting argument ...

Max Kremer makes the case for Postgres as a protocol. What say you?

[https://lassoo.io/blog/2023/07/12/postgres-as-a-protocol/](https://lassoo.io/blog/2023/07/12/postgres-as-a-protocol/)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zh9r4/postgres_as_a_protocol/,dataengineering
Data Engineering's introduction to relational databases assignment,14zcesd,Simple_Debt5396,1689329374.0,,False,True,Career,1,False,3,"I am a student who is enrolled in the IBM Data Engineering course of the coursera, and my final assignment is still not peer-reviewed, kindly if anyone of you who is enrolled in this course may review my assignment, then it'll be very helpful. Here is the assignment's link: [https://www.coursera.org/learn/introduction-to-relational-databases/peer/ywa1e/project-submission-peer-review/review/GuY1bSIbEe6ZGBI3pbP8tw](https://www.coursera.org/learn/introduction-to-relational-databases/peer/ywa1e/project-submission-peer-review/review/GuY1bSIbEe6ZGBI3pbP8tw) ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zcesd/data_engineerings_introduction_to_relational/,dataengineering
Are you more likely to face pipeline design or traditional system design questions in D.E. interviews?,14zjwlq,James76589,1689349569.0,,False,True,Interview,5,False,1,"Or both?! I am a software engineer looking to transition to data engineer, prepping for interviews. Been checking out the usual Youtube prep videos and I'm left wondering how prevalent system design is in the D.E. interview process.

If it makes a difference I'll probably be targeting low to mid level positions.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zjwlq/are_you_more_likely_to_face_pipeline_design_or/,dataengineering
Do you find Coursera or other free online courses helpful?,14zej5d,InevitableTraining69,1689336009.0,,False,True,Discussion,5,False,2,"Seems like there are a lot of online courses for data, AI, databases, etc available online through websites like corsaira and you can pay extra for a certificate which I probably wouldn't do. But I wanted to know if you find these helpful?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zej5d/do_you_find_coursera_or_other_free_online_courses/,dataengineering
Thoughts on FlatFile?,14zj7xr,o_Chamber,1689347972.0,,False,True,Discussion,3,False,1,"We're working on an ETL pipeline that basically requires piping customer data from an SFTP site to our system on a recurring basis. *Presumably* the data will follow an agreed spec based on an initial sample, but that could change, or out-of-sample data could break some assumptions.

Was looking at FlatFile who seems to lead in this kind of thing, but was curious what others think about the quality or limitations of it?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zj7xr/thoughts_on_flatfile/,dataengineering
How to build a CDC Pipeline? Meetup,14zhvnq,Glittering_Bug105,1689344786.0,,False,True,Blog,1,False,1,"Join us on July 20th for a free webinar on building change data capture (CDC) pipelines using [Memphis.dev](https://Memphis.dev) and Debezium. Discover how CDC empowers data replication, real-time analytics, and machine learning operations.

[https://www.meetup.com/developers-and-data-powered-by-memphis-dev-platform/events/294556053/](https://www.meetup.com/developers-and-data-powered-by-memphis-dev-platform/events/294556053/)",False,0.67,https://www.reddit.com/r/dataengineering/comments/14zhvnq/how_to_build_a_cdc_pipeline_meetup/,dataengineering
"Python library for automating data normalisation, schema creation and loading to db",14yfh6p,Thinker_Assignment,1689238647.0,,1689238840.0,True,Open Source,118,False,217,"hey folks,

For the past 2 years I've been working on a library to automate the most tedious part of my own work - data loading, normalisation, typing, schema creation, retries, ddl generation, self deployment, schema evolution... basically, as you build better and better pipelines you will want more and more.

The value proposition is to automate the tedious work you do, so you can focus on better things.

So dlt is a library where in the easiest form, you shoot response.json() json at a function and it auto manages the typing normalisation and loading.

In its most complex form, you can do almost anything you can want, from memory management, multithreading, extraction DAGs, etc.

The library is in use with early adopters, and we are now working on expanding our feature set to accommodate the larger community.

Feedback is very welcome and so are requests for features or destinations.

The library is open source and will forever be open source. We will not gate any features for the sake of monetisation - instead we will take a more kafka/confluent approach where the eventual paid offering would be supportive not competing.[https://dlthub.com/](https://dlthub.com/)  


I know lots of you are jaded and fed up with toy technologies - this is not a toy tech, it's purpose made for productivity and sanity.

&#x200B;",False,0.99,https://www.reddit.com/r/dataengineering/comments/14yfh6p/python_library_for_automating_data_normalisation/,dataengineering
What are the techniques to ensure atomic writes to partition?,14zan40,ad81923,1689323403.0,,False,True,Help,5,False,2,"I have a couple of spark jobs writing data in hourly partitions using Avro and Parquet formats. The data is stored in AWS S3 in the same bucket, but each partition has a different prefix, e.g. \`YYYY-MM-DD-HH\`. How can I ensure that the downstream jobs won't see or be able to access an incomplete partition being written?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14zan40/what_are_the_techniques_to_ensure_atomic_writes/,dataengineering
Opensearch logs,14zekds,pessimistic_dilution,1689336093.0,,False,True,Discussion,0,False,1,I am trying to introduce data science in my organisation. I have access to a lot of logs stored in opensearch. I need to present some solution to get funding. Can you guys give some tips on some solutions you have implemented using logs,False,1.0,https://www.reddit.com/r/dataengineering/comments/14zekds/opensearch_logs/,dataengineering
SQL Testing: a loosing battle?,14ywjwa,jaynerd,1689282451.0,,False,True,Discussion,11,False,14,"I have worked as a software engineer and I am now on my second data engineering position. As time marches on it seems that I am fighting a losing battle trying to implement automated logic tests for SQL testing.

As a software engineer I wrote tests to validate that for input x I would get output y and then make sure all the edge cases were covered. I have worked on data warehouse projects that used a similar ""unit test approach"": load a test data set into the database and verify the result of some sql. When I moved into doing work in a data science department we used Spark to mock out data frames and test the output of an operation.

However this does not seem to be how the industry as a whole handles things. It seems that teams test the general shape of some transform logic such as row counts or if a value falls within a range. I am curious what other folks impressions are (not just your team, but data engineering as a whole)?

On a side note: I read articles like [https://billcrichmond.medium.com/machine-learning-vs-traditional-software-development-96923dc5ffbc](https://billcrichmond.medium.com/machine-learning-vs-traditional-software-development-96923dc5ffbc) which makes me think people are testing transforms, but then I see polls like this [https://www.jetbrains.com/lp/devecosystem-2021/databases/](https://www.jetbrains.com/lp/devecosystem-2021/databases/) that indicate people don't really test.

&#x200B;

&#x200B;",False,0.89,https://www.reddit.com/r/dataengineering/comments/14ywjwa/sql_testing_a_loosing_battle/,dataengineering
AWS Alerting and Notification Best Practices,14z2ieu,priyasweety1,1689297884.0,,False,True,Discussion,4,False,5," Hello Everyone! 

What are the AWS alerting and notification best practices. While we've covered the basics in the current org where we get a notification on every failure, plain email saying the job is failed. We would like to have a thought process to see what is best in 

&#x200B;

 How have you expanded your AWS alerts beyond job failures?    
 Have you utilized Amazon SES to deliver timely notifications? how you have optimized the failed emails  
 How have you leveraged AWS Glue and Lambda to automate your alerting process? 

 For those who have integrated PagerDuty within their AWS infrastructure  

&#x200B;

what is the best enterprise-level strategy for alerting and notifications?",False,0.86,https://www.reddit.com/r/dataengineering/comments/14z2ieu/aws_alerting_and_notification_best_practices/,dataengineering
Small file problem on spark in GCP,14z7y9h,bha159,1689314250.0,,False,True,Discussion,11,False,2,"Hi all  
I am seeking advice and suggestions regarding an ELT workflow that my team has been implementing using BigQuery and Spark. I have encountered some challenges and would greatly appreciate the insights of this community.

Currently, I am using BigQuery for our ELT work, but the cost has been significant due to the rapid increase in data volume. To explore cost reduction options, my team decided to explore Dataproc. Our approach involves exporting BigQuery data to Google Cloud Storage (which is free up to 10TB per day), running a Spark job using the same query as in BigQuery (making it Spark compatible), writing the output DataFrame to GCS, and finally ingesting the data into a table (which is also free).

However on doing this, I have observed two peculiar issues that I am struggling to resolve. Firstly, the BigQuery export process generates files of varying sizes, resulting in a large number of small files. This seems to significantly slow down the Spark job. Secondly, during a test job where the total size of input tables was around 300GB, I noticed that the YARN pending memory started at 2TB as shown on dashboard and kept decreasing as the job progressed. I am perplexed as to why the YARN pending memory would be 2TB for a dataset of just a few hundred gigabytes.

Given these challenges, I am seeking advice from the community. If any of you have attempted a similar workflow or encountered similar issues, I would like to hear about your experiences and the results you achieved. Did you optimize the BigQuery export process to avoid the problem of numerous small files? Do you believe the method of running spark jobs via above mentioned way is appropriate, or would you recommend any changes to improve the Spark job performance? Are there any specific optimizations or best practices you would recommend for Spark jobs?

Any advice or suggestions you can provide would be highly appreciated. Thanks a lot in advance.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14z7y9h/small_file_problem_on_spark_in_gcp/,dataengineering
How do you collaborate with notebook savvy data scientists on streaming data pipelines?,14ypog3,semicausal,1689266335.0,,False,True,Discussion,30,False,13,"The streaming world is dominated by Java based tools like Kafka and Flink but most data scientists live in MATLAB, R, and Python notebooks. I'm curious how y'all collaborate with data scientists in these situations, especially when you want to embed some domain specific data logic / transformation?

\- Do they write data transformations / build models in their native language and throw it over the wall to you?

\- Do they learn your toolchain?",False,0.94,https://www.reddit.com/r/dataengineering/comments/14ypog3/how_do_you_collaborate_with_notebook_savvy_data/,dataengineering
List of / Suggestion for Code based ETL Tools.,14yl2vw,Pillstyr,1689255334.0,,1689314853.0,True,Discussion,17,False,18,"As opposed to other people who asked about no-code/low-code/ GUI-based tools, I want a list of tools which actually require to write code and schedules for data movement.I am very new to all this. I'm currently working as Power BI developer in a company but want to do Data Engineering and ETL stuff in near future.

I read that Informatica and Talend are all GUI-based tools which is not good for long run growth and learning.

EDIT:

Thank You everyone for such great input. This just made my day. I think I have alot to learn ahead. After all your comments, I feel like I haven't even scratched the surface.

Please if anyone of you can share a roadmap. I really need to get in ETL and DE, and soon.

Note: I have 1 year or 1.5 years max to switch and I need it to be ETL or DE job where I could do all this what you guys mentioned in comments.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yl2vw/list_of_suggestion_for_code_based_etl_tools/,dataengineering
How to learn apache spark as a beginner/junior/new grad,14ysuu6,poepstinktvies,1689273837.0,,False,True,Help,3,False,4,"Im looking to get a general idea of the spark ecosystem and fundamentals because at our company we are using a custom made abstraction above ""Apache Spark"". A lot of concepts are mapped 1:1.  Mainly for ETL purposes (data processing etc)

Any idea's suggestions? by reading books, tutorials etc",False,0.75,https://www.reddit.com/r/dataengineering/comments/14ysuu6/how_to_learn_apache_spark_as_a_beginnerjuniornew/,dataengineering
Useful or nah: open-source serverless analytics DB,14yv3kv,robertao211,1689279112.0,,False,True,Open Source,5,False,2,"Hello! At my company, Journalize, our users wanted to add arbitrary tags to their data so they could slice and dice against any dimension. To aggregate data over 10M rows, we set up a Clickhouse server with some magic. I'm curious if people can also use what we built. 

(note: im expecting the data purists to hate this, and I am ready for that… :)

We are calling it ScratchDB: Firestore for analytics. Stream JSON, and we’ll figure out how to get it into an analytical database, and you can run SQL. 

Website: [https://www.scratchdb.com](https://www.scratchdb.com/)

Github: [https://github.com/scratchdata/ScratchDB](https://github.com/scratchdata/ScratchDB) 

The problem as I see it: 

* At some point, you might find that GROUP BYs are slow in Postgres and want an analytical database
* Getting started has technical friction: choosing a DB, new underlying storage models, monitoring servers
* Pricing is opaque: self-hosting vs cloud, credits, vCPUs, it’s hard to answer “how much will this thing cost”, and often the answer is “a lot”

With ScratchDB:

* No need to issue a CREATE TABLE statement. Send your JSON and we will automatically create tables and columns on the fly.
* Just pay for what you use via metrics you can easily understand (storage, processing time)
* ScratchDB is open source (github) and right now it runs on top of DuckDB. (More connectors in the works!) It is written in Go.

I am really curious about what people think. Also, we’d love to know how you’re doing this today, how much it costs, and what your devops looks like. (if you’re willing to shoot me a DM!) 

Let us see if we can help you ingest data into your DB more easily. If this is a pain point on non-analytics workloads, I’m interested in talking as this is also on our roadmap.

Thanks in advance! ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yv3kv/useful_or_nah_opensource_serverless_analytics_db/,dataengineering
Data engineering certification,14ymvb3,prototypefish72,1689259713.0,,False,True,Help,2,False,5,"Hey yall, so I had a couple questions,

Today I decided to try and work on my courera app. I currently work at Tesla and, honestly, im not happy with it. It's good and all but there's no challenge. I'm working on going back to community college to get a transferable mechanical engineering degree, but I'm exploring my options.
I came upon the Data engineering certification and I was wondering, if it's JUST a certification, can I do much with that? I've always had a soft spot for programming and computer science, ngl, I've had it rough and never had the ability to completely apply myself in college. Now, I think I actually can and I feel like this would be useful to have in my tool belt, but, I ask again, is there actual application to an IBM data engineer certification?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ymvb3/data_engineering_certification/,dataengineering
"Is airflow better for triggering jobs in a data pipeline, or actually running the jobs itself?",14y3s4r,chamomile-crumbs,1689203208.0,,False,True,Help,30,False,71,"At work we’re building a lil’ pipeline. Nothing fancy, just reads data from a few API’s, normalizes them and sticks em in a table in our DB. 

We’re trying out airflow for this, and we’ve been putting all of the actual code into DAGs in airflow. So, all python.

I saw another post that mentioned how airflow is mostly a “job scheduler”, which made me second-guess keeping all of our code in airflow DAGs. So I’m wondering: do y’all use airflow primarily as a scheduler for jobs that are owned by other services, or do you also rely on it to run business logic?

If that’s too vague, here’s a specific example:
Ideally, I’d have all of my data pulling/normalizing code in rust. We already have a nicely setup rust environment, and that’s how I would handle our pipeline if it was just gonna be rust scripts and a bunch of cron jobs. 
But since airflow has so many easy integrations, we decided just to let airflow (and thusly python DAGs) handle all of the data pulling and normalization. 

Is the “correct” way to use airflow:
1) having airflow trigger rust scripts?
2) having airflow handle everything from within airflow?",False,0.97,https://www.reddit.com/r/dataengineering/comments/14y3s4r/is_airflow_better_for_triggering_jobs_in_a_data/,dataengineering
Storing realtime events in object storage,14yxlz3,romanzdk,1689285022.0,,False,True,Discussion,4,False,1,Is it good practice to store event data in object storage one by one? I.e. if I get 1M events/messages a day I store 1M small e.g. JSON files into the S3 and then batch process it at the end of day. Is this reasonable or I should somehow buffer all the messages and then save only one (or little more) batched files into the S3? Also why / why not?,False,1.0,https://www.reddit.com/r/dataengineering/comments/14yxlz3/storing_realtime_events_in_object_storage/,dataengineering
Anonymizing 7 terabytes of data in a hybrid cloud environment,14yw2dw,Synthesize2023,1689281361.0,,False,True,Blog,0,False,1,[https://gretel.ai/blog/bring-your-own-cloud](https://gretel.ai/blog/bring-your-own-cloud),False,0.67,https://www.reddit.com/r/dataengineering/comments/14yw2dw/anonymizing_7_terabytes_of_data_in_a_hybrid_cloud/,dataengineering
Can this be called an ETL ?,14ysqvg,deathlolwut,1689273573.0,,False,True,Help,1,False,1,"I have built a custom Prometheus metrics exporter in Python for a Database system. 
The exporter's job is to get data from the DB by running SQL queries (E), Convert the tabular data to key value pairs (T) and then send it to Prometheus (L).
Does this classify as an ETL job ?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ysqvg/can_this_be_called_an_etl/,dataengineering
Enterprise Data World (EDW) Conference,14ys7lx,tolkienwhiteboy,1689272290.0,,False,True,Discussion,0,False,1,"Has anyone attended? How does it compare to other Data conferences?

edw2023fall.dataversity.net

I saw this post from earlier this year but didn't catch any mention of this one.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ys7lx/enterprise_data_world_edw_conference/,dataengineering
Dagster with multiple repos,14yryax,IAteQuarters,1689271695.0,,False,True,Help,4,False,1,"I work at an org where our data engineers have bought into dagster. Dagster seems great, except I can’t wrap my head around how to set it up for multiple github repositories.

So we have one repo that has all the infrastructure code for dagster. Currently our data engineers keep the infra code (terraform) with their job (so its a monorepo). As dagster expands to other teams we have many projects across different repos. So for example some of my models (i am a data scientist) exist in different repos and would continue to exist in different repos. What is the simplest way to allow for me to continue this framework?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yryax/dagster_with_multiple_repos/,dataengineering
Identity resolution in Microsoft Fabric,14yr9y1,data_quantum,1689270130.0,,1689272274.0,True,Help,3,False,1,"Rengineers,

Looking for a steer within the world of Microsoft Fabric, which is quite new to me.

We have about 4 core off the shelf operational systems. 3 have APIs, 1 has csv exports. Not big data.

We are setting up Data Flows within Microsoft Fabric, where each Data Flow is basically an api endpoint (or csv export) for each system, and thus makes the returned data available for querying and analysis (in Power BI).

**Problem: we have no identity resolution solution, i.e., our unique identifiers in system 1 are different to those in system 2 (and so on), making it difficult to link data across systems.**

The outcome I'm looking for is a new table that links customers together from all data sources and generates a linkage key (likely using probabilistic matching). This table and linkage key can then be used in any new local data models (in Power BI for example).

How would you approach this in the world of Fabric? Which experience would you use (likely one of Data Factory, Data Engineering, Data Science, Data Warehouse?). I've narrowed down the likely options here:

Power BI

1. Data Flow
2. Data Set
3. Data Mart

Data Engineering

1. Lakehouse
2. Spark Job

Data Factory

1. Data pipeline

Data science

1. Model
2. Experiment

Data Warehouse

1. Warehouse

Any pointers welcome :-)

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yr9y1/identity_resolution_in_microsoft_fabric/,dataengineering
Looking for study buddy + accountability partner for data engineering as a complete beginner,14yr5ed,ComputerFine971,1689269834.0,,False,True,Career,2,False,1,"I am looking for a partner to study data engineering with, the thing is it is not that I lack motivation, but it is difficult to focus and structure the content when you are learning by yourself, having another person helps to navigate all the ruts in the way. Now disclaimer, I am a complete beginner, no work experience, therefore I look forward to anyone who also wants to learn data engineering from scratch, I know this sub has more experienced people than beginners, so it is alright if you are experienced, I will try my best to keep up with you.

here are few non negotiables, you must be very disciplined and driven and I promise to be same, preferably enrolled in computer science or similar degree or program and are not completely new to computer science as a subject, this should not be experimental, hobby thing for you and you are willing to put in work to progress.

We can decide on what resources to learn and make goalpost for projects, applying to internship or job as we progress, weekly discussions or brainstorming session, most importantly constantly sharing what and how we learnt something new or overcame a complex problem, point being having consistent communication, honesty, enthusiasm with learning goal.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yr5ed/looking_for_study_buddy_accountability_partner/,dataengineering
Trying to make sense of shuffle read vs shuffle writes of a spark stage,14yklmx,ThenBanana,1689254131.0,,False,True,Help,1,False,2,"Hi,

The spark shuffle read on this job is enormous compared to the shuffle write. What does this mean? the partitions are skewed? that I have too much or too little partitions?

&#x200B;

https://preview.redd.it/82ehfwhsfqbb1.png?width=1632&format=png&auto=webp&s=d548f2b5cddd5bde5126dbc3bb5cac168bf47d8e",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yklmx/trying_to_make_sense_of_shuffle_read_vs_shuffle/,dataengineering
What are Quotas in Apache Kafka®? A short explanation and why you need them.,14yo6kb,Marksfik,1689262835.0,,False,False,Blog,0,False,0,,False,0.5,https://aiven.io/blog/introducing-kafka-quotas-in-aiven-for-apache-kafka#a-short-explanation-of-kafka-quotas?utm_source=reddit&utm_medium=organic&utm_campaign=blog_markos_kafka_quotas__global_,dataengineering
Burnout,14yaltn,Used_Ad_2628,1689222451.0,,1689223473.0,True,Discussion,4,False,6,Does anyone else feel like they burn out people a lot faster at the Bay Area companies? Maybe it is the culture?,False,1.0,https://www.reddit.com/r/dataengineering/comments/14yaltn/burnout/,dataengineering
Need help mapping data in Python,14y4yf7,iambatmanman,1689206215.0,,False,True,Help,16,False,13,"I’m frustrated at my attempts, as they seem to be going in circles, but I’m just looking for better alternatives for what I’m doing. 

I work in data migration, and at first it was fine, but now it’s not scaling very well. I need to map non-normalized values to those my company has a set of standards for and so far my attempts just create tons of manual work. 

Currently, I’ve been building “maps” of key/value pairs and coercing the values manually by removing formatting and things so I’m only comparing lower case letters (and numbers if needed). By now some of maps are 100’s off values in length or longer. A good example is a person’s race, these values are user generated and we only allow a handful of things before we treat it differently. But there are a lot of different ways to denote a lot of different races… that’s just one example obviously. 

I’ve tried the `fuzzywuzzy` package which is partially reliable (only on Levenshtein distance). We don’t have the budget for open ai either. 

Can anyone give me some ideas?

Thank you in advance!",False,0.94,https://www.reddit.com/r/dataengineering/comments/14y4yf7/need_help_mapping_data_in_python/,dataengineering
Salary negotiation advice for internship,14yksa1,champagnepapi069,1689254598.0,,False,True,Help,0,False,1,"Hey everyone,

I'm currently facing a salary negotiation dilemma for an upcoming data engineering internship, and I could really use your advice and suggestions. Here's the situation:



I'm a graduate student with 2 years of experience in data engineering. Recently, I received an internship offer from a company, but they are offering me a rate of $25 per hour, which I believe is significantly lower than what I expected considering my education and experience.



During the HR round, I was asked about my salary expectations, and I mentioned that I was flexible. I made this decision because I saw on [levels.fyi](https://levels.fyi) that software developers were being paid around $40 per hour. However, I now realize that I made a mistake by not clearly stating my expected salary during the discussion. I was worried about potentially losing the offer, especially after facing numerous rejections from nearly 400 applications.



Now, I'm contemplating having a conversation with the HR department regarding the salary. I want to make sure I'm fairly compensated for my skills and experience. I would greatly appreciate any advice or suggestions on how to approach this meeting and what key points I should bring up.


Thank you all in advance for your support and insights. Your experiences and guidance would be invaluable in helping me navigate this challenging situation.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14yksa1/salary_negotiation_advice_for_internship/,dataengineering
What is the best way to learn/prepare spark coding interview?,14ybp45,Itchy-Jello4053,1689225852.0,,False,True,Interview,3,False,3,Are there good learning resources or online courses? I assume pyspark is more popular than scala spark.,False,0.81,https://www.reddit.com/r/dataengineering/comments/14ybp45/what_is_the_best_way_to_learnprepare_spark_coding/,dataengineering
Data engineering meetups,14yejyj,btenami,1689235429.0,,False,True,Discussion,3,False,2,"I am based in France and I'm looking to hear news and presentations from other data practitioners (data engineering mostly). I can't find any meet-ups. Do you guys know some ? Preferably in person events, but since I'm not finding anything, online will do too. Thanks !! ",False,0.75,https://www.reddit.com/r/dataengineering/comments/14yejyj/data_engineering_meetups/,dataengineering
Recap: A python library for describing database tables and serialization formats with minimal type coercion.,14y8op7,cpardl,1689216832.0,,False,True,Open Source,2,False,4,"Hey everyone,

I wanted to share here a project I'm really excited about. 

Recap is mainly the work of [Chris](https://twitter.com/criccomini) but I had the privilege to be involved in some design and implementation parts. Enough with the intro stuff though, let's talk about the project.

Recap is a Python library that provides a single schema for... 

*  IDLs (Proto, JSON schema, Avro)
*  Databases (Snowflake, PG) 
* Schema registries (CFLT schema registry, Hive metastore) 

 Read and convert all these schemas in one format.

Recap is still a baby as Chris says but I feel there's enough functionality at this point to reach out to the community and get some feedback. 

The goal is to be able to access, reason and transform between all the different formats and metadata stores that are typically found in a decently mature data infrastructure. 

There's still work to be done but most of the components needed are there. 

Take a look and I'd love to hear your thoughts and feedback.

The main Recap page: [https://recap.build](https://recap.build)

The type system spec: [https://recap.build/spec/0.1.1](https://recap.build/spec/0.1.1)

The Github Repo: [https://github.com/recap-build/recap](https://github.com/recap-build/recap)

Thanks!

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14y8op7/recap_a_python_library_for_describing_database/,dataengineering
How to further career?,14xwkrp,WhelminglyMediocre,1689186554.0,,False,True,Career,14,False,17,"I currently work in a healthcare company. We use SSIS and SSMS (SQL) for most of our data needs. On the rare occasion, we also use C#, Python, and Powershell. Is this stack good enough to start job hunting or would it be better to pick up some software-specific skills? I thought about learning GCP or AWS but am not sure the likelihood of getting hired with only self-study on those topics. I've been at my current company for 8 years (intern -> data analyst -> database engineer) and it's a good environment / fully remote, but the money just isn't there. I'm currently making $75k/80k a year depending on bonus and yearly raises are 2%. ",False,0.92,https://www.reddit.com/r/dataengineering/comments/14xwkrp/how_to_further_career/,dataengineering
Is it normal to feel completely lost during initial months of your data engineering job ?,14xhi13,jojobaoil68,1689147891.0,,False,True,Discussion,32,False,132,"I got into a data engineering role, it's my first job as a DE. And i am feeling absolutely lost, i don't understand what's happening, everything is everywhere, my team mates are very busy so no one properly explains what's happening and some structural change is happening in the whole section of DE teams. And I feel absolutely overwhelmed.
How do you tackle this?",False,0.97,https://www.reddit.com/r/dataengineering/comments/14xhi13/is_it_normal_to_feel_completely_lost_during/,dataengineering
Self-serve tool,14ycbuu,gal_12345,1689227896.0,,False,True,Discussion,8,False,2,"Hi all,
Analysts in my team is working hard to serve other teams instead of helping me. We want to build/ buy some self-serve tool to reduce outside requests. 
We are looking for some tool similar to jinja template, that the outsiders will be able to select their fields, dimensions and filters and this will render the template with the relevant fields. 
We try looker, but it was very difficult to so PoP and it is our most common cases.
Can you help us and suggest self-serve tools?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ycbuu/selfserve_tool/,dataengineering
GPT-4 + Streaming Data = Real-Time Generative AI,14yanye,yingjunwu,1689222640.0,,False,False,Blog,0,False,2,,False,0.75,https://www.confluent.io/blog/chatgpt-and-streaming-data-for-real-time-generative-ai/,dataengineering
internal transfer to data engineer role at amazon,14xzu9d,no_catchy_username,1689194051.0,,False,True,Interview,2,False,5,"Those that work at amazon as data engineer, can you please tell me how your interview process was, how you prepped, what type of questions you were asked?",False,0.74,https://www.reddit.com/r/dataengineering/comments/14xzu9d/internal_transfer_to_data_engineer_role_at_amazon/,dataengineering
"""St. Albnas"" Hackathon",14xvfx1,itty-bitty-birdy-tb,1689183944.0,,False,True,Blog,0,False,10,"We've all no doubt seen the [""St. Albnas""](https://www.reddit.com/r/dataengineering/comments/14442pi/we_have_great_datasets/) meme by now. Data quality is hard 🤦🏻‍♂️.

To have some fun with this, the company I work for (Tinybird) is sponsoring a little ""hackathon"". Clean the data and avoid false positives to get some swag. No winners and losers, just participation prizes! (yay millennials!)

Make sure to read the rules, and have some fun with it!

[https://github.com/tinybirdco/st-albnas-hackathon](https://github.com/tinybirdco/st-albnas-hackathon)",False,0.92,https://www.reddit.com/r/dataengineering/comments/14xvfx1/st_albnas_hackathon/,dataengineering
pure orchestration,14yc2wd,nlimpid,1689227076.0,,False,True,Help,3,False,0,"Hello everyone. I am a newbie to data engineering and not very familiar with this.

&#x200B;

Our group has a requirement where we have a large number of data timed tasks, but they have some dependencies, most of them are written in go code and run in lambda.

&#x200B;

So I started looking for task scheduling solutions. I found a lot of scheduling tools/systems written in Python. And they all combined runtime code  and scheduling configuration together, like  dagster and prefect, but I probably wanted a language independent pure scheduling system.

&#x200B;

At the same time, I also observed a lot of projects using config file to define pipeline, why not use relational database to store pipeline directly, if a lot of config file exist, I think it's hard to manage.

&#x200B;

The closest to what I need is benthos, but I find it a bit unmanageable to define using yaml, I would like to write something like  benthos to trigger task and use relational database to define pipeline.

&#x200B;

Is there any suggestion to me? thank you!",False,0.5,https://www.reddit.com/r/dataengineering/comments/14yc2wd/pure_orchestration/,dataengineering
Dagster - Separating compute and orchestration,14xodd2,Other_Cartoonist7071,1689167886.0,,False,True,Discussion,12,False,20,"Our team is looking at writing a simple framework/platform based on Cloud that'd allow various users to define batch based pipelines. Currently focusing on AWS, there are multiple modules which setup a set of services, dynamic infrastructure, orchestration, roles, notifications for the users as part of the platform. 

As part of orchestration, I started looking at Dagster and one of the thing I immediately realized is that the actual data processing and orchestration is intertwined and deliberately kept together in most of the examples I saw and also a forte of Dagster as I understand compared to the traditional orhestration/workflow frameworks where they are loosely coupled and a DAG is defined.   


When I look at any orchestrator, the main things I look for is that - orchestration, meaning given a set of tasks, it should be able to execute, report on, track, kill at will, rollback, fanout, do reporting and maintain metadata for it. Combining this with the actual data being processed as part of the actions/tasks is a great thing for small scale pipelines but quickly becomes a scaling/deployment problem if I want to make it available as a service (not looking for Dagster cloud yet) where users can submit many such jobs.   


I understand and yet to explore the constructs such as Assets, Operators that allow me integrate with external services (in my case the cloud services which I'd integrate with a good wrapper over common AWS managed services and compute) but before I proceed wanted to check if I am not setting myself on a wrong path just because its supports but is indeed an antipattern.   


Why separate actual compute from orchestration ?   
\- I want to fully exploit and use existing AWS services where given a script (say the script/python code utilizing say polars,pandas, duckdb etc), I have a freedom to pack and run it as a docker image on ECS, Fargate, or lamda   
\- For a very big workloads, which don't fit into services such as AWS Glue/Redshift for ELT/ETL purpose, be able to spawn an appliance say through spot and get it executed in EC2 and so on..  


.. and still use Dagster as a pure orchestrator where it can have a shim layer written that not only launches but tracks these jobs very well, helps with retries, rollbacks etc.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xodd2/dagster_separating_compute_and_orchestration/,dataengineering
Joined a new company as a DE for a big project and I'm feeling a bit underwhelmed - Would like to hear a second opinion.,14xkfjd,King_TN,1689157100.0,,False,True,Career,30,False,32,"Hi everyone - for context I'm a DE with \~3 YoE that I spent working in a small startup where I was, *for the most part*, the sole DE.  
I was responsible for not only implementing an end-to-end data pipeline that handles data from ingestion to analytics dashboards but also built the websockets and APIs needed to interact with different parts of the pipeline - which I really liked.

Around a month ago I got the opportunity to join a big company as a DE, both the job listing and the interview had big emphasis on experience with end-to-end data pipelines and good SWE practices.

I figured this would be a great opportunity to work with more experienced engineers on a project with significantly more data than what I'm used to.

The data stack is: Airflow - dbt (core) - AWS S3 (data lake) - AWS Redshift (dw) - Quicksight.

Most of the tasks I pick up involve building dbt models, solving merge conflicts, configuring Airflow DAGs or sometimes assisting the analytics team.

The good thing is that the company has a great data culture and big emphasis on CI/CD and good SWE practices - they also insist on having DEs handle the building of dbt models and query optimization which is a good thing imo.

My major concern however is that the role doesn't feel as technically challenging as I was hoping for it to be, so do you guys think this is a good position to progress my career long term *from a technical standpoint* or should I keep an eye on other opportunities ?",False,0.97,https://www.reddit.com/r/dataengineering/comments/14xkfjd/joined_a_new_company_as_a_de_for_a_big_project/,dataengineering
What is my starting point?,14y4xqk,MiserableCharity7222,1689206162.0,,False,True,Career,3,False,2,"I’m currently working in a role that allows me to do various data analyst related tasks with Alteryx, python, and some SQL, but if I were to make the pivot to data engineering, what role should I realistically look for that would allow me to prepare for a data engineering role? Due to being in grad school and working full time, I don’t have much time to do side projects, and I won’t have much time to myself till graduation. I would like to transition to data engineering, but I would like to get the proper preliminary experience before even attempting to apply for roles. Any advice would be greatly appreciated. Thank you 

Additional info:
Have experience using SQL, Python, and Alteryx
Currently in grad school and working full, so I do not have much time to do side projects (learn Apache and AWS)
Have some experience in using data automation pipelines via alteryx 
Have a project portfolio and GitHub",False,1.0,https://www.reddit.com/r/dataengineering/comments/14y4xqk/what_is_my_starting_point/,dataengineering
Anyone developing with Kafka? Learn how to enforce good data security practices,14y0nys,data-stash,1689195926.0,,False,True,Career,0,False,3,"Not sure how many people are developing with Kafka on here, but given ingestion of streaming data is fairly typical of most data engineer's these days, I wanted to share the below.

Webinar will focus on practical implementations for enforcing data security when it comes to developing with Kafka.

Join for free: [https://app.livestorm.co/conduktor/kafka-security-masterclass](https://app.livestorm.co/conduktor/kafka-security-masterclass?type=detailed)",False,0.8,https://www.reddit.com/r/dataengineering/comments/14y0nys/anyone_developing_with_kafka_learn_how_to_enforce/,dataengineering
"Want to transition from DS to Data Eng, anyone wants to help with mock interview?",14xtaru,hatidzhek,1689179129.0,,False,True,Interview,7,False,8,"Hello everyone,

I was DS in Google and laid off 4 months ago and I couldn't find any DS position since then (Im living in Switzerland). And I find a great start up but they hiring data engineering position. I would really want to try it since I really like the culture of the company and I did a lot of pipelining in my DS role in Google. But I don't know how Data Eng case study interviews would be. I have no experience on that side and I can't find questions online, maybe i don't know how to search. Is there anyone can help me with mock interview for entry level positions? ",False,0.9,https://www.reddit.com/r/dataengineering/comments/14xtaru/want_to_transition_from_ds_to_data_eng_anyone/,dataengineering
What am I?,14y3akl,dany65ns,1689202049.0,,False,True,Help,1,False,3,"To keep it short I have no idea what title would be most suitable.
I do dashboard design,build dashboard(js,css,html),I clean data and have to come up with new plans on how the data we have I can automate, I currently make no api calls (I do connect to databases but it’s a system where I do not need to make api calls )or build databases YET. I do see myself in the future doing this and full stack SWE devolvement. I have no idea what the appropriate title for me would be.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14y3akl/what_am_i/,dataengineering
"If there was a nature documentary about the ""datus engineerius"" and it's life inside of the corporate habitat, what kinds of things would for sure be pointed out?",14xoc20,3spelledout,1689167792.0,,False,True,Discussion,19,False,13,"I'm actually going to make this, so I want to make sure I don't leave out any great jokes. What should the office look like? What struggles should I include? What would make you laugh? ",False,0.81,https://www.reddit.com/r/dataengineering/comments/14xoc20/if_there_was_a_nature_documentary_about_the_datus/,dataengineering
"Sharing 100 Bigdata Objective Type Questions on Bigdata Streaming, Real-time Analytics, Visualization and Exploration in form of 2 Exams (50 Objective Questions each)",14y7uzp,nkptcs,1689214512.0,,False,True,Blog,0,False,0," Big Data Streaming and Real-time Analytics: [https://mytechbasket.com/play\_quiz.php?paper\_id=48](https://mytechbasket.com/play_quiz.php?paper_id=48)

Big Data Visualization and Exploration: [https://mytechbasket.com/play\_quiz.php?paper\_id=49](https://mytechbasket.com/play_quiz.php?paper_id=49)",False,0.5,https://www.reddit.com/r/dataengineering/comments/14y7uzp/sharing_100_bigdata_objective_type_questions_on/,dataengineering
help with aws cli,14xqpch,87keicam,1689173421.0,,False,True,Help,7,False,6,"hi team,

I have ubuntu box where we run our python scripts of off.

One of my python scripts uses aws cli to extract data from....

when i execute python script manually it will run.

when i add this python script to run on crontab it starts...but doesnt go past 'aws cli command' executed almost at the very begining of python script.

the python script with aws CLI:

 

subprocess.run(\[""aws"", ""s3"", ""cp"", ""link\_to\_aws\_stage"", ""download\_to\_path, ""--recursive"",""--profile"",""ABC""\])

&#x200B;

**Question:**

how to workaround that?

how can i set which ubuntu users can access CLI ?

do you have any better idea

thanks!

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xqpch/help_with_aws_cli/,dataengineering
Funtional Programming in Data Engineering,14xq53v,lou1uol,1689172144.0,,False,True,Discussion,2,False,6,"Hi all,

I have been a lurker forever in this sub and today i have decided to know your opinions in some points regarding the implementation of functional programming in Data Engineering (DE).

After reading around this paradigm (most articles i have found converge to [Maxime Beauchemin article](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a)), there are two points that makes my head scratch a bit:

&#x200B;

* **Dimensional snapshots**:  The fact that this can create a lot of redundancy in dimensional tables, this really does not sound like a problem to me. I actually like the fact that with this approach avoids upserts and SCD's by just re-processing the data in a given time period. What i would like to understand is, for those who already implemented dimension snapshots, if the redundancy on the tables took a toll on the visualization tools performance big enough for you to get back to SCDs or something hybrid.
* **Late arriving facts**: this is the point that really makes me avoid functional programming. Knowing that to we have to center our focus based on the event reception or processing time and not on the event time, it means in my opinion that the dimension partition scheme should be based on the event processing time too. However, the analysts surely do their work based on the event time which it will not benefit from the partition pruning. That could make any query based on the event time be way more costly.

Take in note that i have been working more recently with BigQuery, in which the storage capacity is not a problem and you can partition a table with one column only (aside from clustering). Even in platforms that allows tables with multiple partitions i still think the second point potential costs and performance can be heavily affected.

Take in account these two points, you think functional programming still can be something worth to implement in DE context?

I hope my doubts were clear enough for you to share your take. Best regards!

&#x200B;",False,0.88,https://www.reddit.com/r/dataengineering/comments/14xq53v/funtional_programming_in_data_engineering/,dataengineering
Strategies for Data Quality With Apache Spark,14y3v9u,nf_x,1689203420.0,,False,False,Blog,0,False,1,Very nice theoretical article about what to look at when building data quality enterprise strategy,False,1.0,https://mlopshowto.com/strategies-for-data-quality-with-apache-spark-4688632f149a,dataengineering
Should I take this job?,14xqg8d,timbaktubear,1689172853.0,,1689177749.0,True,Career,7,False,4,"Some background on myself. I recently transitioned into data engineering after 5 years as a data analyst and have been working at a startup since the beginning of the year on a contract basis. I'm practically the sole data engineer so I'm building end-to-end pipelines and I get to work with Python, SQL, DBT, Fivetran, Airflow, Postgres, MongoDB, Elasticsearch. I also do some Devops stuff - DigitalOcean is our main cloud service, but I also work with GCP and Azure (albeit at a surface level). I love the tech stack, but at the same time I'm worried I'm developing bad habits as an engineer since I'm working mostly independently and there's no proper CI/CD, at least for now. There's also no guarantee I'm gonna have a job in around 6 months since there's only a certain amount of money that's been reserved to fund my position, though there are efforts to get more funding.

I recently received a job offer at a FinTech company where the core tech stack is DBT, SQL, Snowflake, and Airflow. Certain things that concern me:

* Python seems to be a small part of the role. In my interview, they mentioned that Python would mostly be used to write up Airflow jobs. The job description also emphasizes more on SQL for deploying analytics code.
* GCP, AWS or Azure not listed on job description. The team has devops engineers so I imagine they'll be the ones handling cloud infrastructure.
* I've seen multiple job titles for the role, mainly ""Data Engineer"" and ""Analytics Engineer"". I asked them about this and they said that data and analytics engineers are one and the same to them, and they had put up the role under multiple titles to attract as many applicants as possible.

Some positives I see for the role:

* It's in a bigger proper engineering team, with a scrum master, other DevOps engineers, QA, etc. and CI/CD pipelines in place.
* I got along well with the hiring manager, who really seems to know his stuff.
* The team is rapidly expanding, which to me seems like a good sign.

I've been applying to jobs for a couple months now, and I frequently see GCP/AWS/Azure experience as a requirement. Python is easily the most enjoyable part of my job, and I want to be viewed more as programmer or a SWE-focused DE in the long term rather than an analytics engineer or a SQL developer (no hate against these types of roles, they're just not what I'm looking for).

Any advice would be greatly appreciated!

Edit: Revised bullet point on cloud services",False,0.83,https://www.reddit.com/r/dataengineering/comments/14xqg8d/should_i_take_this_job/,dataengineering
PARTITION BY whatever,14ws2ht,itty-bitty-birdy-tb,1689083631.0,,False,False,Meme,19,False,207,,False,0.96,https://i.redd.it/2h35rk2tccbb1.jpg,dataengineering
find DSA books,14xvrmx,datapopcorn,1689184704.0,,False,True,Help,4,False,1,"Is there any recommended DSA book ?

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xvrmx/find_dsa_books/,dataengineering
High availability of data without consuming too much storage space,14xlmy3,ApacheDoris,1689160617.0,,False,True,Blog,0,False,5,"This is how it is done:

Apache Doris preserves multiple replicas of hot data and the metadata in its backend nodes, and cold data in object storage with only one copy.

https://preview.redd.it/n5t2a43epibb1.png?width=1280&format=png&auto=webp&s=116202b543f7ad10392348ddb036a5e125c49081

Full post about [hot-cold data separation](https://blog.devgenius.io/hot-cold-data-separation-what-why-and-how-5f7c73e7a3cf)",False,0.86,https://www.reddit.com/r/dataengineering/comments/14xlmy3/high_availability_of_data_without_consuming_too/,dataengineering
Any websites to calculate Databricks drivers and workers required?,14xgcwi,johnyjohnyespappa,1689144002.0,,False,True,Help,6,False,7,"Any easy way to calculate the minimum number of workers, drivers and memory needed in databricks? A website that does the calculation if any?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xgcwi/any_websites_to_calculate_databricks_drivers_and/,dataengineering
What naming conventions do you use?,14xncko,arminredditer,1689165260.0,,False,True,Discussion,5,False,2,"Nomenclature is something I don't see brought very often in this subreddit. What naming conventions do you use when it comes to naming tables/jobs/individual pipelines and such, if any?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xncko/what_naming_conventions_do_you_use/,dataengineering
Redshift Vs S3,14xrl62,planetabhi,1689175412.0,,False,True,Discussion,6,False,1,What is recommended? Having your transformation done at S3 or in Redshift via stored.proecedure where S3 acts as a ODS with delta?,False,0.67,https://www.reddit.com/r/dataengineering/comments/14xrl62/redshift_vs_s3/,dataengineering
Automate GKE deployment process,14xj1mi,iGodFather302,1689152860.0,,False,True,Help,0,False,3,"Follow up post: [Previous post](https://www.reddit.com/r/dataengineering/comments/14o8h6h/data_engineer_looking_to_study_mechanical/?utm_source=share&utm_medium=web2x&context=3)

Hello, So as you know from last time, I was working on a GKE project, where I deployed a docker image that retrieves data from ADH and inserts it into BigQuery, Thank you everyone for your recommendation and your advices, I finally managed to finish the project, I scheduled, everything works perfectly!! My question is: how can I further improve the project by automating the deployment process? I was thinking of terraform and ansible, but I'm here once again to ask about your opinion, and if you have any advice!! 

Edit: I used Makefile but I think it's a bit clumsy 

TL;DR I want to automate the deployment process of pushing an image to artifact registry on google + apply workload cronjobs using ansible/terraform, Any advices? :D",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xj1mi/automate_gke_deployment_process/,dataengineering
Working with HTML files,14xqces,ltofanelli,1689172615.0,,False,True,Help,1,False,1,"Hi everyone ,

I have a lot of HTML files saved in txt on my raw container, and I have to extract only the necessary information from them.Each html file could become a JSON, because they have some levels of nested information.

What transformation would be the best in that case to make the data more structured to silver/structured layer?

Create a JSON?

Create a parquet file for each html, but duplicate data from the first nests?

Create a parquet file for each nest with a key to join them in the database?

Or any other path that u guys have in mind?  


&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xqces/working_with_html_files/,dataengineering
Retrieve the status(failure/success) of Spark application in Spark Standalone mode with client deploy mode,14xprdy,XIAOAGE,1689171250.0,,False,True,Help,0,False,1,"I'm running Spark cluster in standalone mode and doing `spark-submit` to submit Spark applications in `client` deploy mode. I would like to get the status (whether success or failure) of the Spark application.

The reason I have to use `client` deploy mode is that, we are running both Scala and Python Spark applications and `cluster` deploy mode does not support Python Spark applications by now.

The approach I can think of is to start the Spark history server and then use [its REST endpoint](https://spark.apache.org/docs/latest/monitoring.html#rest-api) to query for status from `<app-id>` and looking for whether there's failed job under the app (Because unfortunately, `spark-master:8080/json` only gives whether the app is running or completed, not fail/succeed). Next, to get the app-id in `client` deploy mode, I have to grep and match the log4j line

```
StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-xxxxxxx-xxx
```

I'm wondering is there better approach than this, I've been searching around, but the solution given is all for YARN or `cluster` deploy mode.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xprdy/retrieve_the_statusfailuresuccess_of_spark/,dataengineering
There is no Data Engineering roadmap,14ws3o3,itty-bitty-birdy-tb,1689083715.0,,False,False,Blog,42,False,85,,False,0.91,https://www.alasdairb.com/posts/there-is-no-data-engineering-roadmap/,dataengineering
Data Engineer isn’t really just data engineering,14wv6f6,Dice__R,1689090964.0,,False,True,Discussion,43,False,55,"So many people think data engineers are only responsible for building data pipelines.

But in reality, if you are doing a data lake project, you may also need to understand the cloud infra (VPC, IP, DBA infra, Terraform, K8s).

As a data engineer, I think being a cloud engineer is better than being a data engineer.",False,0.92,https://www.reddit.com/r/dataengineering/comments/14wv6f6/data_engineer_isnt_really_just_data_engineering/,dataengineering
Tips on leveling up from business intelligence to data engineering,14xoa4g,philspyderman,1689167660.0,,False,True,Career,3,False,1,"I’m proficient in Tableau, SQL, Alteryx, and Excel. Been stuck in the corporate reporting world for a bit too long and I’d like to advance my skill set. 

I’m going to try and stretch for a job that will have me working with SSIS, Automic, Databricks, Fivetran, Medallia architecture, and DBT. I don’t know what any of these things are, not even at a basic level. My friend is the hiring manager so I’ll have a leg up, but I need to be at least conversant in these tools and technologies. 

Of course I will be YouTubing and googling these things myself, but figured I’d ask if anyone here knows of a particularly great resource, course, book, etc that provides good coverage of the things I listed, and is catered to someone with my background. Thanks in advance!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xoa4g/tips_on_leveling_up_from_business_intelligence_to/,dataengineering
Flink or Kafka Streams,14xhrhj,BigDataMax,1689148809.0,,False,True,Help,3,False,2,"Hello,
I have platform basing on Kafka. A lot of events go through various topics. I am writting streaming app to process events in real time(join on streams are possible). I consider Kafka Streams and Flink as a streaming technologies. My first thought was to use Kafka Streams but I heard that there is some problem with inner Kafka topics during join operations. Could uou gove me some advice what would u use?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xhrhj/flink_or_kafka_streams/,dataengineering
Advice on Microsoft Azure Data Engineer Associate Certification,14xfm81,VastDragonfruit847,1689141588.0,,False,True,Discussion,6,False,2,"Hello!  


I am currently doing two things :   
\- Religiously auditing the Coursera course playlist (DP-203) -- For the exam  
\- Trying to work through the book - ""Fundamentals of Data Engineering"" -- For the industry

Are there any other resources that are really helpful in acing the certification exam?

Would doing the Azure 900 - Azure Fundamentals course would help me with this course? (I have no previous exposure to any of the cloud computing services)?

Also, is anyone else currently preparing for this certification? I think this course might get a little overwhelming as move further in the course. Hence it'd be nice to have a study buddy. Please don't be shy to reach out if you've begun the course.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xfm81/advice_on_microsoft_azure_data_engineer_associate/,dataengineering
The Preview of Stream Processing Performance Report: Apache Flink and RisingWave Comparison,14wyfs8,yingjunwu,1689098237.0,,False,False,Blog,5,False,12,,False,0.85,https://www.risingwave.com/blog/the-preview-of-stream-processing-performance-report-apache-flink-and-risingwave-comparison/,dataengineering
Resources for learning more about data & analytics products,14xgo7v,dr-data-2001,1689145025.0,,False,True,Discussion,0,False,1,"I’m 2.5 years into my career as a DE.  I’ve got my EL and data modeling skills pretty well developed, but I am rather weak in the analytics aspect.

When I say analytics, I mean extracting value from the data I provide.  I can move, organize, and document data very reliably, but I have no idea what BI folks actually *do* with the data.

I’ve tried asking questions to get a better understanding, but they’re busy, and I’m really lacking foundational knowledge on metrics and BI terminology/methodologies to keep up with the conversation.

Anyone know of good books or resources to get up to speed?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xgo7v/resources_for_learning_more_about_data_analytics/,dataengineering
Apache Kafka,14x6c6f,Kratos_1412,1689116033.0,,False,True,Help,3,False,5,"I am learning Apache Kafka, and I have completed the ""Apache Kafka Series - Learn Apache Kafka for Beginners v3"" course by Stephane Maarek. What do you think should be my next step?",False,0.86,https://www.reddit.com/r/dataengineering/comments/14x6c6f/apache_kafka/,dataengineering
Tool decision - What architecture would you choose and why?,14wqux8,hansguckdieluft,1689080684.0,,False,True,Discussion,27,False,21,"Hey guys,

I do have a very nice challenge - a big corp that needs a new data architecture - greenfield.  
But with quite the unique setup, why I'm very keen to hear about your thoughts. For details see below.

I already have my ideas and plans, but I do not want to influence you, yet.  


My question is, what tools or architecture setup would you choose and why? Let's discuss two scenarios:  
a, Budget is not an issue - what'll be your dream setup?  
b, Budget and resources are an issue - what and why would you choose?  


Looking forward to your insights!

&#x200B;

Challenge:

* Fast-growing corp with 1-3 newly acquired sites per year (completely unknown and probably legacy tech)
* Most uses cases are classic: sales, finance, operations, scm
* Currently 4 sites worldwide 
* Goal: Implementation of a central data infrastructure to support local analytics use cases – NOT to build a big integrated warehouse
* Preferably SaaS tools, but open for discussion
* Data strategy: Data Mesh-like approach: Holding provides infrastructure and expertise, local IT staff and business users shall build use cases with holding support
* Looking for a data integration solution that is able to integrate (flexible and agile) existing and future sites to boost use cases with high ROI
* It's all about getting the data from on-prem into the cloud, as efficiently as possible
* Nice to have: ELT, low maintenance, automated processes, and  data transformations streamlined as CI/CD
* The integration tool must have connectors to the common legacy ERP systems/databases: 
   * MySql, PostgreSQL
   * MS SQL
   * Oracle
   * SAP ECC, R3, S4HANA (on-prem & private cloud)
   * MS Navision (SQL)
* Other sources: SaaS tools, FileServer, Sharepoint, OneDrive, flatfiles, Excel, etc. 

&#x200B;",False,0.97,https://www.reddit.com/r/dataengineering/comments/14wqux8/tool_decision_what_architecture_would_you_choose/,dataengineering
Need help from Spark Gurus,14wwzs9,Straight-End4310,1689094975.0,,False,True,Help,24,False,10,"Backdrop:
I am using pyspark on AWS EMR (22G Mem available, 8 VCores). Trying to process 12 Billion Rows. 
( I am using default settings for memory and executors, new to spark and for lack of better words, dont know how to fix them optimally)

What my job does:
Reads two dataframes, crossjoins and writes the output. ( Yes, crossjoin is costly but thats what my requirement is, map every row to everyother row)

Problem:
Spark creates 200 tasks at Stage 1, 199 get executed instantly and the remaining task doesn't even get executed in another hour. The issue being, 1 task is being run on a single executor so the second executor is being killed by the driver for sitting idle. ( I want both executors, since there are some stages that will come after this particular stage is executed)

Whats already tried:
1) No, I cant use a broadcast join(both dataset have equal rows)
2) Data skew seems to be an issue but I repartition the data before and after the join ( among the 400 partition, data varies from habing 28 million to 32 million records, I guess it isnt that skewed afterall)


Please help me get over this menace, been stuck here for the past 2 days. My managers been poking me lol.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wwzs9/need_help_from_spark_gurus/,dataengineering
how do you use delta live tables?,14x2omf,PinPrestigious2327,1689107698.0,,False,True,Help,1,False,5,"I am investigating it  and I see that it has 2 limitations:

1.  All tables created and updated by Delta Live Tables are Delta tables 
2. i can’t customize their target delta table file paths to their desired location for bronze, silver or gold tables.

in my organization we load data in bronze with its original format(could be json, parquet, etc) so we can not use DLT for this  if I am not mistaken and we can't use it either for a workflow with multiple notebooks that write in bronze , silver a gold successively.

how do you use DLT? which are the remaining use cases?

thanks",False,0.86,https://www.reddit.com/r/dataengineering/comments/14x2omf/how_do_you_use_delta_live_tables/,dataengineering
Benchmarks for stream processing systems,14x3zux,anupsurendran,1689110618.0,,False,True,Help,9,False,4,"Hi everybody,  I have been trying to look at systems that process streaming data (downstream from Kafka).  One of the inputs to our decision-making process is to have a good understanding of the throughputs these systems/platforms can handle.  I was hoping that we don't have to set up all these environments and run a side-by-side comparison.  Do you have some suggestions on where I can find this information? ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14x3zux/benchmarks_for_stream_processing_systems/,dataengineering
Suggestions on Cosmos Migration,14xc5nx,Global_Industry_6801,1689131180.0,,False,True,Help,3,False,1,"We have around 40 TB of data in Cosmos which we need to migrate from one Tenant to another. No change in the schema. We have been speaking to Microsoft and out of the options they gave, the ones which support this kind of volume are the below

1) Cosmos container migration CLI tool - Supports large volume of data and recovery. No setup is needed. But this is still under preview. 

2) Cosmos spark connector - This will be running in Databricks and makes use of the Cosmos bulk execution library. But this  will need custom code and will cost both the DBR cluster and the cosmos DBs for the migration window (Yet to estimate how much)

Anyone here has migrated such data on Cosmos and what is the approach you took ? Also, anyone used this CLI tool which is under preview . Any suggestions appreciated",False,1.0,https://www.reddit.com/r/dataengineering/comments/14xc5nx/suggestions_on_cosmos_migration/,dataengineering
Am I Using Redshift Spectrum Wrong?,14wzd3l,BananaSpears262,1689100292.0,,False,True,Help,11,False,3,"I really like Redshift Spectrum and find it extremely useful when I have a few big files.

The problem I have is when I have event-based data e.g. output from Firehose and there are lots of smaller files in S3 (millions). I have this scenario where there are a lot of files and the file sizes are not big but it still takes a very long time to read with Redshift Spectrum. The data is partitioned and indexed in Glue based on year, month and day.

Am I doing it wrong? Where is the bottleneck? Is it the S3 Get Requests?

Thank you",False,0.81,https://www.reddit.com/r/dataengineering/comments/14wzd3l/am_i_using_redshift_spectrum_wrong/,dataengineering
Graduating with a masters in data engineering. What advice do you have for job hunting?,14wsjy0,Possible-Health6784,1689084764.0,,False,True,Career,8,False,8,"I’ll be doing my last semester this upcoming fall at my university for their masters of science in data engineering. I currently work as a frontend developer. Now, I know that data engineering and frontend are not remotely close, but I needed a place to start working while completing my degree and the market wasn’t doing too hot, so I took this job (and I also hated my previous job). 

Some classes that I took include machine learning, big data and data science, a database and a distributed databases, data visualization,  multimedia processing, and information retrieval and web search. There are some others that I can’t remember, but I’ll be taking NPL and computer security in the fall. 

My plan is to stay at my job for about a year from now as I’ve been with them for 8 months and I’ve only stayed at my previous job for 6 months. I’m planning to take that year as interview prep time. I’m not expecting to get a job right into a FAANG, but I would like a data engineering job with a higher salary than what I currently make as a frontend developer ($70k a year in Dallas,TX with no prior experience). 

Any advice is greatly appreciated",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wsjy0/graduating_with_a_masters_in_data_engineering/,dataengineering
"From DBA to Data Engineer, what learning path to take?",14ww6ky,irtaza23,1689093184.0,,False,True,Help,5,False,4,"So as the title suggests, I'm currently working as a DBA mainly specializing in performance optimization, and I want to make a career shift towards Data Engineering but I'm confused of the roadmap or what to learn.

Currently I have experience and skills in SQL, Teradata, and a beginner Python (projects made using Pandas, Tkinter, and Data scraping libraries)
Additionally I have done some visualization on Power BI and I'm decent at it and DAX.

The country where I'm looking to apply is mostly dominant in Azure Cloud technologies for data engineering roles,  but most of the job postings also require experience in Python and ETL. 

My initial goal was to do the DP 203 Azure Data Engineer Associate certification but now I'm thinking of learning ETL, Data orchestration and Data modelling before I jump to cloud.

My plan is to learn spark with databricks, pyspark for ETL. Maybe some Airflow for data orchestration and Kimball for data modelling

Should I go for the DP 203 certification and then focus on the spark, pyspark and Airflow or do this in reverse or is there another better roadmap which could cover ETL, spark and cloud?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ww6ky/from_dba_to_data_engineer_what_learning_path_to/,dataengineering
Applying Data Engineering best practices to ML?,14x1r3k,lyrical_empirical,1689105657.0,,False,True,Discussion,0,False,2,"Hey all,

I'm on a relatively small and new Data Science team and currently trying to figure out the best tech stack/practices for deploying ML models. Right now we use BigQuery for our data base, DBT for the SQL pipeline (am I even referring to it properly?), and VertexAI workbench where the modeling and experimentation is done, and then a BitBucket repo. There is Airflow usage among the data engineers in other teams so there is a precedent there as far as orchestration goes.

Given that most of my past experience has focused on ad-hoc experimentation and not actual deployment, was hoping to get some general advice on how to set certain pipelines. 

Ultimately, going forward the current suite of models will have to be retrained on a weekly basis, and the results will have to be uploaded to an environment where analysts can interface with the results.

 Currently trying to figure out best practices for building a robust pipeline given the tools we have above and was hoping to get some community feedback from more seasoned engineers.

Right now most of the models are retrained manually.

&#x200B;

General outline of a pipeline that is currently in use that I feel needs to be refactored. 

1.)  SQL queries to pull data into python environment, where feature engineering in performed. (Some of the features are generated in the SQL queries where possible). Features are created using functions and the table with the target variable and features are written to a new table in GCP.

2.)  In a separate script, read in the feature table, and then proceed to do some training (train-test splits / Cross Valdiation / Hyperparameter tuning) -> hyper parameters are saved in a pickle file. Final model is trained using the found hyper paramters -> final model saved in pickle file. 

3.) In separate script, the feature table is again read into python and the model is loaded from pickle and used to generate the output we need. The predicted values are then read into a final big query table which is referenced by analysts/data engineers for client facing platform.

&#x200B;

What would be some better ways of structuring the pipeline above, or are there github ML pipeline examples that exist that I can learn from?

&#x200B;

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14x1r3k/applying_data_engineering_best_practices_to_ml/,dataengineering
CI/CD tool for monitoring changes to analytics events,14ws6oh,aesboe,1689083922.0,,1689085928.0,True,Open Source,0,False,7,"Hi all, many data teams have to deal with broken analytics because events are changed by product developers in the tracking code without any heads-up. We built an open source tool for monitoring changes to event structure before developers merge their code. It integrates with Github but you can plug it into other CI/CD systems as well: [https://github.com/marketplace/actions/syft-event-analytics-monitor](https://github.com/marketplace/actions/syft-event-analytics-monitor)

https://preview.redd.it/k3avxh3mjcbb1.png?width=1024&format=png&auto=webp&s=b09433c1562853b65b9d62ad14eebd5d42d1958b

Currently, it supports tracking code written for Segment, Amplitude, Mixpanel, and Google Analytics. Please try it out - would love your feedback and contributions! Also happy to answers your questions here 🙋🏼‍♂️. Thanks!",False,0.9,https://www.reddit.com/r/dataengineering/comments/14ws6oh/cicd_tool_for_monitoring_changes_to_analytics/,dataengineering
Azure ML or Azure Data Factory pipelines for this specific case?,14wwl54,,1689094091.0,,False,True,Discussion,2,False,3," 

We are currently storing images, and text in AWS s3, and small percentage of data comes with annotations. Every week we should remove most of the annotated data and keep only data that are relevant for further training the models which we are currently doing using airflow and sampling techniques.

We want to migrate fully to the cloud and we are rewriting ML pipelines to Azure stack using Azure ML and storing data in Azure blop storage.

We want to include sampling pipeline, which basically downloads the data from azure blop storage, do some sampling techniques and moves or copies/deletes data to a new location in azure blop storage and creates new data assets.

Would it make sense to include sampling pipeline in Azure ML pipelines, or rather Azure data factory pipelines? Are there any advantages/disadvantages in using either of those?

 ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wwl54/azure_ml_or_azure_data_factory_pipelines_for_this/,dataengineering
Canary Release Open Source Tool to integrate with Apache Airflow - Seeking Your Feedback!,14x5p6i,Specialist-Key-1972,1689114503.0,,False,True,Open Source,0,False,1,"Hi!

I'm excited to introduce CanaryPy, a new open-source tool designed to enhance your experience with Apache Airflow by introducing new releases of your data pipelines minimising the impact of unanticipated issues.

We'd love for you to check it out on GitHub: [https://github.com/thcidale0808/canarypy](https://github.com/thcidale0808/canarypy)

Your feedback and suggestions for improvement are precious to us. What features would you like to see? How's the usability? Would you have any thoughts on integration with your current tools?

Thank you in advance for your insights!

Best,",False,1.0,https://www.reddit.com/r/dataengineering/comments/14x5p6i/canary_release_open_source_tool_to_integrate_with/,dataengineering
Delta sharing vs Databricks connector for Power BI?,14x5jje,damoex,1689114127.0,,False,True,Discussion,0,False,1,"I’m building the Lakehouse concept for clients, using Databricks. I’m using dbt for the warehouse part. So, I generally end up with some sort of dimensional model in a “gold”-layer. 

Now most clients use Power BI to visualize this data. I was wondering, previously mostly,  for lack of better alternative I’ve used the Databricks connector in Power BI. However, nowadays there’s the delta share connector. 

Both have advantages and disadvantages I guess. Any suggestions which you would use and why?",False,0.67,https://www.reddit.com/r/dataengineering/comments/14x5jje/delta_sharing_vs_databricks_connector_for_power_bi/,dataengineering
Who manages data infrastructure in large chains,14wvrn9,eliamartali,1689092269.0,,False,True,Discussion,4,False,3,"How is data infrastructure managed in large chains such as grocery stores? 

When I visit grocery store job postings website, I don't often come across data-related job postings. Do they typically hire third-party companies to handle their data infrastructure and analytics? 

How to search for these 3ed party companies. Do they have a specific term?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wvrn9/who_manages_data_infrastructure_in_large_chains/,dataengineering
The Data Modeling Divide,14wsq7q,carlineng_,1689085190.0,,False,True,Blog,1,False,4,"I've been thinking a lot lately about data modeling, and why despite all the online discussion about ""best practices"", it all still seems so mediocre. That culminated in this blog post about what I call ""[The Data Modeling Divide](https://carlineng.com/?postid=data-modeling-divide#blog)"". Would love your comments/feedback.  


(BTW, I'm a PM at Google working on the Looker team, so while this isn't a promotional post, it's definitely related to my day-to-day work!)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wsq7q/the_data_modeling_divide/,dataengineering
Need advice : Ingestion pipeline tool,14wuip0,derekoms_,1689089488.0,,False,True,Discussion,2,False,3,"Hi all, I need some advice on how to create a data processing pipeline.  
I've seen a few tools (airflow, dagster, prefect and others) but I'm not sure if they meet my needs.

It needs to be deployed inside k8s and can use Azure tools.  


Here's the input data:

* A batch of files (different extensions) linked to a client and a project that can arrive at any time.
* The files are stored in a MinIO storage space. 

&#x200B;

I want to process the files one by one, preferably using an API to trigger the job. 

Each file will pass through the pipeline, but not through the same jobs. Some extension types will require different processing (or can be excluded). 

I also need a dashboard:

* I need to be able to filter by client and project. 
* I need to see pipeline progress (by file) 
* Track the performance of each task. 
* Can be added to client app

Re-run jobs/tasks: 

If a file is made up of the following jobs: \[A -> B -> \[C, D\]\].  
I'd like to be able to re-run from step B, which will trigger the re-processing of C and D.

  
I feel like I have to combine different tools to get the pipeline + the dashboard.

  


We welcome any contributions or tool recommendations!

Thank you very much!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wuip0/need_advice_ingestion_pipeline_tool/,dataengineering
Typical interview with Airflow enjoyer,14vw6y3,ponkipo,1689000632.0,,False,False,Meme,25,False,279,,False,0.97,https://i.redd.it/2231e36jh5bb1.jpg,dataengineering
Getting hard to become a DE,14x8uz2,_mathius_,1689122428.0,,False,True,Help,13,False,0,"Hi, I have experience doing web scraping in Python + MongoDB, I'm trying to watch videos, and so, but I can't. I know, the ammount of info available should be enough, but its hard for me. I have a good programming background I think  


The point is, anyone need help in a project? Maybe I could do some DE-like work with the thinks that I already now while I learn new things, I think that maybe could be a good way to learn about Data Engineering. If not, maybe some of you could give me some tips and if you were in my situation before, how you end becoming a DE? It seems imposible and the only I see is the days going and going and I feel that I'm learning Nothing.  


Thank you very much guys ",False,0.38,https://www.reddit.com/r/dataengineering/comments/14x8uz2/getting_hard_to_become_a_de/,dataengineering
"Fluxsort: A stable quicksort, now faster than Timsort for both random and ordered data",14wkir3,codorace,1689061657.0,,False,False,Blog,2,False,11,,False,0.92,https://github.com/scandum/fluxsort,dataengineering
So do we need to set executor memory and core in Azure Databricks,14x02a8,am_oldmonk,1689101877.0,,False,True,Help,0,False,3,"Am new to working in Azure Databricks, am trying to run a job in the workflow, that creates a job cluster on the run that has one driver and 3 workers each with 56G memory and 8 cores. My question is do we need to set number of executors, memory and cores. 
What happens when we set number of executors and what happens when we don't",False,0.8,https://www.reddit.com/r/dataengineering/comments/14x02a8/so_do_we_need_to_set_executor_memory_and_core_in/,dataengineering
Data modeling in Databricks,14we9ye,,1689042714.0,,False,True,Discussion,17,False,19,"I am working on modeling and organizing data in Databricks. We created 2 workspaces - one for non-prod and one for prod and are using unity catalog. I am going to go with the Databricks recommended medallion architecture for storing data in 3 zones based on the readiness/transformation. 
My questions are as follows,
1. How do I organize these 3 zones? Would that be 3 different catalogs or 3 different schemas under one catalog? We currently have “non-prod” catalog and “prod” catalog created for non-prod and prod env respectively.
2. Do I have to name them bronze, silver and gold or can I name them whatever makes sense for us? For ex: raw, refined and enriched
3. From my understand, the first zone maintains data as-is from the SOR, second zone has necessary transformations, cleansing or mapping based on business needs. And the data modeling actually happens in the last zone where business systems consume this data. I.e Data Marts

Is my understanding right? 

Anything else I should be considering while modeling the data? 

Thanks in advance!",False,0.92,https://www.reddit.com/r/dataengineering/comments/14we9ye/data_modeling_in_databricks/,dataengineering
Top 10 Resources for Data Engineers | What am I missing?,14wzqgi,nathan_data_engineer,1689101150.0,,False,True,Blog,0,False,1,"Wanted to share this with the community! I have collected this list of [resources for data engineers](https://www.polytomic.com/blog-posts/top-10-resources-for-data-engineers) that I posted on my company's blog.

Am I missing anything? What else would you add?

(Full disclosure: the blog has a promotional piece at the end)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wzqgi/top_10_resources_for_data_engineers_what_am_i/,dataengineering
Need help with a small fix in a project,14wy9l1,Uchiha_pandu,1689097845.0,,False,True,Help,0,False,1,"So, I am working as an intern for a company with 3 months and I expected the work to be of Data Analysis(Power BI, Tableau, python ) but the work is mainly related to Data Engineering. I have no problems with this and I am grateful that I will be able to learn something new through this, but I have never performed work related to Data Engineering.  
So, there is a project that I am assigned to and I am kind of facing  a small issue with the ETL process.

Here is the Stack Overflow link to the issue - [https://stackoverflow.com/questions/76657867/bulk-insert-fixed-width-text-file-not-loading-properly](https://stackoverflow.com/questions/76657867/bulk-insert-fixed-width-text-file-not-loading-properly)

Any suggestions will be helpful",False,0.67,https://www.reddit.com/r/dataengineering/comments/14wy9l1/need_help_with_a_small_fix_in_a_project/,dataengineering
Comparing Message Queues and Message Brokers,14wkbt4,Glittering_Bug105,1689061041.0,,False,True,Blog,0,False,7,[https://medium.com/memphis-dev/comparing-message-queues-and-message-brokers-understanding-the-differences-dc776f0c3002](https://medium.com/memphis-dev/comparing-message-queues-and-message-brokers-understanding-the-differences-dc776f0c3002),False,0.82,https://www.reddit.com/r/dataengineering/comments/14wkbt4/comparing_message_queues_and_message_brokers/,dataengineering
Is it possible to automatically get Google Play store app ratings?,14wwreq,Alan_Silva_TI,1689094476.0,,False,True,Help,4,False,1,"Hi everyone!  


Google offers a web page where we can download the app ratings in csv format, but that only works for manual/individual download's I'm looking for a solution to automate the process. 

I've tried to use browser automation with Selenium and Puppeteer, but it doesn't work anymore as Google started banning web drivers to connect to their services since 2021.

Do you guys know any tool or solution for this?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wwreq/is_it_possible_to_automatically_get_google_play/,dataengineering
Langchain intro and getting started,14wqdrb,de4all,1689079475.0,,False,False,Blog,0,False,2,,False,1.0,https://decube.substack.com/p/langchain-intro-and-getting-started,dataengineering
"Datawarehouses Explained: What, How and Pricing",14wn7yo,MisterHide,1689070689.0,,False,False,Blog,2,False,3,,False,0.8,https://bitestreams.com/blog/datawarehouses_explained/,dataengineering
"Challenges of AI, Declarative & Imperative Code for Data Engineering",14wpoc3,AmphibianInfamous574,1689077602.0,,False,False,Blog,0,False,2,,False,1.0,https://patrikbraborec.substack.com/p/data-news-35,dataengineering
Beginner friendly project ideas using Snowflake.,14wicxy,smol_Caterpillar_21,1689054820.0,,False,True,Discussion,1,False,5,I'm a fresher and I have been learning snowflake for some time now. But I wanted to take up a small project which is beginner friendly. I also have an interview coming up for an entry level position which has Snowflake as one of its requirements. I wanted to work on a small project before the interview. Any ideas or suggestions on any suitable beginner friendly projects using Snowflake?,False,0.78,https://www.reddit.com/r/dataengineering/comments/14wicxy/beginner_friendly_project_ideas_using_snowflake/,dataengineering
Anything one should know before going for self-hosted dbt?,14w832y,verysmolpupperino,1689026969.0,,1689036352.0,True,Help,20,False,25,"At my company we developed our own metrics layer, over time we've started using it to build our views. It works, but having the same service handle both missions has increasingly complicated development. We toyed around with writing materialized views directly in SQL, but having no source control or CI/CD is simply not something I'm willing to do.

After a good amount of research, we decided to move our views to dbt. I've heard a lot of consistent complaints about dbt cloud and how it's not worth the money, we have the manpower to self-host and CTO backing, so it feels like the reasonable way to go.

I'm just trying to minimize unknown unknowns, and appreciate any 2 cents.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14w832y/anything_one_should_know_before_going_for/,dataengineering
"I wrote a short primer on fact tables in data warehouses, let me know if I need to add anything",14w9ryg,lancelot_of_camelot,1689031020.0,,False,False,Blog,1,False,9,,False,1.0,https://anniscodes.com/posts/primer-fact-tables/,dataengineering
Codifying validation rules,14wirv9,swanlooker,1689056073.0,,False,True,Help,0,False,2,"I have four or five messy data sources which I am trying to match and merge. The resultant fields need to pass various validation rules. Some could be simple NULL/NOT NULL, some might be Regex, some might be enums, some might be conditional based on other field values. 

I am trying to find a methodical way to document these validation rules, such that they can be reused across various systems in future. 

How have you all approached this in the past? Python snippets? SQL? What tools/libraries do you actually use to run the validations. Where do you store the rules for re-use?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14wirv9/codifying_validation_rules/,dataengineering
Advanced MongoDB Queries in JavaScript for Advanced Data Operations,14wmhx4,codorace,1689068372.0,,False,False,Blog,0,False,1,,False,1.0,https://medium.com/@vikramgyawali57/advanced-mongodb-queries-in-javascript-for-advanced-data-operations-b651e245599f,dataengineering
A Data Layout for Data Lakes,14wmdfz,paolapardo,1689067949.0,,False,False,Blog,0,False,0,,False,0.5,https://qbeast.io/effective-data-layout/,dataengineering
Do Data Engineers get Enough Programming/Coding opportunity ?,14vz8k1,micky_357000,1689007514.0,,False,True,Help,33,False,22,"I was doing backend internship with nodejs and I got to know that I don't like doing frontend and enjoying backend comparatively more.

I got a Junior data engineer job offer which I have accepted, But when I asked someone about how much programming does a DE gets to do.

He said "" DE does not include much Coding.the main work is to building pipeline and databases and some views may be. I will not considered it a coding stuff""

I do enjoy programming, so can anyone here in this space share their views that do you guys as DEs code enough , how much percent of your work is really programming stuff? 

I have heard that Data engineering is software engineering specialising in data.",False,0.89,https://www.reddit.com/r/dataengineering/comments/14vz8k1/do_data_engineers_get_enough_programmingcoding/,dataengineering
Rules execution engine using LATERAL joins,14w4thx,Chuck-Alt-Delete,1689019835.0,,False,True,Blog,0,False,6,"Hey all, I’m a field engineer at Materialize and thought I would share a little pattern we’ve seen come up a few times. Hope you find it useful!

https://materialize.com/docs/transform-data/patterns/rules-engine/",False,0.88,https://www.reddit.com/r/dataengineering/comments/14w4thx/rules_execution_engine_using_lateral_joins/,dataengineering
Need advice: Setting up a data pipeline to ingest daily .csv files into snowflake,14w7r3g,nightslikethese29,1689026235.0,,False,True,Help,17,False,4,"I'm a data analyst learning how to be a data engineer. 

I've inherited a process that currently involves:

1. Vendor sends a daily .csv file to an SFTP
2. Alteryx is used to copy the file to our shared drive, run data transformations, and upload to snowflake. The file is deleted from the SFTP after a successful run in alteryx. 

The process usually takes about 15 minutes daily, roughly 400k records each day. I would like to optimize this process and make it better and more secure. For example, I don't think storing files on a network share drive is smart. 

I have access to GCP resources such as cloud storage and cloud composer (airflow). I'm comfortable writing stored procedures and doing data transformations inside snowflake instead of alteryx. 

How can I best optimize and create a data pipeline to utilize snowflake's compute resources for transformation and parallelism for ingestion? 

Any thoughts or ideas welcome as I'm very new to these concepts!",False,0.83,https://www.reddit.com/r/dataengineering/comments/14w7r3g/need_advice_setting_up_a_data_pipeline_to_ingest/,dataengineering
Tools for keep dbt model and YAML in sync,14w9syy,Creative-Aside-4145,1689031091.0,,False,True,Help,0,False,3,"Hello!

For those of you who use the dbt framework, how do you try to keep the YAML in sync with the model? I’m coming across situations where I removed a column from the model but forgot to remove it from the YAML, or added a column to the model but forgot to update the YAML.

The closest I found was the query posted in this Github issue: https://github.com/dbt-labs/dbt-core/issues/1570#issuecomment-1500395582 . 

Thank you in advance!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14w9syy/tools_for_keep_dbt_model_and_yaml_in_sync/,dataengineering
"Onetable and Delta Lake UniForm both going for Delta, Iceberg, Hudi interop?",14wffkb,InfamousPlan4992,1689045905.0,,False,True,Open Source,4,False,1,"I didn't realize Delta UniForm was only 1-way directional, but now I feel like it is a lock-in trap. These format wars are just draining. I really hope Delta, Iceberg, Hudi can all get together on this new Onetable thing. Do you think they have a shot at unifying or will they always remain as translations?:

[https://www.onehouse.ai/blog/the-road-to-an-open-and-interoperable-lakehouse](https://www.onehouse.ai/blog/the-road-to-an-open-and-interoperable-lakehouse)",False,0.67,https://www.reddit.com/r/dataengineering/comments/14wffkb/onetable_and_delta_lake_uniform_both_going_for/,dataengineering
"Turning Data into graphs/ visuals. Can someone help? I’m recording chickens using various colored ramps, feeders, nipples, and balls for a school project to find out their color preference. Can someone point me to the right direction on how to turn this into a graph? Thanks :)",14w6xtj,Blubberrloverr,1689024425.0,,False,False,Help,1,False,3,,False,1.0,https://i.redd.it/1r3r0h7sg7bb1.jpg,dataengineering
How to get an interesting job in DE?,14w25nl,fire_air,1689014004.0,,False,True,Discussion,6,False,4,"Hi, it's my first post on reddit🥳. I want to start a discussion to get a better understanding of what a good DE should dream about. So my question is which DE jobs are the best ones and which knowledge and experience should I have to get them. Also, a lot of jobs in DE are concentrated in US and I am interested in whether it is possible to get a good DE job from other countries, for example Poland; or these countries, where outsourcing is the biggest part of SWE job market, will inevitably get only the most boring things?  What I can think about is progressing in distributed systems and databases, but for some reason I don't see any demand for such knowledge, for example recently I have implemented a sharded kv store on top of Raft and it seems it hasn't changed my job market value for some reason. Is the only way for a third country national to get noticed is to contribute to spark github? Will appreciate all opinions",False,0.75,https://www.reddit.com/r/dataengineering/comments/14w25nl/how_to_get_an_interesting_job_in_de/,dataengineering
Looking for Energy or Neurobiology-related Portfolio Projects for cognitive science students.,14vrw8x,International-Shirt5,1688990015.0,,1688991230.0,True,Discussion,2,False,14,"Im seeking interesting ideas for two portfolio projects related to the broad field of electrical energy or neurobiology/biology. For these projects, I would primarily like to utilize **AWS** and **Apache Airflow**. They will be semester-long projects for students. For example, they could include:

1. **Predicting energy demand using meteorological data**
2. **Optimizing the utilization of renewable energy based on weather forecasts**
3. **Multidimensional Brain Data Analysis for Neuronal Activity Pattern Identification**

So I'm looking for an interesting idea, mainly using AWS and at least a bit of Airflow .

Do you have any interesting suggestions? The key factor for me is selecting projects that will engage the students, as they will only be motivated if the projects are meaningful. They are proficient in Python, SQL, and have a basic understanding of AWS.",False,0.94,https://www.reddit.com/r/dataengineering/comments/14vrw8x/looking_for_energy_or_neurobiologyrelated/,dataengineering
How to break into FAANG?,14vicy3,highlifeed,1688959306.0,,False,True,Career,115,False,38,"I know that a lot of people say data engineering career at FAANG is not a full experience of data engineering, but it’s been my dream to have a career at FAANG. I just started my career as a data engineer not long ago, but the tech stack isn’t very modern and I don’t use Spark and not even Python. We use Informatica to do all the work, so I imagine I would have to build up my experience in other technologies to fulfill the Data Engineer job requirement at FAANG. With that said, how should I prepare for a data engineer interview at FAANG from ground up? I am thinking of applying possibly end of next year, so I will have around ~2YOE or less if I apply earlier. I know that grinding leetcode and statascratch is a must to prepare for the interview, what are some other stuffs? Also what company among the FAANG has the best data engineering experience?",False,0.74,https://www.reddit.com/r/dataengineering/comments/14vicy3/how_to_break_into_faang/,dataengineering
Need Advice: Struggling with Career as a Data Engineer (Sydney),14vp78r,Many-Local-765,1688981815.0,,False,True,Help,9,False,12,"Hey there,

I'm reaching out to this community for some advice regarding my current career situation. Here's a brief overview:

* Graduated with High Distinction from University of Sydney with majors in Computer Science and Data Science.
* Over 1.5 years of experience as a Data Engineer, proficient in Python, SQL, IaC (Terraform, Puppet), and CI/CD.
* Currently  on a Grad Working Visa in Sydney, expiring end of next year. Working  towards Permanent Residency (PR) application but it is always uncertain.
* Recently  switched jobs to a company (XX) as a Data Engineer, but the actual role  differs significantly from the job description. My team mainly use Alteryx for data wrangling, not the Python and ETL work I expected.
* After  a month at XX, no technical projects have come my way despite me asking  my manager for more coding related tasks. Started looking for new jobs  but facing constant rejections (not even interviews/coding tests),  making me doubt my abilities or if my visa status is hindering me.
* Losing hope for employer sponsorship due to inability to even secure a job in the first place.

Seeking  advice/suggestions for next steps or insights from anyone who has been  through a similar situation. Please share your thoughts.

Thank you in advance.",False,0.87,https://www.reddit.com/r/dataengineering/comments/14vp78r/need_advice_struggling_with_career_as_a_data/,dataengineering
Azure SAP CDC connector considerations,14vvn0r,Commercial-Ask971,1688999374.0,,False,True,Help,5,False,4,"Hello
Ive got a task to create a pipeline to replicate sap data and then get only deltas. I know that there is SAP CDC connector to which I need create self hosted integration runtime on client site. I've got a time to create a list of questions to client. What should I take into consideration? My first question is whether they do already have SHIR and we can share to reduce costs and maintainance time. If they do, do I need to install anything on my machine? Its my first commercial project, all self study was based on auto-resolve IR. After I am done with IR I guess its pretty straightforward, just need linked service pointing to this IR, server name, number, client id and dataset with proper ODP framework 

If they dont, do I need to create a VM, to install SHIR there, and SAP .NET connector in there? 

What I can even ask them?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14vvn0r/azure_sap_cdc_connector_considerations/,dataengineering
dbt seems like such a cool concept,14vcujs,NFeruch,1688944095.0,,False,True,Discussion,50,False,50,"fyi I’m a DE intern right now, but I’m learning about dbt for the first time and it seems really cool so far

The main benefits I see are:
* Time saving
  * You don’t have to worry about any CREATE TABLE, INSERT INTO, or UPDATE syntax. dbt takes care of all of it for you and converts your sql statement into a new table, which leads to the next point of…
* Easiness
  * dbt seems really straightforward. there are “models” (which are literally just sql files with select statements in them) that you can chain up with other models to create transformation pipelines. Instead of having to programmatically do the T in ETL, anyone who knows sql can create the data that they need (provided all the data is present in the source table)

I’m working on a personal project that was using mysql, but am migrating to postgres so that I can use dbt. I figure it will simplify things for me on the data cleaning side, as a lot of the logic is heavily embedded in the script and not easily testable/accessible.

I may be getting ahead of myself as I’ve not had the chance to play around with it in depth, but the switch from ETL to ELT with dbt seems like such a good concept",False,0.89,https://www.reddit.com/r/dataengineering/comments/14vcujs/dbt_seems_like_such_a_cool_concept/,dataengineering
How to Leverage Big Data Analytics for Sustainable Competitive Advantage,14w4sar,Competitive_Speech36,1689019764.0,,False,True,Blog,0,False,1,"As a seasoned IT architect currently working with a startup, I wanted to share my journey with you all, as well as some of the valuable insights I've discovered along the way. Here's an article where I dive into how organizations can use BDA to establish a sustainable competitive advantage, and some of the strategies to effectively implement it. I hope you find this beneficial in your own journey as well.

# How to Leverage Big Data Analytics for Sustainable Competitive Advantage

Today's technological era generates a massive amount of data, and businesses everywhere are trying to turn this raw data into actionable insights. Big Data Analytics (BDA) has emerged as a key strategy, helping businesses gain unique insights to unlock new opportunities and differentiate from competitors. Ignoring BDA might leave you trailing behind in the competition, or missing out on potential advantages.

To achieve the strategic benefits of BDA, you need to understand the processes that allow it to add value and remain competitive. This article offers a guide on how businesses can use several frameworks to evaluate BDA’s strategic value while avoiding pitfalls that come with improper implementation.

The VRIO framework (Valuable, Rare, Imitable, Organizationally embedded) can help assess the potential of BDA to create strategic business value. It prompts businesses to question if their BDA strategies offer valuable insights, are unique, challenging for competitors to copy, and supported by their organizational strategies and culture.

# Value of Big Data Analytics

The key strength of Big Data Analytics lies in its ability to provide unique insights that can be used to seize new business opportunities or counter competitive threats. These insights can improve various business areas, including business processes, product innovation, customer experience, and overall organizational performance.

# Uniqueness in Big Data Analytics

BDA becomes unique or 'rare' when few competitors can acquire or possess similar capabilities. Rarity in BDA can be evaluated in two ways: proprietary data content and analytical capability developed through experience.

# Imitating Big Data Analytics

BDA can be difficult and costly for competitors to imitate due to factors like time investment, the uniqueness of proprietary algorithms, and the maturity and culture of a company's IT department.

# Embedding Big Data Analytics

The final consideration is whether BDA can be organizationally embedded. It can be achieved when BDA aligns with the company's long-term business strategy and is facilitated by processes, policies, organizational structure, and corporate culture.

# Creating Value from Big Data Analytics

Creating strategic value with BDA requires investments in data assets, technological assets, and human talent. A conceptual framework can be proposed to describe how BDA creates strategic business value. This process can be framed by two concepts: Dynamic Capabilities and IT-Value Models. These models help in the capability building and capability realization processes.

# Building Big Data Analytics Capabilities

The process of turning IT investments into valuable BDA capabilities is dynamic. It includes managing and analyzing data to generate new insights. For this, companies need to develop a BDA strategy and understand how it can create tangible and intangible value.

# Realizing Big Data Analytics Capabilities

The real value of big data lies not in its volume but in the ability to derive meaningful and actionable insights from it. When utilized effectively, BDA can help refine business processes, develop initiatives, identify flaws or roadblocks, streamline supply chains, understand customers better, predict market trends, and develop new products, services, and business models.

In the end, creating value from BDA isn't just about having the right tools and capabilities. It's about using those capabilities to generate results, and then turning those results into actions that impact decision-making, improve customer relationships, enhance processes, and more.

I share more articles like this in my blog. If you're interested, you can visit: https://ainsys.com/blog/2023/06/30/bda/?utm\_source=linkedin&utm\_medium=social&utm\_campaign=data\_engineering&utm\_content=BDA\_analytics&utm\_term=BigData",False,0.6,https://www.reddit.com/r/dataengineering/comments/14w4sar/how_to_leverage_big_data_analytics_for/,dataengineering
Blog - Project Nessie: A Look in the Depths,14w4ne0,AMDataLake,1689019462.0,,False,False,Blog,0,False,0,,False,0.5,https://open.substack.com/pub/amdatalakehouse/p/project-nessie-a-look-in-the-depths?r=h4f8p&utm_campaign=post&utm_medium=web,dataengineering
How are you implementing CCPA compliance?,14vutj2,harrytrumanprimate,1688997497.0,,False,True,Discussion,1,False,3,"California privacy act and similar data protection laws allows users to request that their data be hard deleted from source systems. I've been wracking my brain trying to figure out how to implement something like this. My team uses a kafka/dbt/snowflake stack, and I'm a bit lost as to how hard deletes could flow through data warehouse models. I assume others have dealt with this as it's a legal thing - has anyone implemented this? What was your approach?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14vutj2/how_are_you_implementing_ccpa_compliance/,dataengineering
Salaries in Sweden,14vpyst,23reddituser,1688984233.0,,1688986349.0,True,Career,28,False,6,"what is a decent salary (per month, before taxes) in Sweden (Stockholm) for a data engineer with less than 2  YOE and no other prior technical background? I am preparing for an interview and I want to know how much to ask in case I'm asked",False,0.87,https://www.reddit.com/r/dataengineering/comments/14vpyst/salaries_in_sweden/,dataengineering
Simplify Airflow DAG Creation and Maintenance with Hamilton in 8 minutes,14w24v5,theferalmonkey,1689013954.0,,False,False,Open Source,3,False,1,,False,1.0,https://towardsdatascience.com/simplify-airflow-dag-creation-and-maintenance-with-hamilton-in-8-minutes-e6e48c9c2cb0,dataengineering
Power BI operation LS that are not supported by API,14vttnt,These_Rip_9327,1688995073.0,,False,True,Help,0,False,2,"Hi, 
I'm building a CI/CD Pipeline for power bi. We have some reports which are not allowed to be exported by some users.
I want to programmatically disable the export option of some table visual in some reports. I didn't find any API endpoint that does this operation. I also want to programmatically hide some pages in some reports during the deployment. Is there any way to achieve this?

Thanks in advance",False,1.0,https://www.reddit.com/r/dataengineering/comments/14vttnt/power_bi_operation_ls_that_are_not_supported_by/,dataengineering
When to use DataFlow vs Copy activity,14vxrk0,CauliflowerJolly4599,1689004185.0,,False,True,Help,6,False,1,"Hello,
I've a pipeline in Azure data Factory that copies a view (14 columns * average of rows between 12k and 14k) into my DWH.

It is executed once per week with a pre-copy script as Truncate like Truncates table and insert data.

The pipelines takes very long (30 minutes) with a throughput of 5-10 kb/s and I've some doubt that doing a truncate and copying the rows is not gonna be the best solution.
Also I've put as hypothesis that throughput depends from how much the source is being used.

Would it be time and resource friendly to adopt an Upsert or doing truncate and insert is ok but IR is not optimized ?

Could you share some insights?
When I should use an Upsert and when doing a truncate and insert is best to use?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14vxrk0/when_to_use_dataflow_vs_copy_activity/,dataengineering
DE Conferences Europe,14vv4b4,browsingpatx,1688998178.0,,False,True,Career,3,False,1,"As a Data Engineer , I am currently looking for conferences to visit in Europe (this year or next year). Focus can be data Engineering, Analytics Engineering, etc. Any conferences you guys can recommend? Would also be interesting to here about conferences you didn‘t like and for what reason",False,1.0,https://www.reddit.com/r/dataengineering/comments/14vv4b4/de_conferences_europe/,dataengineering
Merging Data Too Big for Pandas + Moving to Cloud for More Compute Power,14v1nq9,waytoopunkrock,1688917171.0,,1688927412.0,True,Help,72,False,42,"I'm analyzing large datasets of real estate transactions (CSV) and parcels (xlsx) for public policy research (I'm a CS student). The files are separate for each year and type (1.5 GB total).

The goal is to merge sales data (left join) to its associated parcel data for the year of the sale.

Then, identity ownership of properties using fuzzy matching on the owner name + address columns to find the largest owners.

I have 2 main challenges I'd appreciate advice on:

\- I've appended sales data for all years. However, the parcel data is too large to append easily using Pandas on my laptop (memory issues). I think it also makes sense to put these files into a more efficient database or at least file format (parquet perhaps). I might take the approach of merging each year to sales first, that way I don't have to append. But the latter point still stands.

\- Later I will also want to run fuzzy matching on the dataset, which I've already optimized a bit by comparing owner zip code first and creating smaller buckets of names.

I'm thinking I want to create a database (AWS preferable), then use a lambda function for fuzzy matching. But I'm struggling a bit with the details and which service might be best, especially for the merge... I have slight experience with RDS and Lambda. Thanks!

&#x200B;

EDIT: Thanks for the suggestions! For this instance, it seems cloud is probably overkill, and there were some great suggestions for how to avoid it right now. However, in the future, I will have much larger data, so that was part of the motivation for using this as a first example.",False,0.95,https://www.reddit.com/r/dataengineering/comments/14v1nq9/merging_data_too_big_for_pandas_moving_to_cloud/,dataengineering
Tool for Loading Adhoc Excel Files to Warehouse,14vl0ga,fgoussou,1688967646.0,,False,True,Help,14,False,4,"Hello knowledgeable folks,

Are there any tools out there that can upload Excel files to Snowflake with all of the following features?

1) Performant: Can handle multiple files with 100's of thousands of rows in a reasonable time.

2) Allows the user to pick datatypes for each columns, including various date formats (M/D/Y, D/M/Y)

3) Produces basic statistics for each column: max, mean, median, histogram.

I am itching to write my own, but I have a feeling that I would be reinventing the wheel. I refuse to believe that no such tool exists given the ubiquity of Excel files in every organization large and small.

&#x200B;

\-------

More Details:

\------

I deal with a lot of small files Excel files (maximum 100 Mbs) from various clients and online resources. Our usual work flow looks something like this:

1) Business analysts examine the file in Excel, manually, to see what surprise changes the client has done to the agreed file format. If the file is too bad (for example, clientID text field had all its leading zeros dropped because Excel thought it is an integer), we throw the file back to the client and ask for a replacement.

2) Use a rudimentary custom webapp written in R Shiny to read the data to to memory and produce a basic report on each column: mean, max, min for float/int columns, and basic counts for each discrete value in discrete columns. The tool tries to infer datatypes automatically but it often gets this step wrong (mostly fumbles with different international date formats and confuses Int/Text for ID fields). 

3) The webapp loads the data to a staging table in Snowflake using ODBC driver (Super slow compared to Python's Snowflake driver).

4) Append the data from step 3 to existing tables after some more quality checks are performed.

I found the Excel add-in for Snowflake ([https://github.com/Snowflake-Labs/Excelerator](https://github.com/Snowflake-Labs/Excelerator)) to be performant and useful for ad-hoc files, but it keeps crashing every time there is an update to Excel. It doesn't seem like it is actively maintained. It also lacks any data quality checks and has a clunky interface.",False,0.83,https://www.reddit.com/r/dataengineering/comments/14vl0ga/tool_for_loading_adhoc_excel_files_to_warehouse/,dataengineering
Learning IDQ,14vqf57,MathematicianAway453,1688985658.0,,False,True,Help,1,False,0,"Hello, 

 

I need to start learning Informatica, IDQ to be specific as it is a new tool in my workplace. I have no experience with anything related with Informatica and my experience with Data has only been working with Tableau as a Data Analyst for a year, so I am familiar with SQL. 

 

Where do you guys recommend to start? I tried youtube tutorials and some random courses but I felt quite lost. If there are any pre-requisites things to learn about first please share them. ",False,0.5,https://www.reddit.com/r/dataengineering/comments/14vqf57/learning_idq/,dataengineering
Hitting plateau in tech learning at work,14uytfe,thedatumgirl,1688909920.0,,False,True,Career,15,False,40,"I'm currently working in a team that requires a lot of functional expertise, but there aren't many opportunities for technical learning or addressing tech-related problems that requires learning. As a result, I'm feeling demotivated because I thrive on technical challenges. Additionally, the high functional complexity of my work leaves me with a heavy workload, making it difficult to find time for side projects. For those who have experienced a similar situation, how did you handle it? Have you considered switching jobs or teams? I've only been with this company for less than a year.",False,0.98,https://www.reddit.com/r/dataengineering/comments/14uytfe/hitting_plateau_in_tech_learning_at_work/,dataengineering
ETL vs Reverse ETL - why do we have 2 different sets of tools?,14vavtv,,1688939294.0,,False,True,Discussion,12,False,8,"I am working on a project to select the right tools for a centralized data platform where we bring in data from various sources, ingest them into a DW, process the data and send it to destination systems (that include the source system as well). We are exploring Fivetran, Airbyte etc for ingesting the data. However, it just feels a bit weird to know that these tools only support ETL. Isn’t reverse ETL (RETL) just reversing sources and destination connectors? Why aren’t there any tools that can perform both ETL and RETL especially when it’s just the same connectors? For ex: Fivetran has Hubspot connector in the sources list but not in the destination lists. I was expecting these tools to perform both. 

Any insights?",False,0.83,https://www.reddit.com/r/dataengineering/comments/14vavtv/etl_vs_reverse_etl_why_do_we_have_2_different/,dataengineering
Need help: Should I apply for internal job with python and SQL under my belt?,14vbdmg,frustratedhu,1688940469.0,,False,True,Career,9,False,7,"I am currently working as a data analyst with around 2 years of experience. I majorly work in R, python, SQL, excel and Tableau with some fundamentals of spark. Though I have written some python scripts for some analysis, I never worked on ""data pipelines"" or any other sort of automation. I now want to make transition to data engineering. I saw on workday that a few DE positions were open last month and are still active. Given the background i have, should I talk to the hiring manager? Or should I prepare more (learn spark, cloud and make some projects and then try) or just talk to the hiring manager? I have already talked to my current manager that I would like to move to DE and he's ok with it. It would be for the first time that I would be going to internal job so any tips/suggestion would be highly appreciated.
Thanks!!",False,0.88,https://www.reddit.com/r/dataengineering/comments/14vbdmg/need_help_should_i_apply_for_internal_job_with/,dataengineering
What exactly is the lakehouse? Is it a technique or a process? How is a lake house different from a warehouse?,14uwsuk,faizfablillah,1688903965.0,,False,True,Discussion,14,False,37,"Does anyone here already know and implement lakehouse? For a company now running a data warehouse in a controlled manner, is it necessary to convert to a lakehouse? What are the key use cases for a lakehouse? What does a lakehouse have that a data warehouse  does not?",False,0.91,https://www.reddit.com/r/dataengineering/comments/14uwsuk/what_exactly_is_the_lakehouse_is_it_a_technique/,dataengineering
Good Windows Laptop for Big data practice,14vq5gx,Rude-Veterinarian-45,1688984820.0,,1688987172.0,True,Help,19,False,0,"Hello everyone,

I'm seeking your input on purchasing a suitable laptop for coding purposes, specifically for practicing Hadoop and other big data tools. I would appreciate your advice on what key features to consider to ensure I make the right choice.

Given the abundance of options available, ranging from i3 to i9 processors across multiple generations, I find myself in a state of confusion. While I understand that the latest models tend to be more expensive, I am primarily looking for a budget-friendly option. 

Budget: ₹45k-50k INR 

PS: this is for my friend.

[View Poll](https://www.reddit.com/poll/14vq5gx)",False,0.4,https://www.reddit.com/r/dataengineering/comments/14vq5gx/good_windows_laptop_for_big_data_practice/,dataengineering
"What is ""Science and Data Engineering""?",14vdt3p,DismemberedBunny,1688946545.0,,False,True,Help,13,False,2,"Hi, I'm in Spain and there's a career called ""Science and Data Engineering"", I've been searching but I found each of them separated, either Science Engineering or Data Engineering, therefore, is this a mix of both? Would I be able to apply to the jobs a Science Engineer or Data Engineer would be able?",False,0.75,https://www.reddit.com/r/dataengineering/comments/14vdt3p/what_is_science_and_data_engineering/,dataengineering
Fast Joins in Apache Beam,14v23hp,troikaman,1688918262.0,,False,False,Blog,1,False,6,,False,0.88,https://ahalbert.com/technology/2023/07/08/fast_beam_joins.html,dataengineering
Is Kimball’s data modeling really functional?,14ul4sl,Tough_Passion4667,1688865403.0,,False,True,Discussion,63,False,73,"Does Kimball's star schema really work in practice or is it just a theoretical ideal model that companies try to implement?
Have you ever seen an by-the-book star schema implementation that solves business needs?

This is a sincere question. I see star schema being required as “state of the art” and standard knowledge for analytics engineering (or other data warehouse related positions), but is there an alternative?",False,0.93,https://www.reddit.com/r/dataengineering/comments/14ul4sl/is_kimballs_data_modeling_really_functional/,dataengineering
alternatives to the traditional star schema,14v3765,Tough-Error520,1688920945.0,,False,True,Discussion,4,False,5,have any of you seen environments that either completely abandoned the star schema approach or maybe followed it 1/2 way that were successful on their own and just worked? Examples could include building lots of wide tables for example that support the business and just work,False,1.0,https://www.reddit.com/r/dataengineering/comments/14v3765/alternatives_to_the_traditional_star_schema/,dataengineering
Why are there no junior or regular data engineer positions?,14ufjuc,InevitableTraining69,1688850690.0,,False,True,Discussion,82,False,91,"Current data analyst and getting into engineering for the first time ever. Have never worked as a DBA or engineer in any capacity, but extensive experience working with data, SQL, data sets, etc. Would like to learn within my current company how data engineering works and learn the ropes. But there is no regular data engineer position, no junior positions. The only ones I have ever seen are senior data engineer. Why is this?",False,0.93,https://www.reddit.com/r/dataengineering/comments/14ufjuc/why_are_there_no_junior_or_regular_data_engineer/,dataengineering
What is a Data Warehouse and why is it Important?,14vmcuu,Emily-joe,1688972150.0,,False,True,Blog,0,False,0,"Data Warehouses help support business intelligence activities and streamline critical decision-making processes by making data centralized and easily accessible. 

Read more: [https://www.dasca.org/world-of-big-data/article/what-is-a-data-warehouse-and-why-is-it-important](https://www.dasca.org/world-of-big-data/article/what-is-a-data-warehouse-and-why-is-it-important)",False,0.22,https://www.reddit.com/r/dataengineering/comments/14vmcuu/what_is_a_data_warehouse_and_why_is_it_important/,dataengineering
How to build a webapp which does Data Analysis and semantic analysis on live streaming data social media API?,14uwk91,lynx1581,1688903153.0,,False,True,Help,7,False,5,"I know python quite well, but am new to data science. I wanted to create a project as the title says which analysis things like user with most hateful speeches, top trending accounts,etc . But im not quite sure how this all comes together . So far i know tools like Social media API,kafka,spark,python libraries(pandas,matplot,plotly,etc),cloud,databricks,flink,etc are involved creating this project, but im not quite sure how to start with beginning this project. like what is needed, what to learn and stuff. So i would like to know if u guys help me with making this project work and also i have alotted myself 2 weeks to learn any necessary techs for this project , so attach some resource u think is useful for me.

PS: already posted this in r/learndatascience , but someone suggested i will get better opinions/advice here.",False,0.78,https://www.reddit.com/r/dataengineering/comments/14uwk91/how_to_build_a_webapp_which_does_data_analysis/,dataengineering
Where to learn how to connect spark to various sources/destinations?,14uy1wd,LeftHelicopter5297,1688907771.0,,False,True,Help,3,False,3,"Hi guys,

I have read two books on spark - ""Learning spark"" and ""Spark: definitive guide"". Now I want to practice more - I understand transformations quite well but the bottleneck in my programming is the setting up the connections. It is very frustrating, I don't understand it at all - I would like to spend some time and money to become very comfortable with it. 

For example I would like to be able to use spark on a local machine to connect to GCP BigQuery, read the data from it, transform it and write the results to oracle DB located on another machine. Or do the same with GCP buckets, teradata etc. 

Maybe the problem is me not understanding APIs at all, I don't know how to make programs communicate with each other, so far at work everything has always been set up for me.

I would like to avoid the constant struggle of learning it on my own and searching through 8 years old stackoverflow posts - is there any resource - be it a book, a course, or maybe mentors that teach this stuff in an organized manner?

I'm a beginner in the spark world but I'm starting to think I'd like to specialize in it - I have trouble finding resources to learn from, and I don't have any seniors at my current workplace that I could learn from (and struggle to find a better position where there would be more learning opportunities).

Thanks a lot",False,1.0,https://www.reddit.com/r/dataengineering/comments/14uy1wd/where_to_learn_how_to_connect_spark_to_various/,dataengineering
Teaching sparksql and sql optimizations,14ur56c,ThenBanana,1688884397.0,,False,True,Help,1,False,5,"Hi,

I am looking for a good guide  to read Spark's EXPLAIN PLAN and Exercise pages of simple SQL optimization, with exercises and solutions. Does anyone know?",False,0.78,https://www.reddit.com/r/dataengineering/comments/14ur56c/teaching_sparksql_and_sql_optimizations/,dataengineering
How much of your current job is spent on non-engineering related tasks?,14uh0vf,DistinctEmployee100,1688854305.0,,1688860566.0,True,Discussion,16,False,14,"In your experience as a DE, how much would you say you currently split between actual engineering/coding work vs project management/people wrangling?

I joined a 7 person DE team in a  mid sized tech company (about 1.5k employees) back in February as an L5 DE and feel like I’ve been duped with regards to the role I was hired for. In the almost 6 months I’ve been here, I’ve probably pushed a collective 200 lines of code and have spent more timing doing PM/Business Analyst style work than anything else.

While I totally understand that requirements gathering and project management skills are big components of being a good/successful engineer, I feel like my job is overwhelmingly leaning in these lanes. For context, I’ve not pushed any PRs in my last two sprints and have instead spent nearly all my time coordinating meetings and processes for our Supply Chain team that will inevitably result in better data for my team like 3/4 steps down the line.

I’ve worked in both small startups and big corporations before and I’ve only experienced this in the sub 500 person orgs where you “need to wear multiple hats” where you don’t have enough people to handle all tasks. However, with my company having a dedicated PMO org, am I wrong in thinking that my role should…you know…be actually focused on data engineering? I plan to talk about this frustration with my manager during my mid year review but it just seems frustrating to me that my job split right now is like 90/10 PM/BA work to actual engineering work and not something more like 40/60.",False,0.9,https://www.reddit.com/r/dataengineering/comments/14uh0vf/how_much_of_your_current_job_is_spent_on/,dataengineering
SCD Type 2,14uswtq,planetabhi,1688890543.0,,False,True,Discussion,8,False,1,"SCD type 2 is all about managing changing dimension,but it expires record when fact changes . What is the context of dimension change here? Date dimension??",False,0.57,https://www.reddit.com/r/dataengineering/comments/14uswtq/scd_type_2/,dataengineering
Coming from Oracle on prem to Databricks - what to consider when writing SQL,14u502l,cptstoneee,1688824719.0,,1688839278.0,True,Discussion,17,False,38,"We have been using Oracle on prem and got introduced to Databricks now in our company. Is there a difference in writing the queries? I read somewhere a Group By  clause should not be used on Databricks as it is fundamentally against the concept of dividing work loads. 

I don’t know if it is true or I probably got it wrong. Is there anything at all to consider?

Would anybody have book recommendations for this question? Or a good blog to read?

Update, to be more specific:
I’m used to materialized views, cascading views, from broad data sets to specific ones for query run time optimization, and indexing

Is the materialized view the equivalent to a Delta table? Should we stick to the concept of cascading views? And what about indexing? Does this get handled automatically in the background by Databricks or do we have to set them as well? And if so, I assume indexing the same columns?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14u502l/coming_from_oracle_on_prem_to_databricks_what_to/,dataengineering
Spark architecture,14ue0vh,catchereye22,1688846938.0,,False,True,Discussion,3,False,10,"Hi DE/Spark folks,

I wanted to understand how does Spark architecture in your project work. Some production scenarios I mean.

I'm unable to relate to how Spark works given a big HDFS cluster. Does it run on one of your edge nodes? I'm sorry just a beginner and trying to understand how it is able to compute data from these multinodr HDFS systems.",False,0.92,https://www.reddit.com/r/dataengineering/comments/14ue0vh/spark_architecture/,dataengineering
"Data modelling Books ,Tools and Tutorials",14u81l5,nifty60,1688832224.0,,False,True,Help,3,False,13,"Hi Folks,

Can anyone recommend good DataModelling books (Primarily for Enterprise Data Warehouse & Data Lake) . I read Data Warehouse Toolkit by Ralph Kimball. It is really great but I just need some more books or forums where I can check the different usecases,approaches,problems and solutions .

Also can some one recommend free tools for visual diagrams of Data Models

&#x200B;

Thanks",False,0.93,https://www.reddit.com/r/dataengineering/comments/14u81l5/data_modelling_books_tools_and_tutorials/,dataengineering
Drawing the line between DE and general IT?,14u832j,KingslyLear,1688832322.0,,False,True,Discussion,8,False,6,"Hi all,

Interested to hear if anyone has dealt with blurring lines between classic DE work as part of analytics departments and general IT/ BA domains and tips on how to manage it?

As an example, I've recently managed to use webhooks to extract event data to ADLS, dumped as json and then databricks autoloader to create near real time analytics. Great! However, now Im getting questions like ""can we land that data into our CRM system too""?

Does anyone work in a team that covers both those angles or seen it done successfully before? I can see the value in moving this data directly into other systems but don't want to bite off more than I can chew so keen to get some direction from more knowledgeable and experienced people!

Cheers!",False,0.88,https://www.reddit.com/r/dataengineering/comments/14u832j/drawing_the_line_between_de_and_general_it/,dataengineering
Pipeline per table or looping pipeline?,14u3lav,Specific-Passage,1688821017.0,,False,True,Help,10,False,12,"Hey all, as we build a data platform at my company, I was tasked with the development of the silver pipeline. We currently have ~60 tables in bronze (all from same source), where ~10 are going to be joined together in silver (think Product and ProductExt tables, same id). What would be best practice for this process? One pipeline for each table and then a ""master"" pipeline that holds all the pipelines? Or one pipeline that loops through a config file, takes the columns we need and performs boilerplate transformations. We are sticking to the lakehouse approach and have parquet in bronze. We currently bring the data in bronze as a bulk load each day, so the transformations will be performed to the full table daily.",False,0.93,https://www.reddit.com/r/dataengineering/comments/14u3lav/pipeline_per_table_or_looping_pipeline/,dataengineering
"Noob question, but how do you deploy and automate a data pipeline?",14trirs,PablanoPato,1688784452.0,,False,True,Discussion,31,False,72,"I’m relatively new to data ops and have a learned a bunch out of necessity, but I really like DE and want to pursue it further. One thing that’s always baffled me though is how you actually deploy and automate a pipeline. I’ve done a few snowflake labs where I did this via python on a local machine, but what’s the next step up?

Let’s say I want to take data from Postgres in AWS RDS and send it to Snowflake. Do I write a python script and upload to EC2 then schedule it to run via a cron job?",False,0.97,https://www.reddit.com/r/dataengineering/comments/14trirs/noob_question_but_how_do_you_deploy_and_automate/,dataengineering
Finance Degree,14u7ufs,Zestyclose_Draw_7259,1688831762.0,,False,True,Career,24,False,5,"Can I be a data engineer with an Accounting and Finance degree? 

I'm looking to do a MSc in Data Science.  Would I then have a easy time finding a job? 

Thank you for the help",False,0.86,https://www.reddit.com/r/dataengineering/comments/14u7ufs/finance_degree/,dataengineering
How much do you actually use SQL or Python?,14u0dbe,miridian19,1688811518.0,,1688812571.0,True,Discussion,50,False,12,"Wondering how much you guys actually use SQL or Python? Everything (like 99%) I do is Python. 

I can count on one hand how many times I've used SQL throughout my year as a DE. All transformations are done using dataframes. Is this the norm?

[View Poll](https://www.reddit.com/poll/14u0dbe)",False,0.88,https://www.reddit.com/r/dataengineering/comments/14u0dbe/how_much_do_you_actually_use_sql_or_python/,dataengineering
Is Google BigQuery or SQL SERVER a good way to learn data engineering?,14u271y,InevitableTraining69,1688817178.0,,False,True,Discussion,18,False,8,"Looking for advice on becoming more enhanced in my skill sets. Going to do a course for free in data engineering and want to pick up some application skills. But wanted to know if I should do either of these database systems. SQL server or BigQuery? My current company uses BigQuery, and also asks for this software called ""microstrategy"" I've never even heard of",False,0.84,https://www.reddit.com/r/dataengineering/comments/14u271y/is_google_bigquery_or_sql_server_a_good_way_to/,dataengineering
Becoming a better Engineer,14ua40n,mjfnd,1688837347.0,,False,False,Blog,0,False,1,"This time I wrote a non tech blog.

Let me know which approach you follow and how it works. Also, add new ones.


Thanks",False,0.57,https://www.junaideffendi.com/blog/becoming-a-better-engineer/,dataengineering
"Ima 35 year pioneer in database technologies, just retired, and would love to help you in your careers, and maybe have a little fun at your expense. Ama.",14usm1r,fatcat623,1688889519.0,,False,False,Career,56,False,0,,False,0.23,https://www.reddit.com/gallery/14usm1r,dataengineering
Experience without access,14tpuz9,SubjectBackground220,1688779794.0,,False,True,Career,17,False,16,"How does one break into DE without access to any of the big tools to gain experience? How can I learn AWS, Hadoop, data warehousing, etc when I don’t have access to those tools as part of my job? I’ve got 20 years of data management and analytics but no access to the IT side of things.",False,0.87,https://www.reddit.com/r/dataengineering/comments/14tpuz9/experience_without_access/,dataengineering
Thoughts on the data janitor (youtube)?,14tdmv2,Nabugu,1688750868.0,,1688751188.0,True,Career,58,False,71,"I recently discovered a YouTuber called ""the data janitor"" who articulates very clearly things that I've rarely heard elsewhere when it comes to getting into data engineering. He has very strong opinions on what are the ways of getting into data engineering and machine learning engineering. I was wondering if some of you know him and if, for those of you who are in a data engineer role, if his takes make sense or not from your point of view. I know the guy’s very assertive “no BS” tone is not everyone’s cup of tea, but I would like to have a discussion on what he actually says instead of his style or the fact that he also promotes his own education platform in his videos.  
  
  
Basically the takeaways from his videos are as follows:  
  
1) Data engineer is not an entry-level role. If you don't have at least one year of experience in a data-related role (data analyst, DBA, etc), there's 99% chance you won’t be hired as a data engineer.  
  
2) A person who wants to become a data engineer shouldn't try to become that first (almost impossible), but should focus instead on a real entry level role such as data analyst.  
  
3) Data roles (DE, DA, MLE, etc) are primarily SQL heavy roles. You can't get away from SQL. Because SQL is not sexy, bootcamps want you to believe that you’ll also need a significant amount of Python (more sexy), but 90% of the time, you don’t.  
  
4) Data roles are very different from software engineering roles. A data analyst is better suited at becoming a data engineer than a DevOps or a Back-end dev.  
  
5) Certifications and certificates of completion are totally different. Certificates of completion (Coursera, Datacamp, etc) that you obtain by simply watching videos and filling blanks are worthless to recruiters. On the other hand, certifications, i.e. you have to take an exam in a physical test center or online proctored and you pass/fail the exam, can definitely have some value, but mostly if they come from the big three (Google, Microsoft, AWS) or traditional tech corporations (Oracle, Cisco, IBM, …). Some of those certifications are very hard to get and thus very respected (example: MySQL 8.0 Database Developer 1Z0-909 from Oracle). Certifications are not worth as much as actual work experience, but it’s still a non-falsifiable signal that you know a tool/framework well enough for a job.  
  
6) He thinks that without prior data experience, if you want to get into data engineering, your primary objective should be to get into a data analyst role first, and to get this role, you need two skills, Power BI and SQL. To signal those skills, two recognized certifications can help if you don’t have any professional experience with them: **PL-300 Microsoft Power BI Data Analyst**, and (since Microsoft deprecated most of its former SQL certs) **DP-300 Administering Microsoft Azure SQL Solutions**. He claims that having those two certifications on a resume can definitely get you interviews for entry-level data analyst roles if you don't have any experience in the field.  
  
  
Thoughts?  
  
For those who are/have been data engineers, do you agree with him or not? Does it depend on the field we're talking (big/legacy tech VS smaller companies maybe)? Or is it broadly true/false?  
  
What I like about him is that he seems very frank and honest about his view of the professional data world, very different from the typical too-good-to-be-true takes that you see here and there that sounds like ""don't worry anon you'll find a job in data if you send enough resumes, plenty of opportunities out there :3"", either because people want you to sign-up to their bootcamp, or just not hurt your feelings.",False,0.99,https://www.reddit.com/r/dataengineering/comments/14tdmv2/thoughts_on_the_data_janitor_youtube/,dataengineering
Azure Data Engineering (Azure Data Factory),14ub3q7,mishie30,1688839748.0,,1688841021.0,True,Discussion,2,False,0,"Hey,

I am conducting a workshop on Azure Data Factory for roles in Azure Data Engineering. It happens from 18th - 25th July. If you are interested, you can be a part of it.  
Just send me a DM.  

I am a professional and trainer and work for a leading tech company.
Price is kept at $75 USD for each participant.



It Covers complete aspects of ADF
- Creating pipeline
- Using multiple attributes of ADF to create pipeline
- Building Triggers in ADF
- Capturing logs of ADF.

It will be an extensive 7 days (14 hrs) workshop via Zoom.
Live session, with recording made available post the sessions.

First session will be free for all to join.


Thanks & Regards
Amit",False,0.33,https://www.reddit.com/r/dataengineering/comments/14ub3q7/azure_data_engineering_azure_data_factory/,dataengineering
Token generation automation in denodo,14tvqn7,Altruistic_Jelly1843,1688796822.0,,False,True,Help,0,False,2,"Hi

We are using Denodo odata datasource to connect to SharePoint list connection. Everytime while querying data from base view, the refresh token gets expired after a while and throws no oauth2 refresh token. Is there a way to automate this like to trigger bearer tokens or something so that, whenever a user runs a query from base view, it always fetch data. Please suggest how to achieve it or any alternative methods if feasible.",False,0.75,https://www.reddit.com/r/dataengineering/comments/14tvqn7/token_generation_automation_in_denodo/,dataengineering
Any one complete the Udacity data engineer course? Did you think it helped you land a DE role?,14tibt7,what_duck,1688761449.0,,False,True,Discussion,8,False,10,I'm considering taking the course to get some exposure (work paying for me). ,False,0.82,https://www.reddit.com/r/dataengineering/comments/14tibt7/any_one_complete_the_udacity_data_engineer_course/,dataengineering
Can't decide between MLE or DE as a Career,14t9gvo,lancelot_of_camelot,1688741526.0,,False,True,Career,34,False,20,"Hello DEs of Reddit, 

I am in the last year of my master's in a CS-related field (cloud computing), and I am a bit torn between choosing a career in DE or MLE. I have experience in both data science through Kaggle competitions and a few internships in small companies and 3 years of experience as a backend developer.

I don't want to pursue a career as a data scientist anymore, as I found that I enjoy coding and building real software much more than creating Jupyter notebooks, besides,  the DS job market is over-saturated. I started learning some DE skills such as Kafka and I recently got my first AWS certification. 

However, I am afraid that I would get a boring DE job, I absolutely hate doing sysadmin and DBA tasks even though I am good with SQL, I prefer building streaming systems, and data pipelines and enjoy data modelling to some extent. This fear of ending up doing boring DE tasks with old unused technologies has pushed me to consider MLE as a career track since I have some experience with ML and I find it pretty exciting being an MLE means I can build ML-centric software that is production ready, however, I don't know how hard it is to get an entry MLE job (in Europe) compared to DE. 

Thus I can't decide on which skills I should learn: continue learning DE skills (Kafka, Airflow, DBT, Data Warehousing, etc) or focus on getting MLE skills (Spark, Tensorflow, PyTorch, AWS, Kubernetes, etc). I would be grateful if you could give me some input on this topic \^\^.

&#x200B;",False,0.92,https://www.reddit.com/r/dataengineering/comments/14t9gvo/cant_decide_between_mle_or_de_as_a_career/,dataengineering
What do you use for version control of your database schema?,14tbs9q,NFeruch,1688746696.0,,False,True,Discussion,17,False,12,"A project I'm working on is finally getting large enough to the point where we need to programmatically declare the schema for each of our table, and ideally version control it

For reference, I'm interfacing with a MySQL database through sqlalchemy mostly

I've played around with alembic a little bit, and it looks pretty good so far. Are there any other tools that you guys use for version controlling schemas?",False,0.94,https://www.reddit.com/r/dataengineering/comments/14tbs9q/what_do_you_use_for_version_control_of_your/,dataengineering
Is data engineering the next data science?,14syob6,scriptosens,1688710904.0,,False,True,Discussion,97,False,71,"It very much looks like DS becomes over-saturated with passionate and (self)educated employees.  
Additionally, AI such as co-pilots and no-code environments make this domain even more competitive.

Look at the number of subreddit members r/datascience \~ 1 million, r/dataengineering \~ 10 times less

Do you think that DE will be the next DS (sexiest job...)?  
What do you think is worth to put time and effort in to get one step ahead - is this really platform engineering, distributive computing (Kubernetes) or something else?",False,0.86,https://www.reddit.com/r/dataengineering/comments/14syob6/is_data_engineering_the_next_data_science/,dataengineering
SQL Practice Platform,14tjmxd,DataNerd760,1688764425.0,,False,False,Blog,0,False,4,I wanted to share something with the community that i believe will be helpful for people here. An issue I have seen many have and have seen mentioned in this forum is not having a database to practice on before landing your first analyst position at a company. I create this website as a place where SQL writers can practice their skills with challenge questions or free query on real or generated datamarts. This is a release 0.1 so there will definitely be some improvements to make but i want to see if the community agrees this solves an issue and sees value in it. Please check it out.,False,0.83,http://campsql.com,dataengineering
What could go wrong with docker containers?,14t92fo,conlake,1688740561.0,,False,True,Discussion,21,False,10,"I understand that Docker containers eliminate the issue of 'it works on my computer,' but I have encountered a problem while developing an application using Docker Compose, where memory disk and RAM are utilized on the host computer (which is outside the Docker containerization).

Consequently, if I run this Docker Compose on a computer with low RAM or limited memory disk, it will result in an error, which defeats the purpose of Docker containerization. So, I have a few questions:

1. What are the best practices for managing memory disk and RAM requirements for Docker containers?


2. Are there any other potential issues that I should be aware of or able to configure when running the same Docker container on a different computer? This particular issue caught me off guard, and while it makes sense to me, I want to be prepared for any other potential challenges that may arise in the future",False,0.71,https://www.reddit.com/r/dataengineering/comments/14t92fo/what_could_go_wrong_with_docker_containers/,dataengineering
What is keeping companies on hadoop + HIVE instead of spark?,14tebqr,yonz-,1688752493.0,,False,True,Discussion,4,False,4,"Been learning about pipelines and it seems like a no-brainer, am I missing something?",False,0.7,https://www.reddit.com/r/dataengineering/comments/14tebqr/what_is_keeping_companies_on_hadoop_hive_instead/,dataengineering
Airflow or something else for orchestrating non-data-centric pipelines?,14t7jpx,Wallaby99,1688736967.0,,False,True,Help,12,False,11,"Hi all, I'm looking to see if Airflow is the best solution for what I need or if there is some other package that might be better suited.

Quick background: My work involves processing video data. First ingesting it, transcoding it, running it through several AI models (computer vision, audio processing, etc) to find the most interesting moments, and then making 10-30sec clips of those moments. It's not strictly a 100% data-centered task: the ingestion part involves downloading from remote devices and normalizing the video, the AI part involves several stages of running models but also includes some compute-heavy analysis of usually 4-500k db records per stream, and the final output stage is almost purely using ffmpeg to cut clips from the original stream.

Nevertheless, it is a pipeline, with about 10 stages, and I thought Airflow would be a good way of managing it. The idea of defining a DAG for each type of stream, defining which steps need to occur (some in sequence, some concurrent, etc), dispatching tasks as they come up, and monitoring the pipeline is basically what I'm looking for, and Airflow seems to be very good for this.

But there are several concerns I have about Airflow and I'm wondering if these are easily addressed, or if there might be a different orchestrator that would be better.

1. It seems Airflow is really designed to be cron on steroids. That is, it's designed for static jobs that are run on a set schedule. But my pipeline doesn't run on a schedule. It's triggered each time a new video is available. So sometimes there could be no jobs, and sometimes there could be a hundred videos that need to be processed. Does Airflow do well with these sorts of unscheduled jobs and would I run into any scalability issues if, at peak times, it needs to orchestrate several thousand videos all in various stages of the pipeline?
2. Regarding static jobs, how static do jobs or DAGs need to be? While for probably 90% of our videos, the DAG would be fairly static (would need to pass a few parameters to specify which video to process), some videos require more dynamic DAGs, where based on a preliminary analysis of the video, a DAG would be created for that specific video. Does Airflow support truly dynamic, one-time DAGs, where we could essentially write a custom DAG for each video at the first step of processing and then have Airflow manage it from there (and then cleanup the DAG once the video is done)? Like I mentioned, we don't need this sort of fully dynamic feature right now, but it would be nice to have for future needs.
3. How flexible is error and failure mode handling? While half of our errors can be fixed by simply restarting the step, the other half require looking at the actual error and re-routing to a different step or DAG entirely. For example, if a specific AI model finds no detections, the video might need to be sent to an entirely different pipeline with a different set of models / tasks, and then rejoin at the output phase to create the final clips.

So those are my main concerns. I guess the tl;dr version is, how flexible / dynamic are Airflow DAGs in practice? Can we write them to make decisions mid-pipeline about which path to go down (i.e. due to an error, or other conditions)? At the extreme, is it easy to write a completely custom DAG for each job? Or is there a better orchestrator to do this?

Thanks for any insight you can share!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14t7jpx/airflow_or_something_else_for_orchestrating/,dataengineering
Ballista (Rust) vs Apache Spark. A Tale of Woe.,14tdo5y,DarkClear3881,1688750955.0,,False,False,Blog,8,False,5,,False,0.73,https://www.confessionsofadataguy.com/ballista-vs-apache-spark-rust-vs-scala/,dataengineering
CI/CD for Databricks,14ta1b3,JKMikkelsen,1688742764.0,,False,True,Discussion,25,False,6,"What is the preferred method of deploying Data Lakehouses in Databricks?

I’m used to the SQL Server world with database projects in SSDT with table, views and stored procedures (etc.) and use state-based deployment of the SQL objects.",False,0.88,https://www.reddit.com/r/dataengineering/comments/14ta1b3/cicd_for_databricks/,dataengineering
optimization with parquet storage format,14tnrkf,ravicric,1688774251.0,,False,True,Discussion,0,False,1,"If you want to optimize/improve your spark code performance, then must watch this presentation from databricks. 

 [The Apache Spark File Format Ecosystem - YouTube](https://www.youtube.com/watch?v=auNAzC3AU18&t=1189s) ",False,0.6,https://www.reddit.com/r/dataengineering/comments/14tnrkf/optimization_with_parquet_storage_format/,dataengineering
ByteDance Open Sources Its Cloud Native Data Warehouse ByConity | ByConity,14t5flu,Junior-Salary-6859,1688731567.0,,False,False,Open Source,0,False,6,,False,0.81,https://byconity.github.io/blog/2023-05-24-byconity-announcement-opensources-its-cloudnative-data-warehouse,dataengineering
Data Engineering Zoomcamp Timeliness - Total Hours required,14tby7e,Amanlikeyou,1688747075.0,,False,True,Career,3,False,3,"Anyone complete the Data Engineering Zoomcamp while working? 

I want to switch jobs in 2 months max. How many hours would you say are required to complete all the material and the project? 

I understand the certificate is not available until the next cohort.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14tby7e/data_engineering_zoomcamp_timeliness_total_hours/,dataengineering
Data Eng on Twitter vs. Threads?,14teai2,grahamdietz,1688752410.0,,False,True,Discussion,1,False,2,"I am fairly new to the domain and to this here Reddit community. I am trying to ramp up quickly coming from a more app-development background. I am not on Threads yet, and honestly, I need a new app/channel like a hole in the head. Do people here recommend getting into Threads specifically to follow DE topics? I am told that it is less vendor-infected than LinkedIn and Twitter... but it's early days?  
Also, thanks to the community here - Reddit has been great to gain perspective and discover new resources!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14teai2/data_eng_on_twitter_vs_threads/,dataengineering
What next after AWS certifications?,14t6bjc,Usurper__,1688733919.0,,False,True,Discussion,16,False,6,"Hi,  
I'm a DE with few years of experience. I just finished getting all the AWS associate certifications and I was wondering what I should be focusing on next. 

SQL: I know some SQL, but I'm not an expert. I was thinking getting leetcode and grinding the easy/mediums there.

Cloud: Maybe do Azure certifications or start studying for AWS Data Analytics speciality or SA PRO

Projects: Complete DataTalkClub Data Engineering zoomcamp. I hear good things about this.  


Maybe something completely different?  
My goal is to advance my skills to acquire more money in the job market.

&#x200B;",False,0.87,https://www.reddit.com/r/dataengineering/comments/14t6bjc/what_next_after_aws_certifications/,dataengineering
Will data engineering split as a profession?,14thvca,Thinker_Assignment,1688760394.0,,False,True,Discussion,6,False,0,"It feels like the field of data engineering is evolving - slowly the people doing end to end are disappearing, and being replaced by platform engineers, engineering analysts, etc.

it feels like ml pipeline engineers are another breed as well.

What is your experience around this? what roles do you see data engineering splitting into?",False,0.44,https://www.reddit.com/r/dataengineering/comments/14thvca/will_data_engineering_split_as_a_profession/,dataengineering
Breaking into data engineering from finance erp configuration,14tgozi,averagenetizen,1688757779.0,,False,True,Career,0,False,2,"Would recruiters consider me if i come from an application support role where i assist with setting up finance systems (i model them, but within the software). I use SQL quite a lot but all our clients have a few users and so it is not advanced SQL but i still possess that knowledge.

I also do migrations from older versions of the software i consult to the newer version, whilst its mostly done via a tool in our application i do have to fix a lot of data related errors since the older version does not have as many constraints.

I have used python in instances, like replacing data in a file with that in an excel.

Would all this be considered when applying for data engineering roles?",False,0.75,https://www.reddit.com/r/dataengineering/comments/14tgozi/breaking_into_data_engineering_from_finance_erp/,dataengineering
File pattern matching Databricks,14t786q,sugarbuzzlightyear,1688736164.0,,False,True,Help,4,False,3,"Hi!  


We're using Delta Live Tables with Auto Loader in a project. The data is stored in ADLS, but within a specific folder there are several different file types:  


weatherFile\_321.json  
weatherFile\_123.json  
weatherFile\_132.json   
locationFile\_231.json  
locationFile\_231.json  
temperatureFile\_212.json  
temperatureFile\_122.json  
humidityFile\_983.json  
humidityFile\_839.json  
...  
etc.  


My path is defined as follows:

json\_path = ""abfss://<storage-container>@<storage-account>.dfs.core.windows.net/weatherLandingZone/raw/2023/7/6/""  


Now, is there a way to use file pattern matching in Databricks using wildcards so that I would be able to ingest only certain files? Something like this:

""abfss://<storage-container>@<storage-account>.dfs.core.windows.net/weatherLandingZone/raw/2023/7/6/**weatherFile\_\***""  


Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14t786q/file_pattern_matching_databricks/,dataengineering
Results from my poll ~90 days ago,14t9t55,keeney_arcadia,1688742267.0,,False,False,Discussion,1,False,2,,False,0.75,https://i.redd.it/8e88l0fs5kab1.jpg,dataengineering
The Open Data Lakehouse: Towards Democratized Data Analytics,14tf7ll,Embarrassed-Cod-2756,1688754508.0,,False,False,Blog,0,False,1,,False,1.0,https://celerdata.com/blog/open-data-lakehouse-towards-democratized-data-analytics,dataengineering
New BI Engineer. Should I pursue an MSCS?,14t87us,Linkfan92,1688738585.0,,False,True,Career,1,False,2,"I previously worked as a Senior Operations Analyst and just completed my MBA. My work used to involve heavy analytics and report building in PowerBI. 

Most of my reports required me to build out complicated datasets that I'd then hand over to our architect team to implement.  I realized that building out the data was my favorite part of my position. 

Due to my proximity with our architect team and them being happy with the work I provided them, I was offered a position as a BI Architect/Engineer which I accepted.

My new role involves zero data analytics or report creation. Instead, 100% of my time is spent designing and creating data pipelines used by our analysts and data scientists.

We're very heavy in SSMS but are switching to AWS and redshift.

I'm currently enrolled in an MS in data analytics (very heavy in machine learning)... However, I feel with my new role and career track it may be worth it to pursue a MSCS since my undergrad was a Fine Arts degree.  Alternatively, I could finish my MSDA and pursue certificates.

My company provides 5k a year in school reimbursement and I personally love being a student. So cost/time are not huge in my decision making process. Currently working remote making 95k + 10% bonus in the midwest.

I'm looking for what will give me the most job security in the future and allow me to expand my DE skills.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14t87us/new_bi_engineer_should_i_pursue_an_mscs/,dataengineering
Database Question,14syany,space-trader-92,1688709736.0,,False,True,Help,13,False,8,"Who sets up and manages the database connected to an application? Is it the software engineer, data engineer, database administrator? 

Don’t shoot me for the basic question :)",False,0.9,https://www.reddit.com/r/dataengineering/comments/14syany/database_question/,dataengineering
Ibis: The last dataframe API you'll need to learn? I hope...,14scpwd,bitsondatadev,1688657666.0,,False,False,Meme,50,False,81,,False,0.83,https://i.redd.it/om8sn7dv4dab1.jpg,dataengineering
Do you think it's valuable to maintain a personal portfolio even though you have a job?,14sh6f0,gintokiredditbr,1688667193.0,,False,True,Career,27,False,45,"











Hi guys, in my current job I only have the possibility to use excel, but I think of learning sql, python and others as a way to have better jobs. The point is that for that I would need a daily practice, which I could only achieve with personal projects. Do you think this is a valid way to keep the knowledge with me and show that I master the tool? or would a recruiter/employer not care too much about this? Honestly, I'm a little discouraged about learning only through courses and not having the opportunity to put anything into a project, I think that way I would understand better. Anyway, thanks for advice.",False,0.96,https://www.reddit.com/r/dataengineering/comments/14sh6f0/do_you_think_its_valuable_to_maintain_a_personal/,dataengineering
First End-to-End Data Engineering Project: Formula 2 Data Pipeline for for Automated Updates of a Kaggle's dataset.,14sjl7h,NationOfSheeps,1688672647.0,,False,True,Personal Project Showcase,8,False,19," I recently completed my first end-to-end data engineering project, aimed at strengthening my data engineering skills. The project is not complicated, but since I am used to only analyzing and modeling perfectly curated data (I am a data scientist) without getting involved in pipelines either putting models into production, I think it is a good start. 

Architecture:  


[ELT Architecture](https://preview.redd.it/ncxt2ysldeab1.png?width=1280&format=png&auto=webp&s=7354144cbbf90d326b44283d8a9661e1c6ddb23e)

 The project focuses on processing data from Formula 2 races.  The pipeline consists of several stages, including data retrieval, data cleaning, and storage in an S3 bucket. Additionally, the processed data is formatted to update the existing dataset hosted on Kaggle. The entire process is automated, leveraging the FIA calendar and race\_id (despite their unorganized nature). Definitely not the best architecture, but the goal was to test multiple AWS services

GitHub Repository: [here](https://github.com/Alarchemn/F2-Data-Pipeline)

Kaggle dataset: [here](https://www.kaggle.com/datasets/alarchemn/formula-2-dataset)

I come from academia, PhD in nonlinear control engineering full of hard math. Migrating to real, executable projects is truly exciting.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14sjl7h/first_endtoend_data_engineering_project_formula_2/,dataengineering
How to Install Hadoop in Windows 10 & 11 | Data Engineering Tutorials,14t3jw5,Few_Spirit_7451,1688726350.0,,False,False,Help,9,False,1,,False,0.54,https://youtu.be/knAS0w-jiUk,dataengineering
Using Orchestration Tool to refresh BI tool,14sp81z,Culpgrant21,1688685190.0,,False,True,Discussion,17,False,7,"Has anyone used their orchestration tool to refresh the BI dashboards that are pulling in those tables? This would be for only BI tools that are caching data locally. 

The DAG would be source -> transformations -> ping BI tool API to start refresh. 

This is something I have gone back and forth in my mind on. I have never seen any team put it into production but I could see some benefits. 

Let me know your thought!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14sp81z/using_orchestration_tool_to_refresh_bi_tool/,dataengineering
Terraform Certification,14sot3r,va1kyrja-kara,1688684268.0,,False,True,Career,11,False,7,"Is it worth pursuing the Terraform certification in data engineering? My role is DevOps focussed, I really like the IaC component, and enjoy working with Terraform. Is it a step in the wrong direction?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14sot3r/terraform_certification/,dataengineering
Which would be better certification values wise,14t04i2,omkargurme616,1688715524.0,,False,True,Help,4,False,1,"I have the chance to take only one certification course , Either i can go for databricks data engineer associate certification or databricks spark associate developer certificate 

which would add more value to my resume and which is better , do guide me on the matter ",False,0.67,https://www.reddit.com/r/dataengineering/comments/14t04i2/which_would_be_better_certification_values_wise/,dataengineering
Is cloud a big scam?,14rt3ur,tarzanboy76,1688604986.0,,False,True,Discussion,125,False,194,"I'm working in an organisation that is going through a process to migrate everything to a consolidated cloud platform (Azure).  Our BI stack moving forward is Power BI, ADF / Synapse, Databricks and Azure Datalake.  I'm not an infrastructure guy, but I've been working with out infrastructure team and some consultants in getting our platform setup and secured.

Firstly, my overwhelming experience over the past few weeks has been that setting something up in the cloud is super simple... but setting something up in the cloud *and ensuring it meets basic security requirements* is considerably more complicated.  Really, *really* complicated.  Unsurprisingly, it's a specific skill.

Secondly, the big pitch for cloud services is that you ""only pay for what you use"".  The problem is that *everything* costs and it often costs a lot.  And while cloud providers give you some options to manage costs, a lot of this is lip service.  For example, we have VM's setup as a gateway to our on-premise systems.  These only need to be switched on while ETL jobs are running.  You'd think that ADF or Synapse pipelines would have a basic connector for managing Azure VMs.... but, nope.  You can do this programatically using Python or Powershell... but those need to be running somewhere, like Azure Function which, you guessed it, costs more money.  Not to mention the time and effort to develop something.  

This leads me to me third issue...  scalability.  Cloud services pitch nearly infinite scalability, but they are talking about scaling *up*.  Scaling downwards?  Not so much.  Want to run a Python notebook in Synapse?  The minimum cluster uses nodes with 4 vCores and 32GB memory, and you must have *at least three nodes*.  My 'hello world' script (metaphorically speaking) doesn't need 12 cores and 96GB memory.

Don't get me wrong... I get it... cloud services open up all sorts of options for large businesses.  But how many orgs have been massively oversold on cloud?  How many thought it would reduce headcount in their infrastructure teams, when these teams were already running on bare bones in the first place?  And how many thought cloud services would give them levels of scalability that they'll never actually need?",False,0.93,https://www.reddit.com/r/dataengineering/comments/14rt3ur/is_cloud_a_big_scam/,dataengineering
Bioinformatician of 5 years looking to pivot to data engineering and would like some advice on how to enter the field,14sfwgc,buffbuf,1688664408.0,,False,True,Career,10,False,7,"Hi everyone. I've been an academic bioinformatician for 5 years with only a BS working in different advanced academic roles. I want to leave academia for good and parlay my skills into data engineering. I've noticed that where I've excelled in bioinformatics has been setting up well-documented, reproducible pipelines that preprocess and even sometimes analyze large datasets, and that's the kind of work I enjoy the most; I am best in the role of a technician of sorts. The issue is, I am obviously not immediately suited for this type of work, because while I am skilled in R, Python, HPC environments, and specialized genetic analyses, I have not had to work with SQL pretty much ever and don't have many of the common skills in the average data engineer job listing.

I got into the field I'm in by taking an internship at the university where I was getting a master's degree and then dropping out of my master's degree once I got a firm foot in the door (plus the master's degree felt like a scam). I would like to avoid getting a master's degree because I have a strong feeling that most of them are scams, though that is a filter for many recruiters. Is there like an internship or apprenticeship equivalent for data engineering? I am willing to take a temporary pay cut to work in this field and learn as much as I can. I don't need to be a biotech data engineer but that would help since they would most appreciate my background experience.

Also I am just looking for any and all advice on how to break in.

Thank you!",False,0.89,https://www.reddit.com/r/dataengineering/comments/14sfwgc/bioinformatician_of_5_years_looking_to_pivot_to/,dataengineering
No entry level jobs for data engineer or machine learning engineer??,14s0d0h,AnishNehete,1688624978.0,,False,True,Help,55,False,35,"Job requirements are ridiculous, On LinkedIn Data Engineer roles require 3+ years of experience and Machine learning Roles require around 7+ years of experience. I was preparing for a data engineer role but perhaps there's no point in doing that anymore, Maybe I should start focusing on being a data analyst and then switch jobs?? What advice would you guys give to a guy in college interested in the field of data??",False,0.84,https://www.reddit.com/r/dataengineering/comments/14s0d0h/no_entry_level_jobs_for_data_engineer_or_machine/,dataengineering
Front-End for Azure Data Factory,14sgjs7,TheFirstGlassPilot,1688665828.0,,False,True,Discussion,4,False,3,"I'm keen to canvas wider opinion so any input is appreciated. 

I have a data factory that accepts 5 parameters and then generates Excel extracts. What would you view as the best option to make input of these parameters and self-service execution of the pipeline available to end-users of the extracts? 

Thanks in advance.",False,0.81,https://www.reddit.com/r/dataengineering/comments/14sgjs7/frontend_for_azure_data_factory/,dataengineering
Any self-taught data engineers here who feel like there are a lot of people who are focused on discrediting you?,14rqj5o,Capable-Jicama2155,1688598315.0,,False,True,Discussion,51,False,84,"I get the sense during interviews that some people are more focused on trying to figure out anything I don't know, rather than talking about skills that actually pertain to the role. 

A lot of times, people with traditional computer science backgrounds talk to me in interviews like I'm an ""exhibit"". Getting comments like, ""Oh, I've always wanted to meet a self-taught person"", or ""You've done well so far, for a self-taught engineer."" 

Then proceed to ask me questions about something oddly specific like linting, or data warehouse processing algorithms. 

I feel like I have to be ""beyond perfect"" in a lot of situations, and even then people will still waste my time, and go with someone with a more traditional computer science background. 

It's really exhausting, and is making me consider doing a post-bach or masters, just so I don't have to deal with this anymore. 

Context - I'm a senior data engineer who leads a team with a lot of members that have under-graduate or masters degrees in comp sci or data science. ",False,0.89,https://www.reddit.com/r/dataengineering/comments/14rqj5o/any_selftaught_data_engineers_here_who_feel_like/,dataengineering
Data Engineering getting closed source?,14ryr4w,lezzgooooo,1688620266.0,,False,True,Discussion,37,False,27,"I got into data engineering since there are a lot of open source tools to play around with.

Now, recruiters are looking into experience with Databricks instead of Spark.

Snowflake instead of Postgres and MySQL.

You also need certifications on AWS, Azure or GCP which despite having free credits, at some point will require you to pay with your credit card to use any of the services.

I know that there are still lot of open source pipelines and free tools but the direction for most groups seem to be closed source and pay to use.",False,0.97,https://www.reddit.com/r/dataengineering/comments/14ryr4w/data_engineering_getting_closed_source/,dataengineering
Working with Medicare CCLF Data - Seeking Insights and Advice,14sh2zb,Uchiha_pandu,1688666976.0,,False,True,Help,2,False,2,"I'm currently undertaking a summer internship at a healthcare analytics company where I've been tasked with expanding our reporting capabilities to include Medicare data. While we're well-versed in Medicaid reporting, this is something new that the company is trying and I have been assigned on this project to figure out if there is a way to create reports using the Medicare data. I'm reaching out to the community for any insights or advice regarding working with Medicare CCLF (Claim and Claim Line Feed) data. I am the only intern as it is a small company and I am working alone on this project!

I am struggling in identifying the attribution field within the data. Since we provide separate reports to each clinic, it's crucial  to differentiate the data based on clinics. Unfortunately, I haven't been able to locate this field in the Claims files yet. However, I have figured out how to calculate Part A and Part B claims from the data.

Currently, I'm planning to initiate the ETL (Extract, Transform, Load) process by loading the fixed width files into SQL Server. My approach involves creating tables and utilizing Bulk Insert. I would appreciate any feedback or suggestions if anyone has experience working with Medicare data or any recommendations on how to proceed.

Thank you in advance for your help and valuable insights!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14sh2zb/working_with_medicare_cclf_data_seeking_insights/,dataengineering
Databricks vs Redshift,14s2msk,the_travelo_,1688632065.0,,False,True,Discussion,23,False,9,"What are the fundamental differences between Data Warehousing workloads between Databricks and Redshift?

Assuming cost is not a factor, what features are present in one that are not in another?

Databricks announced Materialized views which is something that Redshift has had for a while.

Redshift has Dynamic Data Masking which I believe Databricks doesn't have the equivalent

Both seem to have the merge command available.

Redshift can be used natively with Step Functions for orchestration which is somewhat the equivalent of DB workflows

Databricks has built in quality rules with DLT and I'm not aware of anything similar in Redshift 

I get the huge difference in approaches but I'm after more low level features that are available in one and not in the other that make the life of a Data Warehouse engineer easier?

Thanks! 

Note: this is not a which one is better discussion but more so a healthy comparison between both",False,0.81,https://www.reddit.com/r/dataengineering/comments/14s2msk/databricks_vs_redshift/,dataengineering
7/24 ETL Job Monitoring,14sjfg6,rayman903,1688672255.0,,False,True,Career,24,False,0,My company wants me and my friends not to sleep on a regular basis and monitor ETL jobs all the time. Is this normal for companies or I am in a dead-end position?,False,0.5,https://www.reddit.com/r/dataengineering/comments/14sjfg6/724_etl_job_monitoring/,dataengineering
Who chooses what tools you can use?,14sg25w,Kindly_Job_5126,1688664773.0,,False,True,Discussion,0,False,1,"Question for engineers and data scientists doing work that involves creating AI solutions, especially generative AI for text.

**1) Who at your company is the key purchasing decision maker on what tooling you can use to do your work?**  
*For example: Vice President of Engineering for the AI pillar*

**2) How big is your company (roughly how many employees)?**

**3) What is your biggest technical barrier with fine-tuning or testing your models?** 

*For example: Gathering data examples for fine tuning*

  
That's all! Thanks in advance!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14sg25w/who_chooses_what_tools_you_can_use/,dataengineering
DE new on the job,14sf3co,DarthDatar-4058,1688662659.0,,False,True,Help,3,False,0,"Hi guys,

Just started working as a DE at a startup company. I was wondering, how long did it take for you guys get familiar with the data model of the company you're working for? 

I'm pretty proficient in SQL and Python, but I feel like I'm unable to help with the more complex tasks because I'm not too familiar with the data model yet. I often use the information_schema tables to find the tables I'm looking for. Should I be able to memorize the tables at some point? 

Any tips on how I can speed up the process of getting the know the data structures?",False,0.5,https://www.reddit.com/r/dataengineering/comments/14sf3co/de_new_on_the_job/,dataengineering
How to deploy data checks at scale?,14s8te8,JLTDE,1688649077.0,,False,True,Discussion,2,False,2,"Hello. We have our data in Bigquery and the data-checks are going to be a core part of our data platform. Moreover, we would need to scale it to hundreds of clients and run daily checks as well as periodic checks. 

I investigated for a while and I decided to go with elementary ([https://github.com/elementary-data/elementary/tree/master](https://github.com/elementary-data/elementary/tree/master)), since we use dbt and it has kind of a nice UI. However, we've been running into ""query too complex"" errors in bigquery since a very early stage of our system, so I don't think it will scale very good. We discarded great expectations for the cumbersomeness of it and because we didn't really like it.

Now, we are thinking on implementing our own solution, since we have a more narrow use case than great expectations or elementary could have, so we wouldn't need that much features. However, I am not sure what would be the best tech to use.

Something like elementary for data in bigquery?  
Pandera using pandas or pyspark dataframes?  
Use polars and create our own queries?  
Our own SparkSQL code in a cluster?

I hear you!",False,0.76,https://www.reddit.com/r/dataengineering/comments/14s8te8/how_to_deploy_data_checks_at_scale/,dataengineering
How to Create a SQL Query Using AI in Less Time?,14spz88,chrisgarzon19,1688687014.0,,False,True,Blog,1,False,0,[https://dataengineeracademy.com/blog/how-to-create-a-sql-query-using-ai-in-less-time/](https://dataengineeracademy.com/blog/how-to-create-a-sql-query-using-ai-in-less-time/),False,0.3,https://www.reddit.com/r/dataengineering/comments/14spz88/how_to_create_a_sql_query_using_ai_in_less_time/,dataengineering
Data Engineer vs Backend developer role ?,14sdqfc,R_A_D_E_O_N,1688659805.0,,False,True,Discussion,12,False,1," 

Hello developers,

PLEASE HELP ME OUT AS I AM IN A DILEMMA AND UNABLE TO MAKE A DECISION

Background: B.Tech CSE, 2 years of experience as Data Engineer in WITCH. Tech stack (AZURE,  
DATABRICKS, PYSPARK, SQL, PYTHON). I am good at creating batch pipelines and data  
transformation. I have also solved 250+ leetcode questions and loves DSA.

I am looking for switch into product based companies, I am unable to decide should I prepare for SDE roles or for Data engineer roles  
There is very little information regarding Data engineering job opportunities in youtube and other sites.  
Somehow i feel like data engineering is 2nd rated compared to SDE roles.  
There are so many software developers in FAANG but very very few data engineers, why is that?  
All I see data engineering job opportunities in WITCH companies?

Should I study DSA and system design and aim for SDE 2, SDE 1 role or stay in Data engineering lane?  
Any advice is highly appreciated.

Thank you",False,0.57,https://www.reddit.com/r/dataengineering/comments/14sdqfc/data_engineer_vs_backend_developer_role/,dataengineering
Start Your Stream Processing Journey With Just 4 Lines of Code,14rr4j7,yingjunwu,1688599769.0,,False,False,Blog,12,False,20,,False,1.0,https://medium.com/better-programming/start-your-stream-processing-journey-with-just-4-lines-of-code-5863573268b9,dataengineering
Is a CI/CD pipeline difficult to set up in your experience?,14rmmkp,what_duck,1688589545.0,,1688589964.0,True,Discussion,26,False,21,"My organization is trying to modernize and have put an unfortunate soul (me) in charge of creating a CI/CD pipeline for our products. Our goal is to use Jenkins to communicate via Webhooks from our DevOps platform, but there's been a number of roadblocks given our IT infrastructure. I've also had some trouble communicating *why* CI/CD is important.

I'm curious what others have experienced in setting up a CI/CD platform.",False,0.94,https://www.reddit.com/r/dataengineering/comments/14rmmkp/is_a_cicd_pipeline_difficult_to_set_up_in_your/,dataengineering
Upgrading from hive_metastore (Managed Tables) to Unity Catalog,14rwj7h,oneDatumPlease,1688614050.0,,False,True,Help,4,False,7,"Hi all,

My team is in the process of building out an Azure data lake using delta lake tables.  When we started the development, we utilized hive\_metastore instead of unity catalog; but some unity features are now catching our eye.

There's some documentation out there regarding the upgrade process, but all of these upgrade scenarios only work on **external** tables managed by hive\_metastore.  Unfortunately, we are currently using **managed** tables within hive\_metastore. Documentation/blogs for managed table upgrades seems sparse.

[This MS article](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate#--upgrade-a-table-to-unity-catalog) talks about the external upgrade process using the wizard, but only briefly talks about the upgrade process for managed tables at the bottom (basically just running CTAS statements).

Has anyone gone through this upgrade process?  Specifically from managed tables to Unity?

Thank you very much for any info/stories/lessons learned!

&#x200B;",False,0.9,https://www.reddit.com/r/dataengineering/comments/14rwj7h/upgrading_from_hive_metastore_managed_tables_to/,dataengineering
What is the need of CI build?,14s3b1w,akhilseban,1688634119.0,,False,True,Help,4,False,2,"I am exploring azure Devops for ARM deployment as part of infrastructure provisioning.
What is the need for a CI build. Why can't I directly release the code changes?",False,0.76,https://www.reddit.com/r/dataengineering/comments/14s3b1w/what_is_the_need_of_ci_build/,dataengineering
"How to optimize data modeling to reduce data scan for many-to-many ""partition/clustering"" fields?",14s6clc,yfeltz,1688642869.0,,1688643509.0,True,Discussion,2,False,1,"*Disclaimer: I'm now asking for a project on BigQuery, so specific answers for BigQuery will help a lot in the short run. But, I would love to understand it better conceptually, so I'll be able to apply it to other systems as well.*   


Usually, when optimizing a ""big data"" table we would partition and/or cluster it.   
Both allow one-to-many hierarchically ""indexing"" more than one field.   
The classic partition example would be time:  
```
year/month/day/hour  
``` 
Same for clustering, if we cluster by `col_a` and then by `col_b`, it means a one-to-many between `col_a` to `col_b`, for example:  
```
  col_a   col_b  
 ------- ------- 
  a1      b1     
  a1      b2     
  a2      b3     
  a2      b4     

```
  
How about a case where we have?

```
  col_a   col_b  
 ------- ------- 
  a1      b1     
  a1      b2     
  a2      b1     
  a2      b2     
```

In this case, the relationship is many to many, and common query pattern predicates would use `col_a`, `col_b`, or both.  


How would we optimize such a table to reduce data scans?  


Thank you. :)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14s6clc/how_to_optimize_data_modeling_to_reduce_data_scan/,dataengineering
I attempted to create the Ultimate Guide to dbt,14r4v10,tbrownlow,1688547009.0,,False,True,Blog,8,False,95,"When I was first learning dbt I found a lot of really great resources (dbt docs, blog posts, slack threads etc.) and wanted to try to put that all together in one place for anyone else new to dbt, or anyone wanting to learn a bit more about it.   
The guide covers everything from ‘what is dbt?’ to advanced topics like model refactoring best practices.  
I hope it's useful! **And if you spot anything missing, or ways to make it better, please let me know!**  


Desktop link: [https://count.co/canvas/JpkaYdqr9oN](https://count.co/canvas/JpkaYdqr9oN)  
Mobile link: [https://taylor-count.medium.com/the-ultimate-guide-to-dbt-bad192ab4914](https://taylor-count.medium.com/the-ultimate-guide-to-dbt-bad192ab4914)  


Full disclosure: I do work for [count.co](https://count.co), the canvas in which the guide was built. ",False,0.98,https://www.reddit.com/r/dataengineering/comments/14r4v10/i_attempted_to_create_the_ultimate_guide_to_dbt/,dataengineering
Need help in DBT,14ryzco,Suspicious_Goose_659,1688620929.0,,1689758593.0,True,Help,3,False,2,"FIXED. I used incremental function

Hello, need help with this problem. This is already a functioning code, however, it replaces all the active\_count it has inputted to null whenever there's a new active\_count to input. 

I think the problem is in my case method, is there any way I could ignore active\_count whenever generated\_date is not equal to current date?

&#x200B;

    with 
    cte_date_generator as (
    
        select -1 + row_number() over(order by 0) index, start_date + index generated_date 
        from (select '2023-07-01'::date start_date, current_date()::date end_date)
        join table(generator(rowcount => 20000 )) x
        qualify index < 1 + end_date - start_date
        order by generated_date desc
    
    )
    
    select 
        generated_date,
        case 
        when generated_date = CAST(current_timestamp as date) 
            then (select count(*) from my_table) 
            end as active_count
    from cte_date_generator",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ryzco/need_help_in_dbt/,dataengineering
"A spark question, job which is stuck for a long time",14rw00q,BleepBloop736,1688612617.0,,1688619957.0,True,Help,6,False,3,"My spark script reads json data and writes data in parquet. But the write has to be partitioned by a field and it would generate probably 10k directories/partitions. As it is huge in number job is not progressing, it is stuck for a long time. 

What are the things I can try to fix this issue?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rw00q/a_spark_question_job_which_is_stuck_for_a_long/,dataengineering
Every Major Announcement at Snowflake Summit 2023 and 1 Word Never Mentioned,14roylf,ian-whitestone,1688594696.0,,False,False,Blog,5,False,6,,False,0.88,https://select.dev/posts/summit-2023,dataengineering
Do data Engineers get paid Similar to software developers?,14res9p,micky_357000,1688572706.0,,False,True,Career,29,False,21,"I have recently got a job after completing my Bachelors, as a trainee/junior data engineer in a Fintech , During my college times everyone was talking about SDE/backend/frontend/mobile-dev jobs and I have idea about them that a good developer can earn good with few years of experience.But Data engineering was unheard of and I knew a few things about data science and understood that these 2 are different .

Can you help me by answering these 4 questions in short?
(even yes /no answers will be appreciated)

1.)Since the demand of Software developers is high and they are in demand everywhere, do data engineers have similar kind of demand in big MNCs and faang type companies.Since Data is increasing day by day and every co. I think would use them so this job profile should be hot but I haven't heard about this job till I got my current job offer.

2.)I have explored frontend and backend web dev , and somewhat liked backend web dev using node js, and while practicing DSA and writing backend I started a liking towards programming, so do data engineers get to code enough ?

3.) I got a basic intro to SQL in my college and then I didn't got much interested in it , so do you guys already have a interest in SQL when you choose Data engineer at first or you started enjoying it when you started working with it?

4.)Can you share that salaries of software developer in your company vs your salary as a data engineer with same year of experience.Is there a huge difference or is it somewhat similar.",False,0.96,https://www.reddit.com/r/dataengineering/comments/14res9p/do_data_engineers_get_paid_similar_to_software/,dataengineering
Iceberg won the table format war: But not in the way you thought it might,14rcaj9,bitsondatadev,1688567397.0,,False,False,Blog,29,False,25,,False,0.77,https://bitsondatadev.substack.com/p/iceberg-won-the-table-format-war,dataengineering
Is it common in the US to ask for a handwritten cover letter in DE(or at all)?,14rcq73,IfThisThenWhat,1688568386.0,,1688583759.0,True,Interview,37,False,23,"I am a Junior DE from Argentina and got contacted via LinkedIn for a DE position in a Miami based Fintech company called fivvy.

The interview went great, however at the very end the recruiter asked me to write by hand a brief description of myself and the value I would add to the company, sign it and send them a picture so they could asses my personality traits with graphology in lieu of a psychological evaluation.

I am really confused, and considering pulling back from the recruiting process. But maybe it is a cultural thing and I am reading too much into it.

The role itself is very similar to what I do in my current job(AWS, ETL using glue, requirements gathering etc.) But the pay is roughly 80% more


Update: thanks everyone for the insight, I rejected the request and told the company that I wasn't comfortable with a graphology evaluation, but also told them that I was still interested in the position and if they were still interested in my services I was open to other forms of psychological assessment.

I just hope they don't start asking for my zodiac sign 😂",False,0.9,https://www.reddit.com/r/dataengineering/comments/14rcq73/is_it_common_in_the_us_to_ask_for_a_handwritten/,dataengineering
Just got certified! - Databricks certified associate developer for apache spark 3.0 in Python,14qzt8y,Background_Debate_94,1688530389.0,,1688544365.0,True,Career,50,False,142,"Just got certified! I am a new data analyst who wants to hopefully move into the data engineering field.

I have done a few projects just finding it hard in the current market to find a job. Decided to keep working at my current job and in the meantime finish off a few certs to hopefully attract a few recruiters. Gonna go for the data engineer associate and professional next

For anyone wanting to get it, I highly recommend getting it, it stays forever(no expiry), fairly simple took me 2 weeks assuming you have general python syntax knowledge, plus access to documentation in the exam.

The resources I used were:

1. [https://www.udemy.com/course/databricks-certified-developer-for-apache-spark-30-practice-exams/](https://www.udemy.com/course/databricks-certified-developer-for-apache-spark-30-practice-exams/) \- used this to test my knowledge and basically research what topics are more likely to appear, has a nice breadth of important topics
2. [https://chowdhury-joyjit.medium.com/field-notes-for-the-databricks-certified-spark-developer-exam-ca0b6eb452fc](https://chowdhury-joyjit.medium.com/field-notes-for-the-databricks-certified-spark-developer-exam-ca0b6eb452fc) \- a good reference guide
3. [https://www.udemy.com/course/databricks-certified-developer-apache-spark-30-python/?referralCode=D20A15144B8AF7D2C5CF](https://www.udemy.com/course/databricks-certified-developer-apache-spark-30-python/?referralCode=D20A15144B8AF7D2C5CF) \- essentially 90% of the actual exam questions pretty decent explanations  


P.S Sorry couldn't forget to include [Spark Internals Explanation](https://www.youtube.com/watch?v=7ooZ4S7Ay6Y)! A phenomenal resource to dive deep on how Spark works under the hood",False,0.94,https://www.reddit.com/r/dataengineering/comments/14qzt8y/just_got_certified_databricks_certified_associate/,dataengineering
Beginner Data Engineering Project with Kafka and Airflow,14rfp9v,RepresentativePen297,1688574605.0,,1688577655.0,True,Discussion,3,False,12,"I plan on starting a simple ETL pipeline using Airflow and Kafka and want your guys' opinion on what I plan to do.

I am using the [random user API](https://randomuser.me/api/) to get the data, then performing a simple transformation to prepare it to be put into its respective tables. I plan to simulate spikes of user data generating using a random number generator via Airflow and then pass the data into Kafka using producer nodes. Then I can put the data into a PostgreSQL RDS instance for the data to be stored. I know I don't need to use Kafka, but I want to dabble in Kafka a little to gain some experience. On the note of experience, I know the basic jist of Kafka and Airflow, but I am by no means an expert on either of them; I want to apply the knowledge I have learned so far.

Here is what the DAG would look like:

&#x200B;

https://preview.redd.it/01epxslza6ab1.jpg?width=1046&format=pjpg&auto=webp&s=9c2801230c6b4ca11e66b293320f209984195667

&#x200B;

Here is what the data model would look like:

&#x200B;

https://preview.redd.it/varxvs70b6ab1.jpg?width=1200&format=pjpg&auto=webp&s=4c3ae9f86f0904df900b8174497c15b6d171ace5

&#x200B;

Here is my [GitHub](https://github.com/Nishal3/) if you want to see and critique my other projects.",False,0.88,https://www.reddit.com/r/dataengineering/comments/14rfp9v/beginner_data_engineering_project_with_kafka_and/,dataengineering
Do you study tools that are not applicable to your current job?,14rrqnr,gintokiredditbr,1688601303.0,,False,True,Discussion,3,False,3,"

Let's say that your current job only allows you to use excel, do you think it would be worth studying outside to prepare for new positions that use power bi, sql, python and others? Or do you think it wouldn't be of much value to learn something that I can't put into practice right away? Because all this would consume time and resources, let's say the time after work, whether on weekends or on weekdays. Or would I be better off using that time to learn things useful for my current job? What career advice would you give?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rrqnr/do_you_study_tools_that_are_not_applicable_to/,dataengineering
What does it take to break to into database engine development or tools like Kafka?,14rrelb,snabx,1688600458.0,,False,True,Career,11,False,2,I kind of got into data engineering by chance but my interest after reading a few chapters from 'Design Data Intensive Applications' book is in database engines and tools like Kafka or Spark. I wonder how people break into this field of development. Most of discussions I find is about SQL and writing Python scripts which is something I do as well but I imagine it'd more be interesting in developing the tools underlying instead. I found r/databasedevelopment pretty good but it's not that active.,False,0.75,https://www.reddit.com/r/dataengineering/comments/14rrelb/what_does_it_take_to_break_to_into_database/,dataengineering
Struggling to figure out best way to do this,14rwet1,anasp1,1688613702.0,,False,True,Help,1,False,1,"I have a csv file sitting in S3 and what I need to do is migrate this into some actual database and not just a csv file in S3... How do I do this portion? So some feedback I've heard so far is (since the file is small, only around 800 - 900 records of data to start with) use Lambda to write it row by row into a database of my choice in AWS RDS (for example I could use postgres). 

How do I access this data in the database from an app I'd want to build (like how do I perform search with a specific NLP algorithm over the database)? What if my user wants to perform a search using some query and I want to use some NLP algorithm that I develop to match the query with the closest record or top 5 records in the database. I'll be writing the NLP algorithm / possible search code in Python but the front-end may be built with something else, is that an issue? 

Is there anyone that's well versed with this type of stuff and doesn't mind if I chatted with them regarding all this? That would help a ton!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rwet1/struggling_to_figure_out_best_way_to_do_this/,dataengineering
📣 Apache Iceberg has won the table format war,14s3c1s,rmoff,1688634196.0,,False,True,Blog,3,False,0,"📣Wait, what? Apache Iceberg won the table format war, apparently 😄  


**👉🏻** [**Iceberg won the table format war (But not in the way you thought it might)**](https://bitsondatadev.substack.com/p/iceberg-won-the-table-format-war)  


🙄 You might roll your eyes given that the author (Brian Оlsen) works for Tabular but IMHO it's a pretty even assessment of the current situation ✅  


I like Brian's optimism about the power of open standards to force the hand of vendors to do the right thing for users. I don't know if it will play out that way—and there are definitely some of the big vendors playing the ""open-source washing"" game ❄️  


But given three formats of approx the same capabilities, adopting based on the strength of its true openness seems like as good a basis as any other.  


**What do folk here think?** 

\- Will one of the formats ""win"" and others ""lose""?  
\- Will we just end up with a generic layer atop the others (UniForm etc) with a slight friction to bias the user to select the default (i.e. in the case of UniForm, Delta)?  


**obXKCD**

&#x200B;

https://preview.redd.it/8ryczu4b8bab1.png?width=500&format=png&auto=webp&s=10fcef5ef227d50ed07dcfdc589e6294664b2b09",False,0.4,https://www.reddit.com/r/dataengineering/comments/14s3c1s/apache_iceberg_has_won_the_table_format_war/,dataengineering
"Can someone explain what new capabilities Fabric brings without using the words “all”, “everything”, “none”, “never”, or any superlatives?",14rvulo,BestTomatillo6197,1688612241.0,,False,True,Discussion,1,False,1,"I’ve found a combination of Python, Task Scheduler, API, and triggers can get almost any “pipeline” and data warehouse tasks we’ve needed done. It seems hard to believe any out of the box MS program can take “dirty” data from different types of databases and files and organize them the same way scripts can. But everyone is making it sound like you put in some credentials and you’re done.

Can someone explain what new features MS Fabric brings to the table we’re supposed to be excited about?",False,0.67,https://www.reddit.com/r/dataengineering/comments/14rvulo/can_someone_explain_what_new_capabilities_fabric/,dataengineering
Help with choosing techstack for a new DE team,14rildp,sasu1992,1688580682.0,,False,True,Help,11,False,4,"Hello,

We are starting a new team to work don data engineering/data warehouse at my current company. Over the last couple of weeks, I am doing lot of reading to identify a correct tech stack.

Considerations:

1. Our DB is in the range of couple of hundred GBs, not huge in terms of TBs
2. In the short future, we would want to get started with reporting (BI) with an eye on Data Science/ML later 
3. We're an AWS heavy shop. So naturally, our first consideration is RedShift.

During my short research, I have heard good things about SnowFlake and Redshift seems to be underwhelming as data warehouse. But, considering our data size and cost, I'm not sure Snowflake would be a good choice for us.

So any suggestions on the techstack (building ETL pipeline, orchestration tools, data warehouse with ability or providing supportive tools to build self serving reports) would be greatly appreciated.

Thank you",False,0.7,https://www.reddit.com/r/dataengineering/comments/14rildp/help_with_choosing_techstack_for_a_new_de_team/,dataengineering
"Implement AI data pipelines with Langchain, Airbyte, and Dagster",14rhh9l,jeanlaf,1688578305.0,,False,False,Blog,1,False,4,,False,0.7,https://airbyte.com/tutorials/implement-ai-data-pipelines-with-langchain-airbyte-and-dagster,dataengineering
Anyone used Octopai (Data Lineage)?,14rn8by,cdigioia,1688590855.0,,False,True,Discussion,4,False,2,"We're looking at a fancy demo video for them.  

But I lack any concept of what all is out there / how much of a pain a tool like this is (e.g. does maintaining it take more time than is saved via using it, etc.)",False,0.76,https://www.reddit.com/r/dataengineering/comments/14rn8by/anyone_used_octopai_data_lineage/,dataengineering
"Apache Doris 2.0 Beta Now Available: Faster, Stabler, and More Versatile",14r8zo3,ApacheDoris,1688559364.0,,False,True,Open Source,14,False,9,"1. A query optimizer that is more effective than manual optimization
2. Faster log queries with less storage space consumed
3. 20 times higher concurrency
4. Enhanced Data Lakehouse capabilities (more data sources supported and faster performance)
5. A self-adaptive parallel execution model for higher efficiency and stability in hybrid workload scenarios
6. Efficient management of memory and CPU resources
7. Elastic scaling of computation resources and hot-cold data separation for much lower storage costs
8. Faster, stabler, and smarter data ingestion
9. No more OOM errors
10. Support for Kubernetes deployment

[Release Note](https://doris.apache.org/docs/dev/releasenotes/release-2.0-beta/)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14r8zo3/apache_doris_20_beta_now_available_faster_stabler/,dataengineering
Professional Data Engineering Community,14rguyt,AutoModerator,1688577029.0,,False,True,Meta,5,False,3,"Hey Data Engineers,

We recently [announced](https://www.reddit.com/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/) that we would be opening up a new community for data engineering professionals to network and join in-person events. We had over 300+ signups on the waitlist with data engineers from over 33 countries. We've sent out emails to everyone on the waitlist - if you didn't receive an email, please check your spam folder or reach out to info@dataengineering.wiki.

We are now generally accepting members and you can [join here](https://community.dataengineering.wiki/). 🥳

&#x200B;

[This could be you](https://i.redd.it/7t1ig7dlf6ab1.gif)

We've had a lot of offers to help out and we've put together a few ways you can get involved in the professional community. If you're interested in any of these, please message the mod team:

1. **Volunteer to speak:** we are putting together some local and virtual meetups and are looking for speakers to share something they are passionate about.
2. **Help organize a meetup:** if you're interested in starting a local meetup in your area let us know and we can connect you with speakers and resources to help you get it started.
3. **Join as an existing meetup:** we can create a space just for your local area which you can customize and use for free to operate your existing meetup.
4. **Contribute to our newsletter:** we run a free monthly newsletter and are always looking for interesting ideas to share.
5. **Share your story:** share your written story about how you got to where you are today and give advice and inspire others who might want to take the same path.
6. **Share your feedback:** most importantly, we need your feedback to keep improving this community. If you would be willing to leave us a review that we can use externally, please let us know!

As always, we are listening to your feedback and using it to shape the community and we cannot do it without you. Thank you to everyone who has offered their time, help, and expertise to make our community great.",False,0.67,https://www.reddit.com/r/dataengineering/comments/14rguyt/professional_data_engineering_community/,dataengineering
Going with Microsoft as default option due to less new stuff for lawyers to read?,14rkg4r,Dull_Lettuce_4622,1688584691.0,,1688586378.0,True,Discussion,1,False,2,"One thing I've realized in my career:
Similar to the ""no one has gotten fired for hiring IBM"" trope, we are solidly in a a 20 year era where due to Microsoft by and large keeping their enterprise licensing bundled and their terms of service similar and more consolidated for Lawyers to interpret, for any medium to large company already on Microsoft for office, word, 365, teams, etc. had a massive incumbency advantage to adopt Microsoft.

To me this ""bundling"" is somewhat anti competitive and is a natural monopoly of sorts. It's in aggregate efficient for the world for lawyers to need to review less contracts and info sec people to review less policies, but it leads to insane market concentration.

If any of y'all had the pain of working for a fortune 500 company with strict IT approval/compliance processes, y'all will know of the pain for getting any software approved and installed.  It takes tremendous time and political capital to get anything done. For example just today because GitHub copilot is under the Microsoft umbrella and part of our ""greenlist"", we were able to fast track our approvals and get it on our corporate counsels plate of work, whereas if we went with something less known like say a model that say a startup like anthropic builds, due to relative unknowns, it woildve taken out lawyers month to get comfortable.

As an ultimate end user and middle manager pushing sisyphean boulders, this makes me excited in the short term the cloud platforms are buying up various parts of the AI toolkit (ex. Neeva, mosaiqML etc.) and integrating + bundling it together.

As an economist and hopeful future startup founder, though this incumbent natural monopoly and market power due to having a legal ""moat"" doesn't feel right to me.",False,0.75,https://www.reddit.com/r/dataengineering/comments/14rkg4r/going_with_microsoft_as_default_option_due_to/,dataengineering
Data engineering bachelors degree major,14rpbzh,Carefull_eater,1688595592.0,,False,True,Discussion,3,False,1,"Hi guys I have a choice to study in a college that offers a Data engineering major for bachelors degree , do you advice me to start this journey and what can I transfer later with this degree for well paying career.?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rpbzh/data_engineering_bachelors_degree_major/,dataengineering
Better VSCODE local dev with snowflake+dbt,14r5j5q,rudboi12,1688549124.0,,False,True,Discussion,3,False,9,"I'm currently migrating my new teams pipeline from databricks notebook (all in dev, no cicd, or major tests. scheduled in databricks. All spark.sql code, no pyspark at all.) to snowflake +dbt. 

While everything is going well, I want to create a local development environment everyone on my team can use easily (mostly non engineers). One of the main pain points is that 3/4 of my team has windows computers since they are analysts. 

Best way to do this is creating some devcontainers with VScode. And while I've already done this and it's working quite nice, I want to take it a few steps further and want to keep all the development inside VScode. For example, right now, I tests some queries in snowflake worksheets and when it works, I basically copy paste the query to vscode and change the table format to dbt (source or ref). This is a bit tedious for me and for my analysts it will be worse since they are use to get results instantly in databricks notebooks. 

Is there actually a better way of developing with dbt? can i possibly run a dbt model and get the query result inside vscode directly (without having to change table name)? 

Any other tips would be appreciated. 

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14r5j5q/better_vscode_local_dev_with_snowflakedbt/,dataengineering
Need advise on data management,14rmkmg,hello_tyc_123,1688589413.0,,False,True,Help,2,False,1,"Hi guys, I want to preface this by saying I'm not a classically trained data engineer, and everything I know in the area is essentially through readings so I apologize in advance for my ignorance.

I work at a company that isn't very ""tech"" (no engineers), so I want to create a data management system that allows different platforms to communicate with each other.  We are currently using SSMS for only one of our data sources so the plan is to consolidate the data sources into a data warehouse with ETL.

My approach, being limited in knowledge, is writing python scripts to call the different API end-points (typically dimensional) and send the output to a SQL database where I can then transform into fact tables before plugging into a BI tool.  This is all in theory. The problem is, I'm not confident enough to send everything to SSMS because I don't want to mess anything we have already up. With this in mind, I've begun exploring things like PostgresSQL as an alternative, if I'm able to verify that everything works there then maybe I can send the outputs to SSMS so everything can live in one place.

I'm not very familiar with hosting a SQL server like postgres (we don't have IT) and I don't know if putting data on there is secure (?), as well as if my approach would be even considered acceptable from the data engineering stand point. Wanting to get some feedback as no one really speaks my language at my workplace. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rmkmg/need_advise_on_data_management/,dataengineering
How to ingest records from Cosmos DB using Spark,14rg8sp,Dramatic_Necessary_3,1688575754.0,,False,False,Blog,0,False,2,,False,1.0,https://harshmatharu.com/blog/ingest-records-from-cosmosdb-using-spark,dataengineering
"Join, Merge, and Combine Multiple Datasets Using pandas",14rfc42,python4geeks,1688573885.0,,False,True,Blog,1,False,2,"Data processing becomes critical when training a robust machine learning model. We occasionally need to restructure and add new data to the datasets to increase the efficiency of the data.

We'll look at how to combine multiple datasets and merge multiple datasets with the same and different column names in this article. We'll use the `pandas` library's following functions to carry out these operations.

* `pandas.concat()`
* `pandas.merge()`
* `pandas.DataFrame.join()`

The `concat()` function in `pandas` is a go-to option for combining the DataFrames due to its simplicity. However, if we want more control over how the data is joined and on which column in the DataFrame, the `merge()` function is a good choice. If we want to join data based on the index, we should use the `join()` method.

**Here is the guide for performing the joining, merging, and combining multiple datasets using pandas👇👇👇**

[Join, Merge, and Combine Multiple Datasets Using pandas](https://geekpython.in/multiple-datasets-integration-using-pandas)",False,0.63,https://www.reddit.com/r/dataengineering/comments/14rfc42/join_merge_and_combine_multiple_datasets_using/,dataengineering
Testing spark applications,14rchm7,LeftHelicopter5297,1688567851.0,,False,True,Help,6,False,2,"Hi,

What do you mainly use for testing your spark jobs? Is it unittest or rather a pyspark specific library? 

Could you please elaborate on how the testing fits into your development process? At my current position there is no formal testing process, so I open up the console, create some dummy data that I feed into the dataframe, run some transformations and check the result with my eyes.

I would like to learn some more professional way of running tests but don't know how to approach it, and, if possible, how to gradually improve my testing process.

Thank you",False,0.75,https://www.reddit.com/r/dataengineering/comments/14rchm7/testing_spark_applications/,dataengineering
Showcasing Complete ETL Pipeline Skills,14rcg9h,conlake,1688567760.0,,False,True,Discussion,2,False,2," I'm currently working on a project to demonstrate my ability to develop end-to-end data pipelines. I'm wondering if there are any skills or tools that I may have overlooked. Currently, my process involves the following components:

* I utilize Airflow as the orchestrator, which retrieves data from an online API.
* Next, I perform data transformations using Spark.
* The transformed data is then loaded into a PostgreSQL database.
* I create various views from this data using PostgreSQL.
* Finally, I use Grafana to automatically generate and display dashboards based on the views. 
* All of these components are implemented using Docker containers.

1) Is there anything essential to the data engineer position that I may have omitted? Could you show the logic of your typical end-to-end data pipeline and what tech-stack do you use to do it?

2) What's the best practice or ideal data pipeline/tech stack?

3) How is your performance (data engineers) typically measured? (e.g. By the number of pipelines done?)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rcg9h/showcasing_complete_etl_pipeline_skills/,dataengineering
Dask for data engineering,14r8spe,Asleep-Organization7,1688558832.0,,False,True,Discussion,11,False,3,"I would like your opinion of the Python library Dask. 

I made some tests with several datasets with a total of 8 GB and it performed a simple ETL process in 30 sec. 

I did the same process wit PySpark and it took 28 sec.  
I also found this article talking about Spark vs Dask  
[https://www.coiled.io/blog/moving-from-spark-to-dask](https://www.coiled.io/blog/moving-from-spark-to-dask) 

Is Dask that good to handle middle to big-size datasets?   
Then why is a library not that well known? 

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14r8spe/dask_for_data_engineering/,dataengineering
"Is this a bad idea? Using ""recursion"" in AWS step functions to orchestrate sequential trino steps instead of explicitly assigning every step to a task.",14rbu13,AUGcodon,1688566372.0,,1688566655.0,True,Help,1,False,2,"Hi all, wanted a second opinion on if this idea is one of those picking up a penny in front of a steam roller things. We are limited to using lambda to make an api call to trino and step functions as the orchestrator

Option 1 - Explicit 

    Step function Task 1(Trino Transformation 1) > Task 2(TT2) > Task 3(TT3) 

Option 2 - ""recursion"" 

    Trigger(send step ID) > Recursive Task(output next step Id), rerun function until end state is reached. 

What kind safeguard should I put in place to prevent infinite recursion? 

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rbu13/is_this_a_bad_idea_using_recursion_in_aws_step/,dataengineering
"How to Make Your Data Ingestion Simple, Fast, And Robust - 11 Advanced best practices for working with data ingestion pipelines.",14r4zcp,sbalnojan,1688547389.0,,False,False,Blog,1,False,4,,False,1.0,https://meltano.com/blog/how-to-make-your-data-ingestion-simple-fast-and-robust/,dataengineering
dbt Enterprise vs Team Plan Pricing,14rf3ss,morse__code,1688573384.0,,False,True,Help,1,False,1,"Anyone have experience moving from the dbt Team plan to the Enterprise tier?  Just trying to understand the rough pricing difference per user and biggest benefits.

&#x200B;

BACKGROUND: 

I work for a company that consists of 3 historically very decentralized business units where I have built out a data warehouse for 1 of the 3 using dbt Team with a small team of 3 people.  Trying to understand whether it makes sense to get Enterprise across all 3 to develop more consistently across as well as facilitating a 4th corporate centralized DW from data shared up from the business units.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14rf3ss/dbt_enterprise_vs_team_plan_pricing/,dataengineering
Object Management for AI/ML,14redmy,swodtke,1688571886.0,,False,False,Blog,0,False,1,,False,1.0,https://blog.min.io/object-management-for-ai-ml/,dataengineering
Best approach of data engineering this kind of data for powerbi or tableau,14rc102,pectin232,1688566799.0,,False,True,Help,0,False,0,"Engineering shop with many estimate date 1-10, est date 2 1-10, etc .. using it for powerbi. Should I split the table with one just dates and another all the others like Yes, no, proj desc, proj other info. Best approach?  a small sample below. There are  alot more. Just wondering.  
[PROJV.PIN](https://PROJV.PIN) pin\_char

, PROJV.REGION\_NUM ""Region""

, PROJV.PROGRAM\_TYPE\_CDE\_DESC ""Program Type""

, PROJV.WORK\_TYP\_CDE\_DESC ""Project Type of Work""

, (to\_char(PROJV.EARLIEST\_LET\_DTE,'YYYY-MM-DD')) ""Let Date""

, PROJV.COUNTY\_CONCAT ""County""

, PROJV.ROUTE ""Route""

, PROJV.TERMINI\_TXT ""Project Description""

, PROJV.SCOPE\_OF\_WORK\_TXT ""Scope of Work""

\--     , PROJV.TRACTS\_TEXT ""ROW Tract Info""

\--     , PROJV.BEG\_LM\_NBR ""Beg LM""

\--     , PROJV.END\_LM\_NBR  ""End LM""

, PROJV.PROJECT\_LENGTH ""Project Length""

&#x200B;

\--ATTRIBUTES

, PAJ.BR\_PROG ""Bridge Program Yr""

, BR\_SYS.BR\_SYS ""Bridge System""

, PROJV.BRNO\_CONCAT ""Bridges""

, PAJ.GFT\_YR ""GFT Year""

, PAJ.RR\_IND ""Railroad Involved""

, PAJ.IA\_IND IA

, PAJ.DONE\_UNDER ""Done Under Other PIN""

, PAJ.PARENT\_PROJ ""Project Work Being Done On""

, PAJ.WK\_PRGM ""Work Program Project""

, PAJ.ENV\_DOC ""Environmental Document""

, PAJ.TAMP\_CLASS ""TAMP Asset Class""

, PAJ.TAMP\_WK ""TAMP Work Type""

, PAJ.TAMP\_NHS ""TAMP NHS""

, PAJ.PPRM\_NHS ""PPRM NHS""",False,0.33,https://www.reddit.com/r/dataengineering/comments/14rc102/best_approach_of_data_engineering_this_kind_of/,dataengineering
I created an interview preparation Trello template with technical & behavioural questions,14ql6jc,TheDataPanda,1688491177.0,,False,True,Interview,6,False,46,"Link: https://trello.com/b/Rvw8Jygt/interview-preparation

I have been looking for entry level or junior data engineering roles, and have been using a Trello board to help with interview preparation.

It contains lists of common interview questions, technical questions, and behavioural questions.

I’ve tried tagging behavioural questions based on attribute it’s measuring (e.g., conflict management) so you can filter by these.

For the technical questions, these are a combination of questions I’ve been asked, or questions you could be asked if applying for a data engineering or analyst role.

For each card/question you can write potential answers in the description field.

You can obviously customise it however you like, but hopefully is useful for some of you",False,0.97,https://www.reddit.com/r/dataengineering/comments/14ql6jc/i_created_an_interview_preparation_trello/,dataengineering
"Informatica vs. Oracle Data Integrator vs Oracle GoldenGate vs others (Spark, Python)",14r6ltr,Ok_Cancel_7891,1688552385.0,,False,True,Discussion,0,False,2,"I have recently stumbled on a job opportunity that uses Oracle data integrator. As someone who is 'at home' with Oracle, seemed intriguing, but then I checked market share and found Informatica has much greater market share...   


on the other hand, I am more fond of 'hands-on tools', like writing scripts by myself, not utilizing any kind of GUI.   
What is the relevance and prospects of tools like ODI and/or Informatica, and even Oracle's GoldenGate vs Spark and others?  


when talking about this, obviously, I am talking about big corporate clients with some regulations and needs for support, etc etc..",False,1.0,https://www.reddit.com/r/dataengineering/comments/14r6ltr/informatica_vs_oracle_data_integrator_vs_oracle/,dataengineering
Pyspark project ideas for an already employed data engineer,14rbmdo,LeftHelicopter5297,1688565885.0,,False,True,Career,2,False,1,"Hi,

I work as a mid DE and have some exposure to spark - it's only small things - read a file (mainly csv/json), use a predefined schema, apply some transformations, save it to orc. Everything was set up for me, airflow runs things, so I mainly need to make small edits in preexisting scripts and navigate airflow to the proper script. I don't even have access to spark UI and it all runs on-prem, on one machine. 

I would like to break into a spark heavy role, ideally databricks. I've done some courses, read two books on spark and whenever there's a chance to do something with spark at work, I volunteer to do it. I am also preparing for databricks spark certification (my job market is full of shitty consulting companies, I have no chance to land a job in a product company for now, as I'm not competent enough).

The job market in my country became quite rough recently and I would like to improve my chances to land a better position. I literally have no private git repo and I would like to change that, create some projects that I could link to in my CV.

Could you recommend some projects that I could do after hours in something like a month or two and then perhaps build on? I would appreciate even vague ideas.

Thanks guys",False,0.67,https://www.reddit.com/r/dataengineering/comments/14rbmdo/pyspark_project_ideas_for_an_already_employed/,dataengineering
Azure Performance vs On-Prem,14quid6,atlvernburn,1688513977.0,,False,True,Discussion,14,False,14,"My company is moving from on-Prem to Azure, and one thing I don’t get about ADF/Databricks is how small items perform so poorly in Azure (or cloud enviros in general). 

I get that they’re built for scale, and the Spark cluster spin up has a floor of runtime. But this is an annoying limitation. 

I have several jobs that are several small pipelines as one large job, which between ingestion and transforms are comparatively very slow in Azure. But, old on-premise ETL tools written in C did this so simply (think SSIS or BODS). 

How do people run workloads in Azure especially when they have SLAs? I see more complicated items running much quicker, which is great, but how do fellow DE’s do smaller loads in Azure?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14quid6/azure_performance_vs_onprem/,dataengineering
EtLT approach: Load data into BigQuery with DBT,14ra8cd,yfeltz,1688562583.0,,False,True,Blog,0,False,1,"Isn't DBT only for the 't'? Well... yes, it is!  
I had the opportunity to use it as a workaround for the 'EL'.  


So, I just coined a new data jargon: 'EtLT'. Will it catch? 😄  


Happy pipeline! 🚀  


[https://medium.com/everything-full-stack/etlt-approach-load-data-into-bigquery-with-dbt-aadf6dbba124?source=friends\_link&sk=4a450bf55e37293ef5078985950db5b2](https://medium.com/everything-full-stack/etlt-approach-load-data-into-bigquery-with-dbt-aadf6dbba124?source=friends_link&sk=4a450bf55e37293ef5078985950db5b2)

&#x200B;",False,0.67,https://www.reddit.com/r/dataengineering/comments/14ra8cd/etlt_approach_load_data_into_bigquery_with_dbt/,dataengineering
Anybody use Alteryx,14qi60z,Ok-Enthusiasm-6194,1688484263.0,,False,True,Discussion,71,False,48,I work at a small firm that is about thirty years behind in tech.  All of our data is in spreadsheets scattered across a vast and confusing folder tree. We are looking to use alteryx as a no-code ETL and eventually creating an azure database. I noticed most DEs use dagster or airflow. Does anybody have experience with a no code tool like Alteryx? I worry no-code equates to no flexibility,False,0.96,https://www.reddit.com/r/dataengineering/comments/14qi60z/anybody_use_alteryx/,dataengineering
DE Interview question for handling ETL pipeline errors.,14qiuds,HealthyCobbler1588,1688485836.0,,False,True,Interview,8,False,43,"I have DE interview coming up  and I am thinking to prepare few questions based on handling ETL errors.

A data pipeline should address these issues:

1· Partial loads (A scenarios where Partial processing of the files or records or any failures of ETL Jobs occurred; to clean up a few records and re-run the job)

&#x200B;

2 · Restart-ability (You have to re-run from a previous successful run because a downstream dependent job failed or reprocess process some data from history. for e.g. We need to run since last Monday or a random date)

&#x200B;

3· Re-processing the same files (A source issue where they sent multiple files; We need to pick the right records)

&#x200B;

4 · Catch-up loads (In case you missed executing jobs for specific runs and playing catch up; Batch Processing) .

Any Answers on these would be super helpful. Thanks. 🙏 ",False,0.96,https://www.reddit.com/r/dataengineering/comments/14qiuds/de_interview_question_for_handling_etl_pipeline/,dataengineering
Data Engineering start to finish,14qzi8p,knowledgeMeUp,1688529422.0,,False,True,Help,1,False,3,"Hi Everyone, 

What is the best resources for understanding how to build out a pipeline from scratch. I'm currently in an interview for a company that seems to have no pipeline set up based on my conversation with the hiring manager.

It had me thinking if I was to get the job I wouldn't know where to start. Where can I find good resources on setting up a production and dev environment, setting up testing, integrating CI/CD, integrating ochestration tool like airflow and all the small details that go into setting up a new environment.

I figured I should get started now since it's always good to learn things start to finish.

Any resources you could provide would be great (books, articles, other post, ect...)

Thanks in advance!",False,0.81,https://www.reddit.com/r/dataengineering/comments/14qzi8p/data_engineering_start_to_finish/,dataengineering
Are there any side hustles as a DE with smaller barriers to entry than building a full-scale web app?,14qqy5s,Reddit_Account_C-137,1688504686.0,,False,True,Career,9,False,14,"I'm in my mid 20s, single, and have a hybrid job which gives me a crap ton of free time to just sit around wasting time. I already lift, read, etc. so I've got all the good habits I could possibly want to build. However, I would like to make more money.

Are there any side hustles for an early career DE that don't have a huge barrier to entry or that aren't basically gambling?

I've considered web apps but frankly I'm coming up with solutions without very well documented problems. Additionally the time required to build a good functional web app could easily be over a year to ever earn that first dollar.

I've considered scripts for predicting sports card values or for gaining an edge in sports betting but once again that just seems like gambling with extra steps.

I'm too early in my career for consulting.

The best idea I've come up with is some sort of DE/analytics blog with an emphasis on sports or small scale IoT projects.

Does anyone have any other suggestions? Should I just be focusing my time on learning new skills to get a raise and/or a new job?

&#x200B;",False,0.82,https://www.reddit.com/r/dataengineering/comments/14qqy5s/are_there_any_side_hustles_as_a_de_with_smaller/,dataengineering
Which Kimball to Start with,14qusqd,El_Cato_Crande,1688514805.0,,False,True,Help,12,False,6,"So I wanna read the Kimball books as I've seen them mentioned many times as being a great source of quality information about how to do things.

I see three books:
The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling

The Data Warehouse Lifecycle Toolkit: Expert methods for designing, developing, and deploying data warehouses

The Data Warehouse ETL Toolkit: Practical Techniques for extracting, cleaning, conforming, and delivering data


Should I start going through them in any particular order or just dive in where it seems appropriate? Also would it make more sense to get one of the more modern editions of the books or the content regardless of the edition is still relevant and worth it.

Thanks a lot guys. Really appreciate the help you've all been",False,0.88,https://www.reddit.com/r/dataengineering/comments/14qusqd/which_kimball_to_start_with/,dataengineering
Feeling stuck with streaming Firestore data to BigQuery,14r67qw,data_analyst_0103,1688551262.0,,False,True,Help,0,False,1," Hi. I am pretty new to this data field where I am learning new things every day.

My goal was to **stream Firestore data to BigQuery** so that I can use it further on for visualizations in either Google Studio or Power BI.

I installed the **Straming Firestore to BigQuery extension in Firebase** and also completed the backfilling of data using the ""**npx** [u/firebaseextensions](https://www.reddit.com/u/firebaseextensions/)/fs-bq-import-collection"".

Here is the issue that I am facing:

* When I UPDATE the documents that have been pushed to the Firestore, they do not appear in the raw\_changelog table in BigQuery. But when I create a new document myself or edit the same created document it shows in the raw\_changelog as CREATE and UPDATE operation respectively. Why is that? Why the data being pushed by the app is not being recorded by the changelog?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14r67qw/feeling_stuck_with_streaming_firestore_data_to/,dataengineering
IoT Data Direct Ingestion to Kafka Using WaterStream MQTT,14r4dc3,TheSqlAdmin,1688545378.0,,False,False,Blog,0,False,1,,False,1.0,https://blog.shellkode.com/iot-data-direct-ingestion-to-kafka-using-waterstream-mqtt-11f52c73a84e,dataengineering
Advice for a personal project,14r475p,SqueezyOrangeJuice,1688544801.0,,False,True,Career,1,False,0,"I really want to break into data analysis out of my current job. I work at a large hospital in the Emergency Department as a patient care technician. I graduated with my Mathematics degree but unfortunately never got around to applying for internships or doing any AMAZING data projects worth putting on my resume. I've heard it would be in my best interest to start on my own project. Since I work in the hospital, I thought it would be fun to make a clone of the software that we use, EPIC. 

I just want to start small and focus on an Emergency Department version. A tab where you can scroll and view patient names and room number, a tab for those in the waiting room, a tab that shows general data like average wait times, etc. I just don't know where to begin!

What tools would I need? I know a fair amount of SQL and Python, but never really used anything like Tableau or PowerBI. Is there anywhere I could get mock patient data? And this might be a dumb question, Is it legal to make a clone of this software and present it to employers?

Any help appreciated!",False,0.5,https://www.reddit.com/r/dataengineering/comments/14r475p/advice_for_a_personal_project/,dataengineering
"How can I ensure that my data model accurately represents the data in my OLAP DB? How can I be certain that my DIM and FACT tables effectively capture the business workflow, leading to optimized and efficient queries?",14qvo0e,faizfablillah,1688517373.0,,False,True,Discussion,3,False,4,"How can we streamline the process of understanding source data? Frequently, questions like ""What business process generates this data and why?"" require accurate and prompt answers, but the current methods often make it difficult to achieve this efficiently.

Typically, the data team must engage with business users and stakeholders regularly, relying on self-learning to gain insights. Unfortunately, this process can be lengthy and arduous.

Any advice to make this better?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14qvo0e/how_can_i_ensure_that_my_data_model_accurately/,dataengineering
VulcanSQL: Create and Share Data APIs Fast!,14qdocu,chilijung,1688473066.0,,1688473681.0,True,Open Source,18,False,32,"Hey Reddit!

I wanted to share an exciting new **open-source project: ""VulcanSQL""**! If you're interested in seamlessly transitioning your operational and analytical use cases from data warehouses and databases to the edge API server, this open-source data API framework might be just what you're looking for.

**VulcanSQL (**[**https://vulcansql.com/**](https://vulcansql.com/)**) offers a powerful solution for building embedded analytics and automation use cases**, and it leverages the impressive capabilities of DuckDB as a caching layer. This combination brings about cost reduction and a significant boost in performance, making it an excellent choice for those seeking to optimize their data processing architecture.

By utilizing VulcanSQL, you can move remote data computing in cloud data warehouses, such as Snowflake and BigQuery to the edge. This embedded approach ensures that your analytics and automation processes can be executed efficiently and seamlessly, even in resource-constrained environments.

GitHub: [https://github.com/Canner/vulcan-sql](https://github.com/Canner/vulcan-sql)

&#x200B;

https://preview.redd.it/x5mq6yz9xx9b1.jpg?width=1898&format=pjpg&auto=webp&s=7535c114eebed8618007b730ea41249976184a8e",False,0.9,https://www.reddit.com/r/dataengineering/comments/14qdocu/vulcansql_create_and_share_data_apis_fast/,dataengineering
"user identity, machine identity and now we need data identity ? what do you think?",14qws59,Extreme-Summer-2756,1688520815.0,,False,True,Discussion,4,False,3,"thinking of creating some kind of data identity system which will put a data in some kind of envelope - add id, tags etc to that envelope and share that data envelope with whoever needs data access.          In short data bundle concept with identity tied in. 

These data bundles are like container images which has registry, versioning which can be used by AI models to train a data. As good data will always be an issue going forward ?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14qws59/user_identity_machine_identity_and_now_we_need/,dataengineering
Implementing a modern data platform in a small team,14qiyie,Specific-Passage,1688486091.0,,False,True,Discussion,11,False,9,"I am a junior data analyst shifting towards a data engineering role. We are implementing a modern data stack with azure blob storage, azure synapse and databricks. We just have one source of data (homegrown ERP system developed in-house) using sql server. The ERP system is pretty bad in terms of data quality (poor data, lots of etl to do). What’s the best way to approach data engineering? Our plan is use databricks notebooks for etl, synapse pipelines for orchestration, great expectations for validations. Our team is intermediate in Python, but looking to grow. Solution should be simple, yet scalable. Team of 5. Any suggestions/tips?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14qiyie/implementing_a_modern_data_platform_in_a_small/,dataengineering
How do Lakes Provide Adequate Query Performance?,14qfef7,PencilBoy99,1688477719.0,,False,True,Discussion,13,False,11,"Lets assume we're talking about structured data that's been pulled into a lake.

In most cases, data has certain characteristics that you might search by (indexable columns) (""employee id"") and/or have values that might be optimized for read (e.g., numeric values in a columnstore structure).

If all you have is just progressively cleaner layers of data that's actually just in object store (e.g., parquet files in s3) but doesn't use anything like what I described above, how is that efficient to query?

My guess is that they're using a layer that brute forces the efficiency (distributed queries), or the product has an additional part that actually lets you store the data in a more traditionally table like structure.",False,0.83,https://www.reddit.com/r/dataengineering/comments/14qfef7/how_do_lakes_provide_adequate_query_performance/,dataengineering
Kimball books,14q9f0u,rogerbarario,1688459926.0,,False,True,Help,30,False,26,"Hi,

Looking to get more knowledge on data warehousing and I've seen the Kimball books mentioned a couple of times and was just wondering if anyone could point me in the direction of the best one to get for a beginner please",False,0.94,https://www.reddit.com/r/dataengineering/comments/14q9f0u/kimball_books/,dataengineering
Hey! I've Analyzed Thousands of Data Engineer Job Postings - Feedback Needed,14q92b9,Lukkar,1688458714.0,,1688570059.0,True,Help,13,False,26,"Hey there! I've done some data analysis based on Glassdoor job postings for the search term ""data engineer"" I looked at around 3,000 job entries from Europe, USA + Canada, South-East Asia + Oceania.

Here's the analysis I conducted:
You can find it here: *[+100 insights - Data Engineer 🧭🗺️](https://www.kaggle.com/code/lukkardata/100-insights-data-engineer#%F0%9F%8C%9F-Introduction)*

I based my research on various sources like blogs, courses, and vlogs to figure out what companies mean when they use the title ""data engineer"". The data science roles can be vague, and responsibilities tend to get mixed up. So, I'm here to gather valuable feedback.

___

One area where I have the most doubts is regarding the selected technologies. I'm not sure if I've categorized them correctly or if I've chosen the right ones. You can check the tech here: *[Tech Knowledge Required](https://www.kaggle.com/code/lukkardata/100-insights-data-engineer#14.-Tech-Knowledge-Required)*

Here are the specific questions I have according to tech:

1. Is Databricks more of a ""cloud platform"" or a ""data integration & processing platform""?
2. Are the *""Tech Knowledge Required""* categories well-defined?
3. Are there any redundant categories?
4. Are there any missing or unnecessary tech categories?
5. Do you think there's anything else I should add?
	
Please point out any misspelled words or confusing phrases in my analysis. Feel free to provide any additional comments apart from the technology aspect.

___

Lastly, I have a premium question for you. I've heard that in most companies, a ""Data Scientist"" is essentially just a ""Data Engineer"" with added responsibilities like creating regressions and data classifications. I'm curious about your insights or personal experience with this. Or does it depend on the company?

Thanks a lot for taking the time to read through this!

___

[EDITED]

I've got comments about a bad data source (Glassdoor). In some countries, it is not even used.
I will be thankful for providing more reviewed examples.
I'm aware that no one fits all, but it could be even for the country where you work (Especially if you are from the EU).

The list of countries is here [link](https://www.kaggle.com/datasets/lukkardata/data-engineer-job-postings-2023)",False,0.93,https://www.reddit.com/r/dataengineering/comments/14q92b9/hey_ive_analyzed_thousands_of_data_engineer_job/,dataengineering
Are Surrogate keys and building fact tables in datawarehouses a thing of the past?,14qczkc,Olafcitoo,1688470969.0,,False,True,Discussion,15,False,13,"Hello Reddit,

I am building a DWH for a client that has most of its data stored in Azure Cosmos containers. Each items, but not all are stored with GUIDs as IDs.

&#x200B;

I am tasked with making a data warehouse to store historical changes, but also to enable reporting in Power BI. I have developed a theoretical pipeline inspired by Databricks Medallion Structure that loads the data into seperate SQL tables using ADF for orchestration:

\-  staging SQL tables with ID's, data in json structure and a timestamp

\- A bronze layer, which checks for changes in the data between the staging tables and present table. It will skip unchanges records, and create a new row with updated set time\_from and time\_to columns in order to form SCD2s.

\- A silver layer, where most of the data will be cleaned & transformed, a tabular schema will be established using  statitically defined SQL queries with data types.  

\- Gold layer, where I was considering making a star schema inspired by Kimballs 3NF. This would involve making fact tables and add surrogate keys either in this layer or the prior, maybe using a mapping table, as I could imagine that the data from the source could change relatively often.

&#x200B;

So my issue is, how valid is this approach or a thing of the past? I come from a prior Power BI consultancy where we strictly used Kimballs architecture, but I was never taught how to establish the Fact tables. Additionally, I am unsure how to establish a fail-proof method of adding surrogate keys and handle changing dimensions in this regards. 

&#x200B;

&#x200B;",False,0.93,https://www.reddit.com/r/dataengineering/comments/14qczkc/are_surrogate_keys_and_building_fact_tables_in/,dataengineering
Big data processing for machine learning,14qqmzn,Jebin1999,1688503949.0,,False,True,Help,4,False,2,"Do you have any suggestions for the routine job to extract Big data from MySQL server to CSV, preprocessing (cleaning and transformation), and prepare for machine learning? Is there easy way anyone using?

&#x200B;",False,0.75,https://www.reddit.com/r/dataengineering/comments/14qqmzn/big_data_processing_for_machine_learning/,dataengineering
Data engineer vs Fullstack as a job and why should I choose Data engineer?,14quusz,InternationalTour303,1688514976.0,,False,True,Help,7,False,1,"Hello redditors of r/dataengineer I feel like i walked in to a party I was  never invited to but here I am, asking a question.I was recently accepted to get educated as an Data engineer or a Fullstack .net developer.Now I am going to be honest, I an a geotech engineer and love tech but looking to challenge myself abit intellectually workwise but dont really know either of the occupations well enough to decide, I am not necissarly trying to combine my previous work with a new one, More of a fresh start and I wanted to know if you guys think i should choose Data engineer over Fullstack .net developer and why.",False,0.57,https://www.reddit.com/r/dataengineering/comments/14quusz/data_engineer_vs_fullstack_as_a_job_and_why/,dataengineering
What skills is top 1% in data engineering?,14pzuls,Jealous-Bat-7812,1688430620.0,,False,True,Discussion,43,False,58,"Basically the title, I wanted to know if smart ETL’ing, or great cloud skills makes us into the top 1% in our field. What in your opinion is the skills required to be in the top 1% of data engineering field?",False,0.89,https://www.reddit.com/r/dataengineering/comments/14pzuls/what_skills_is_top_1_in_data_engineering/,dataengineering
Transformation in Tableau vs Backend [ETL],14q3yo0,cida1205,1688442530.0,,False,True,Discussion,12,False,15,Complex calculation to be done in tableau and minimalistic aggregation to be done in backend is my understanding. I am still speculative what shouldn't be done in the visualization layer and what should be done in visualization layer,False,0.81,https://www.reddit.com/r/dataengineering/comments/14q3yo0/transformation_in_tableau_vs_backend_etl/,dataengineering
Snowflake basic concepts with queries,14q8rts,akashTheTraveller,1688457760.0,,False,True,Personal Project Showcase,1,False,5,"\*Hi All!\*

  


Lately, I've been immersing myself in Snowflake tool and exploring its capabilities. Along the way, I've compiled a collection of queries that I've executed and organized them on GitHub. It's a resource for anyone looking to refresh their understanding of the basic concepts.

  


Here's the link to the GitHub repository: \[Snowflake Queries Repository\](https://github.com/dattapadal/Snowflake_tutorial.git)

  


Thanks",False,0.78,https://www.reddit.com/r/dataengineering/comments/14q8rts/snowflake_basic_concepts_with_queries/,dataengineering
data news #34,14qfahg,AmphibianInfamous574,1688477441.0,,False,False,Blog,0,False,1,,False,0.67,https://patrikbraborec.substack.com/p/data-news-34,dataengineering
Best workflow solution,14pxq7v,cyamnihc,1688425041.0,,False,True,Discussion,6,False,9,"Currently working as DE in a small team with the only technical person. We have bunch of GUI based ETL jobs and python pandas files( eat up lot of memory) that are triggered using cron on an EC2 server. I have been tasked with revamping the workflow orchestration. Two reasons for doing this: alerting users in case of job failures and better/efficient workflow management. I looked at few solutions and found two possible solutions: Airflow or AWS lambda. Which one among these would be the best (or any other)
P.S: trying to move from DE to SDE/SWE, so want to implement a solution that can help me highlight on my resume for SDE/SWE positions",False,1.0,https://www.reddit.com/r/dataengineering/comments/14pxq7v/best_workflow_solution/,dataengineering
What best practices get / extract data that frequently update with a little big data,14qbe2n,azharizz,1688466192.0,,False,True,Help,4,False,1,"So, in my work there is data we get / extract data from Postgresql, MySQL, etc with estimated schedule every 15 minutes with data we get incremental 1 day. The data that we get it's more than million. why 15 minutes, it's because we need to serve data fastly but not realtime to other transformation process. I want it stay incremental 1 day by incremental column.

We extract the data by using query in source db. Before i getting into airflow, we used pentaho / kettle for this ingestion / extraction. But, the problem when we extract data on airflow using pandas + psycopg to send it into aws S3 is getting slow and consume so much resource in cpu than im using pentaho before that more consume of memory and fast for getting data. I guess this is cause of Java Connector in pentaho that make it run faster (?) i dont know.

This is for old pipeline:
Postgresql (Source) -> Pentaho Extraction (JDBC ?) -> Local System (CSV) -> AWS S3 -> Redshift

New Pipeline : 
Postgresql (Source) -> Airflow Extraction (Pandas + psycopg) -> Local System (CSV) -> AWS S3 -> Redshift

Old Pipeline is faster, and resources consume less. When i tried full refresh on pentaho transformation it's never getting error about high memory, but when i tried airflow with pandas and psycopg it's get error high memory (memory leak).

So, i'm still want to use Airflow for this data extraction and will remove pentaho. But, i don't know what the best practice if we get data that having schedule will update every 15 minutes. Is there any Open Source tools or libraries that help this problem ? 

I Have tried but it's still not solving the problem:
- Asynchronously get data using AIOPG / aiomysql but there is not really much improvement or nothing is improved by speed.
- Apache Spark, first when i call SparkSession.builder.getOrCreate() every run schedule it's take time consuming about 30 - 60 seconds, secondly sometimes spark is fast but sometimes it's slow.
- Airbyte, first it's good using JDBC and good when tried for full refresh. But, airbyte is not based query, because i dont want to get all the column table, and sometimes there is more than 2 incremental column so i preffered query based instead. The dbt transform in airbyte make it confuss me because is it like we get all the data with all the column first then we transform it ? so it's kinda ineffective or maybe time consuming

I'm newbie in data engineer. That's why i want to know what maybe best practice for extraction data that schedule every 15 minutes with big data ? I still want to get data with incremental 1 day, fastly, less consume resource or getting no error when trying with full refresh data. (Full refresh data is not every 15 minutes but it trigger manually) 

What about your company data extraction method ? is anything wrong with my trial in async, airbyte, or spark that maybe im skipped ?
Thanks A lot",False,1.0,https://www.reddit.com/r/dataengineering/comments/14qbe2n/what_best_practices_get_extract_data_that/,dataengineering
Not using window functions?,14pmlt0,data_questions,1688399089.0,,False,True,Interview,46,False,24,"Has anyone interviewed DE candidates and — in response to them answering a SQL interview question with a window function — asked them how to solve it without the window function? If so, why? To me, that doesn’t seem like a value added constraint to add to the interview.",False,0.89,https://www.reddit.com/r/dataengineering/comments/14pmlt0/not_using_window_functions/,dataengineering
Reducing Data Platform Cost by $2M,14pk4p6,Junior-Salary-6859,1688393434.0,,False,False,Blog,1,False,27,,False,0.97,https://engineering.razorpay.com/reducing-data-platform-cost-by-2m-d8f82285c4ae,dataengineering
How to monitor business objects through EDA?,14q82kg,zenbeni,1688455444.0,,False,True,Discussion,1,False,1,"We have many streams and main business objects can be found in different flavours in different datastores (data mesh kinda architecture). The problem is ensuring that data quality stays good along the way as we can have troubles finding the root cause of bad / missing data. (not published event or bad consumed event, lag?)

What would be a good way to monitor this, I'm feeling tracing, logs & supervision are very tech solutions that are great for globally ensuring that everything is working, not so great for focused data supervision (we do not have very exploitable logs so fetching other app logs and yours through 1 splunk query is not possible), they can't really help when you particularly want to check quickly the sanity around 1 business object (it requires devops, dev access, takes time). 

I'm thinking that building a business object timeseries (dynamodb or cassandra for instance) for storing latest footprints pushed at end of processes of said business objects (id 1, gone stream A ok at timestamp t1...), so I could easily fetch by API what happened lately on these elements accross multiple apps & datastores, would it be a good way or I am actually reinventing the wheel somewhere? The idea of having such timelines would also allow real time data health checks (for some random picked ones for instance) which I believe would be very valuable. Did anybody build something similar or could cover this data quality check?  
",False,1.0,https://www.reddit.com/r/dataengineering/comments/14q82kg/how_to_monitor_business_objects_through_eda/,dataengineering
Which ETL tool for this simple use case?,14q75x4,Rogitus,1688452484.0,,False,True,Help,5,False,1,"There are multiple data sources, from API to live stream data (e.g. mqtt).. I have to take this data, transform it and send it to another server microservice. 

I'm looking for a tool which is easy to use (also people without CS background are in my team), open source and easy to deploy in AWS (but I guess every tool is.. idk).. 

I would like to push for Kafka since it's smth I would learn to use. It doesn't seem really easy to deploy though, so I can imagine they could stop me on that. 

My questions:

- is kafka suited for such a use case? Or maybe overkill? 

- what other tools could I use? What about Airflow?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14q75x4/which_etl_tool_for_this_simple_use_case/,dataengineering
How much DSA is required for a data engineer?,14ph895,Old-Article6420,1688386130.0,,False,True,Help,35,False,27,"I know basic python. I can connect to different databases fetch data, transform it and load to other database. 
I want to become good at python. So, my question would be if DSA is even required for becoming a data engineer and if yes then how much?",False,0.93,https://www.reddit.com/r/dataengineering/comments/14ph895/how_much_dsa_is_required_for_a_data_engineer/,dataengineering
Hello DE people Need your help to solve a simple problem as part of the assessment.,14q59ct,Loser_Lanister,1688446459.0,,False,True,Help,2,False,0,"Given a data set A for training an AI model, we would like to have a function/method that subsamples A and yields a subset B in a random manner, which will be used for mini-batch selection. Please write a function in pseudo-code which receives a set A and outputs a set B.”  
For the completion of the task, please consider the following:  


1. edge cases,
2. how the function can be unit tested,
3. responsibility of the function,

&#8203;

    Ans:
## I thought of one approach:
    random_A = random.shuffle(A)
    B = random_A[0:len(B)]    

Need your help to get an idea to different approach, unit testing this.",False,0.43,https://www.reddit.com/r/dataengineering/comments/14q59ct/hello_de_people_need_your_help_to_solve_a_simple/,dataengineering
Implementing and using SCD Type 2,14pnufd,piri9825,1688401917.0,,False,True,Help,9,False,7,"I currently have set up a pipeline which uses SCD Type 1 and overwrites whenever there is a change to Table A. Table A contains a mapping which is used to calculate an important business metric. When new mappings are added/removed (which happens every few months or so) the new mapping is used historically and makes the historical records of this metric wrong in Table B.

I can see that the solution to this issue is to use SCD Type 2 (recording the start and end dates and apply the correct mapping for any given date) but can't figure out how to do this practically. I could work out how to apply SCD Type 2 as there's some info online about doing so but haven't been able to find much on actually utilising this feature.

The calculation for the metric is already somewhat complex due to some nuances that needed to be included and I can't imagine handling applying the mapping to specific dates on top - especially given that handling of nuances involve messing around with the mappings to generate extra data points that are to be used in the business metric calculation.

I appreciate that this description is somewhat abstract but I was wondering if there are any good resources on actually applying SCD Type 2 into data transformations or is it just a matter of playing around with the code until I can solve all the issues and get the metric to calculate properly?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14pnufd/implementing_and_using_scd_type_2/,dataengineering
How much python do i need to know to be a DE? Currently on day 28 of 100 days of Code. Should I finish it ? or focus on DE specific skills ?,14pekpd,benboga08,1688377921.0,,False,True,Career,33,False,20,"Good day DE folks, I hope your are good today.

BACKGROUND:

A while back, I decided to learn python via the 100 days of code by Angela Yu. Currently , I am in day 28 of said course learning about GUI via tkinter.

QUESTION:  

How much python do i need to be DE? Should I finish the 100 day course or should I focus more in DE related skillset rather than more general python knowledge.

&#x200B;

Thank you.",False,0.81,https://www.reddit.com/r/dataengineering/comments/14pekpd/how_much_python_do_i_need_to_know_to_be_a_de/,dataengineering
Hired as DE but working as Analyst. WWYD?,14pkrzu,Marawishka,1688394906.0,,1688509625.0,True,Help,27,False,6,"Hello, everyone! A few months ago I started to learn both Python (Pandas, PySpark) and SQL on a 2 months bootcamp.
  
There, I learned the basics of SQL and Python, and we did an end to end on Azure with Databricks and a Kaggle dataset in the form of an SQL database for the end of the course.  
  
I started working as a data engineer for a consulting group associated to the people on the bootcamp, since I did a great job and only a few were qualified for the job: me being one of them (I really put a lot of effort and endless hours for that purpose).That being said, I'm galaxies far from being a proficient DE.  
  
So, I was assigned to an awful project for a company that works on prem, with awful xlsx files generated by an outdated software, and I'm in charge of giving support of Power BI. A few weeks ago I knew close to nothing about Power BI, dax and all the stuff involved, so that's really unfair.  
  
No one is judging my progress, I'm doing well so far, but that's not what I signed for: power Bi dashboard support and documentation basically. Apart from being awful it is not giving me any helpful experience as a DE.  
  
So, for people with experience, is this playing by the rules? Is this normal? What would you do?

Edit: Don't get me wrong, I'm really grateful for this opportunity, I'm just trying to get the most of my time here.",False,0.72,https://www.reddit.com/r/dataengineering/comments/14pkrzu/hired_as_de_but_working_as_analyst_wwyd/,dataengineering
Doing DE but designation is Lead Analyst. Will that affect next job search?,14pol99,ppdas,1688403650.0,,False,True,Help,6,False,3,"Hi DE community, I have a question that's bothering me for a while. While I am a Lead Analyst, but I mostly do DE and Analytics Engg work...tech stack being Snowflake, dbt, Airflow.... Will my designation affect my future job search as a Data Engineer.",False,0.8,https://www.reddit.com/r/dataengineering/comments/14pol99/doing_de_but_designation_is_lead_analyst_will/,dataengineering
I need reviews on the certification that would mark my self-learning journey into the DE world.,14pai2s,VastDragonfruit847,1688363966.0,,False,True,Help,9,False,22,"Hello!  


I've been stuck in tutorial hell forever. It was bad because I didn't have anything valuable to show for at the end. DE feels more like tap dancing with tools than tutorials. Hence I've decided to get certified as [Microsoft Azure Data Engineer Associate](https://learn.microsoft.com/en-us/certifications/azure-data-engineer/?ns-enrollment-type=Collection&ns-enrollment-id=40yztymq0o5p3w).   


About me :   
I already know Python at an intermediate level. Have worked with SQL (course curriculum + Leetcode/Stratscratch). I'm working on Tableau projects as well. I'm interested in preparing eye catchy visualizations but I feel like that would not be enough. It would take me some time to reach that level but it's something that I'd do on a daily basis with lesser motivation too. I would need to have something concrete, foundational, and more importantly ""showable"" running in parallel. I think data engineering concepts might prove very handy as a skill.  


Why this post?  
There must be people who have been on a journey like mine. I'm trying really hard to enter the job market but not getting any calls. I was wondering if the wise people on this subreddit could guide me in this regard. Point me out to the better ways that they've learned themselves. How would you do it as a beginner if you did it all over again? ",False,0.92,https://www.reddit.com/r/dataengineering/comments/14pai2s/i_need_reviews_on_the_certification_that_would/,dataengineering
Creating a Data Pipeline to Extract Data from Azure AD and Visualize it in Power BI.,14pvhqy,Expert__Bat,1688419801.0,,False,True,Discussion,4,False,1,"Hi, I want to   
automate the refresh of data pulling from Azure AD  and build the pipeline that would visualize the analysis in Power bi   


should I use Synapse or is their a better solution?  
 ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14pvhqy/creating_a_data_pipeline_to_extract_data_from/,dataengineering
Which Cloud certificate for foundations will be the best in terms of growth and applicability for data engineering and Machine learning engineering roles?,14pnzv7,AnishNehete,1688402276.0,,False,True,Discussion,7,False,2,"Confused between Azure , AWS and GCP and I'm getting mixed Opinions. Any guidance would be appreciated.",False,0.75,https://www.reddit.com/r/dataengineering/comments/14pnzv7/which_cloud_certificate_for_foundations_will_be/,dataengineering
Has anyone interviewed at Specsavers?,14ptj64,dildan101,1688415376.0,,False,True,Interview,2,False,1,"Hey all,  


I have an interview with Specsavers for a Junior Data Engineer role, I'd like to know if anyone else here has gone through their process and can give some insight into what I should prep for.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ptj64/has_anyone_interviewed_at_specsavers/,dataengineering
Archive Your Reddit Data While You Still Can,14pe0g4,wagfrydue,1688375974.0,,False,False,Blog,4,False,6,,False,0.88,https://xavd.id/blog/post/archive-your-reddit-data/,dataengineering
Dynamic SQL to get Column Metrics,14p6pjy,Touvejs,1688352121.0,,False,True,Open Source,7,False,18,"I often times work with client data with questionable documentation and it's always a pain writing ad-hoc SQL to check out what columns are available, what's populated, and previewing each table to see what the data looks like. So I've this idea knocking around in my mind for a while, ""why not just write some dynamic SQL to look at the schema and then use that schema table to generate queries on each column to get some metrics?""

This weekend, I decided to tackle this problem (no, I don't have any friends). So I wrote some scripts using plpgsql for postgres-based databases (postgres, redshift, oracle?) and snowflake that would solve this problem for me. 

Feel free to view the scripts here: [https://github.com/Touvjes/SQL-utilities](https://github.com/Touvjes/SQL-utilities)

Essentially what these scripts do is 1) create a metadata table which includes one row per column in specified data table and 2) use variant-specific control flow structures to loop through the metadata table to execute queries using the schema+table name in each metadata table row to identify for each column: an example value, the total non-null count, the total distinct count, and the percent populated. 

Naturally, one could write such queries by hand, but its tedious if you're often being given dozens or hundreds of table and stakeholders ask ""can we do x,y,z analysis?"" and one has to go and figure it out why invariably, the data doesn't allow for such analysis.

 What these scripts allow you to do is take the input of either a single table (and in the future, a whole schema, or a whole database) and answer the questions: 

* what does the data look like in each column of each table? 
* how variable are each of those  columns (i.e. # distinct values) 
* how reliably is that data populated?

One might ask, can't we just connect to a database with python/java/c# and use an ACTUAL programming languages to generate the sql to generate the statistics and thereby preclude the necessity of using obscure, under-documented, borderline-masochistic sql-variant-specific scripting languages?  (Where's the challenge in that?)

The answer to that is Yes, unless the answer happens to be No. 

By that I mean that in theory, you certainly can-- and to be fair, I'm sure there are a plethora of data analysis/data mining software solutions that solve this exact problem. However, in practice, especially when working with client data, such solutions are not always feasible. Therein lies the impetus for my endeavor to develop native solutions. 

Note that with the framework in place, it is fairly easy to follow the structure given in the scripts to add and include your own metrics if wanted. A word of fair warning though-- these scripts execute a query that generates a new query for each column, which in turn generates a query for each metric, most of which will result in a full table scan. Needless to say, this is FAR from optimized. 

Comments, constructive criticism, and contributions are welcome. I will probably eventually  write equivalent scripts for Mysql and T-sql next time I do projects using those variants. Else, If someone thinks this is possible in purely vanilla ansi sql, feel free to knock my socks off with a demonstration of that. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14p6pjy/dynamic_sql_to_get_column_metrics/,dataengineering
Any real intersect between Data Engineering and Web Dev?,14po7bc,SeriouslySally36,1688402757.0,,False,True,Discussion,7,False,1,"Besides the obvious ""Data Engineers are Software Engineers"" and ""My Website uses a Database"".",False,0.67,https://www.reddit.com/r/dataengineering/comments/14po7bc/any_real_intersect_between_data_engineering_and/,dataengineering
Is there a better way for this pipeline?,14pl3tw,New_Introduction_154,1688395654.0,,False,True,Help,8,False,1,"Hello all!  


So, there's a requirement that requires me to group and filter data from multiple files stored in separate ADLS storage accounts.  


The way I have thought of it is to basically add the datasets in ADF studio and then create a data flow with filter/group by activity, and then add that data flow to a pipeline.

  
I was wondering if there was a better way of doing it?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14pl3tw/is_there_a_better_way_for_this_pipeline/,dataengineering
Following tools to learn considering work experience,14pkjk7,Koxinfster,1688394376.0,,1688394574.0,True,Career,6,False,1,"Hello guys,

In the past 2 years I've been working in a start-up and developed different data related skills, the tasks being really split among  different parts, fact that helped to adapt & be flexible on learning different tools.

Besides, Python & SQL skills, other stuff that I learned are:

\- Google Cloud: from cloud storage, big query, IAM roles, cloud functons, postgres DBs to running VM's and setting up webservers for different tools such as airflow;

\- Terraform:  Setting-up & manage cloud infrastructure via terraform, knowledge of locals, outputs, workspaces, modules, core stuff when it comes to terraform.

\- FastAPI: Develop rest endpoints & unit-tests with pytest, knowledge of sql alcemy, crud, schemas, models, etc.

&#x200B;

I am planning to learn new technologies that would fit / complete my current knowledge. The thing is that i have the feeling that there the choices are quite large. I tried to check different job postings to see what they requests. I found on many scala, spark, snowflake on some, but I am not sure if that's the answer.

&#x200B;

Considering the above, based on your experience and current vibe in the data related work, what are some stuff that i should consider learning or direct to? If you would have some sources for courses or something you found really valuable, that would be also highly appreciated.

&#x200B;

Thank you for your time!",False,0.67,https://www.reddit.com/r/dataengineering/comments/14pkjk7/following_tools_to_learn_considering_work/,dataengineering
Seeking Guidance: Roadmap for Becoming a Competent Junior Data Engineer in 3 Months,14oqn1f,Kratos_1412,1688310603.0,,False,True,Help,28,False,69," I'm in need of your expertise and guidance as an aspiring data engineering student. Despite having knowledge in Python, SQL, Scala, Hadoop, Spark, and Airflow, I'm feeling overwhelmed and confused about where to start. Can you provide any advice or resources to set me on the right path?

If there's already a similar roadmap or guide available, please let me know where I can find it. Otherwise, I'd love to collaborate with you all to create a roadmap specifically tailored for junior data engineers with a similar background.

To kickstart the discussion, here are a few questions:

1. Essential skills and knowledge areas: What are the key competencies that a junior data engineer should focus on, considering prior knowledge in Python, SQL, Scala, Hadoop, Spark, and Airflow?
2. Recommended programming languages, frameworks, or tools: Are there any specific technologies that would complement my existing skill set and boost my growth as a data engineer?
3. Top resources for self-study: Can you recommend any books, online courses, tutorials, or other learning materials to further enhance my skills and knowledge?
4. Projects or exercises for hands-on experience: What practical projects or exercises would you suggest to gain hands-on experience and build a strong foundation?
5. Staying up-to-date with the latest trends: How can I stay informed about the latest advancements and trends in the field of data engineering?

I'm dedicated to investing my time and energy into this journey and excited to learn from your experiences and insights!

Thank you in advance for your support. Let's collaborate and create a comprehensive roadmap for aspiring junior data engineers!",False,0.89,https://www.reddit.com/r/dataengineering/comments/14oqn1f/seeking_guidance_roadmap_for_becoming_a_competent/,dataengineering
Data Profiling Query,14parwo,gaimsta12,1688364848.0,,False,True,Help,3,False,3,"I'm going to be honest - I have little experience in data engineering. I'm a mathematician applying for a data science position and they've asked a series of questions for a case study, with one surrounding data characteristics. In short it asks the following:  
Some languages are case sensitive, some not. Some sources contain non-ascii characters. These cause issues - etc.   
Outline a strategy and approach to:  
1 Detect source characteristics for contextual data  
2 Decide whether to enforce case standardisation (upper/lower/other)  
3 If standardisation is enforced, present the original representation in subsequent reporting  


I'm not looking for just an answer, they'll certainly query the approach and I need to back up my bullshit. Can I please be pointed in the right direction and if anyone knows some useful resources I'm all ears. thank you!",False,0.81,https://www.reddit.com/r/dataengineering/comments/14parwo/data_profiling_query/,dataengineering
Is there a platform agnostic solution to ingestion new files as they land in data lake acting as a raw layer?,14pis5o,andyby2k26,1688390197.0,,False,True,Help,1,False,1,"I'm working on a few pipelines for my own development and want to work with landing JSON API responses into a data lake (I'm using GCP Cloud Storage + BigQuery). As per the title, what is the ""correct"" method to only ingest new files?

I work with Snowflake in my actual job and through Snowpipes + COPY INTO you can define files to only be processed once and loaded into the data warehouse, but this is a solution specific to Snowflake. Is there a method that works cross platform/provider? Or am I getting hung up/confused about something which isn't actually an issue?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14pis5o/is_there_a_platform_agnostic_solution_to/,dataengineering
Legacy system assessment model: Retire or not to retire?,14pho9o,Competitive_Speech36,1688387376.0,,False,True,Blog,1,False,0,"My team and I have been re-building our company's data architecture. In the process of doing so, I got together six key principles to transforming data architectures and thought I would share them, as a strong data architecture is crucial for businesses looking to stay competitive in the digital landscape, as it improves decision-making, time to market, and data security. When executed with efficiency, a resilient data architecture unleashes unparalleled degrees of agility. 

Balancing contemporary software with outmoded systems that bear a long list of constraints and issues presents a complex predicament for CTOs and CIOs. In this composition, I delve into these questions:

* Establishing if the system should be labeled as legacy;
* Merits and demerits of retaining legacy software;
* Evaluating disparate types of systems’ traits;
* Proposing a blueprint for pinpointing the correct answers to legacy systems.

### Recognizing a Legacy System

To ascertain if stakeholders regard the system as legacy or otherwise, you can question yourself using the prompts below. A positive or negative response to the system will suffice. Bear in mind, some of these are subjective and you might prefer to answer Maybe or Unsure:

* Does the system play a vital role in business operations?
* Is the system dated? Was the latest update significantly in the past?
* Has the system been modified to fit organizational objectives?
* Is the system deteriorating as modifications are implemented?
* Do maintenance costs escalate as modifications are implemented?
* Is the system based on antiquated languages?
* Is the system’s documentation scant or nonexistent?
* Is the system’s data management subpar?
* Is the system’s support capacity restricted?
* Does the system lack the architectural design to adapt to upcoming requirements?

If more than half of your responses are positive, you’re dealing with a legacy system. So, what comes next?

### Assessing the Pros and Cons of Legacy Systems

Before you choose to modernize, transition, or entirely discard a certain legacy software, you should comprehend its pros and cons.

The advantages include:

* High costs of modernization compared to maintaining the legacy system;
* Reliability and familiarity associated with legacy technology;
* Traditional applications are generally fully merged with corporate operations and continue to function efficiently, proving their utility over time;
* Modernizing software could halt business operations, while legacy systems guarantee smooth, uninterrupted procedures.

The disadvantages are:

* Talent challenges – scouting for new professionals to sustain and support obsolete technology is increasingly like chasing a mirage;
* Modern business processes often require automation, which legacy systems don’t accommodate;
* Incompatible and outdated procedures;
* Legacy systems trigger compatibility issues;
* Legacy systems may not receive security updates, leaving them exposed to external breaches that hinder the system’s operation.

Depending on their priorities, CTOs from different companies might weigh the cons more than the pros and opt to retain the old tech or vice versa.

### Legacy System Evaluation Model

Several evaluation models exist for software, each spotlighting different features. The Open University's research group suggests a model derived from pre-existing ones that merges business and technical aspects with modern architectural features and organizational considerations. This creates a broader, unified method that acknowledges the real-world complexities of legacy systems.

The model's outcome can then be depicted on a decisional matrix that indicates a proposed solution. (See Image1 I attached).

The attributes you need to evaluate, and their corresponding values to locate your system on the matrix, are as follows:

For every attribute, decide if the response is positive or negative in the legacy system's assessment. Then, assign a value from very low to very high (from 1 to 5; don’t know is 0).

Business Attributes:

* Financial worth: Market worth, Profitability ratio, IRR;
* Data worth: Proportion of mission-critical archives, Proportion of application-dependent archives;
* Usefulness: Range of business function coverage, Current usage frequency, Metrics for customer/user satisfaction;
* Specialization: Proportion of highly specialized functions, Proportion of generic functions.

Technical Attributes:

* Maintainability: Amount of code lines, Function points, Control flow, Knots, Cyclomatic complexity, Rate of dead code;
* Decomposability/architecture: Architectural modularity, Proportion of modules with distinct concerns, Expandability, Interoperability, Architectural style, Consumption;
* Degradation: Backlog growth, Defect rate increase, Response-time increase, Maintenance time per request increase;
* Obsolescence: System age, Operating system version, Hardware version, Technical support availability, Security, Legality, System evolution required for business goals.

Organizational attributes:

* Internal development & maintenance;
* Outsourced development & maintenance, 
* Technical maturity, 
* Commitment to training, 
* Skill level of system support, Response to change

To convert the business attributes and technical attributes into final values, the authors use the following equations: (See Image2).

The combined values can then be plotted on the decision matrix from above that indicates a recommended solution. 

For more articles like this, please visit my blog: [https://ainsys.com/blog/2023/02/08/legacy-assessment/?utm\_source=linkedin&utm\_medium=social&utm\_campaign=data\_engineering&utm\_content=legacy\_systems&utm\_term=ITarchitecture](https://ainsys.com/blog/2023/02/08/legacy-assessment/?utm_source=linkedin&utm_medium=social&utm_campaign=data_engineering&utm_content=legacy_systems&utm_term=ITarchitecture) ",False,0.5,https://www.reddit.com/r/dataengineering/comments/14pho9o/legacy_system_assessment_model_retire_or_not_to/,dataengineering
Looking for Alternatives to SPSS/SAS Formats for Storing Complex Metadata - Any Suggestions?,14phkrg,BudgetAd1030,1688387099.0,,1688404123.0,True,Help,9,False,1,"Hello everyone,

In my research group, a substantial amount of data is stored in SPSS's SAV or SAS's sas7bdat/sas7bcat formats. While these formats work well for those using SPSS or SAS, due to their comprehensive metadata handling capabilities (variable labels, value labels, missing value definitions, multiple response sets, etc.), they create significant challenges when trying to utilize other programming languages or tools for data analysis and manipulation.

To increase interoperability and efficiency within our group, I'm searching for an open-source, language-agnostic format with capabilities similar to the SAV/sas7bdat/sas7bcat formats, specifically the ability to store complex metadata.

I'm aware of libraries like ReadStat, but I've experienced problems when reading files or encountered incorrect or poor conversions of complex data types, which then need to be corrected afterwards. Interestingly, I've noticed it's often easier to convert data to the SAV/sas7bdat/sas7bcat formats than it is to convert from them.

Are there any fully-featured, open-source, file-based formats for tabular data that are language-agnostic and capable of storing complex metadata like SPSS's SAV or SAS's sas7bdat/sas7bcat formats?

If a direct equivalent doesn't exist, how have you managed to work with complex metadata when using these formats? Do you manually perform data type conversion, formatting, and carry the metadata around?

Thank you in advance for your insights and suggestions!

Edit: [Description of metadata information you can specify in SPSS](https://ezspss.com/how-to-define-variables-in-spss/)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14phkrg/looking_for_alternatives_to_spsssas_formats_for/,dataengineering
4 years data analyst/bi experience. want to shift into data engineering (UK) - possible?,14oorix,throwawayaccountdata,1688305693.0,,False,True,Career,4,False,26,"Hi

Since 2019 to mid 2021 I was a junior Power BI contractor for a financial firm in London, also got exposure to SSAS. From 2021 I've worked in another financial company in London, working mainly on data quality using a tool which has similar components to pipeline dev in SSIS (eg node-like flows importing data, processing and cleaning and then exporting it)

I'm also beginning to get exposure to Python API development and more SSIS work. I'm currently on £35000 pa which I'm told and I think is too low with 4 years financial IT experience, especially in London.

My question is, should I stay for another year and get the MS data engineering certification to help boost my CV and continue getting experience in Python and SSIS? What is the best roadmap to get as much Data Engineering experience as possible while still aiming for a higher salary? How can I leverage my non-DE-but-still-data experience for a DE position?",False,0.94,https://www.reddit.com/r/dataengineering/comments/14oorix/4_years_data_analystbi_experience_want_to_shift/,dataengineering
How to handle designing a database of quiz questions of different types?,14p41oc,codeyCode,1688344420.0,,False,True,Help,4,False,3,"Let's say there is a database of quiz questions, however there are 3 types of questions: Multiple Choice, Multiple Answer and Likert. 

Multiple Choice and Multiple Answer quizzes will have a question with multiple answers to choose from. This is simple enough for 2 data tables: 1 for the questions and 1 for the answer choices. However, likert, as you may know, does not not have multiple choices. Rather it asks you to pick a number on a scale of 1 - 10, where 1 represents something and 10 represents the opposite (unsatisfied vs satisfied for example). So these questions attributes will include the scale, as well as the two extremes being measured (satisfied vs unsatisfied)

I have two options

Option 1

* Have 1 Questions table with all the questions and a column for ""type"", specifying  if it's MC, MA or Likert
* Then have 2 Answer tables: 1 for the multiple choice and multiple answer questions and one for the likert

Option 2

* Have 2 Question tables: one for the multiple choice/answer questions and one for the likert questions. The one for the likert questions will have the answers as attributes (satisfactory vs unsatisfied)
* Then have one table for the Multiple Choice and Multiple Answers.

I'm not sure which is better or if it matters?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14p41oc/how_to_handle_designing_a_database_of_quiz/,dataengineering
How to read lots of avro files as a of mixed types as a single data frame,14p9wnm,ankurcha,1688362030.0,,False,True,Help,3,False,0,"I want to read a directory of avro files with lots of different schemas into a single data frame with element as a single column.

The default spark-avro uses the schema form one of the files and reads all the files but this is not what I want to do. Ideally I want all the avro messages to be read as GenericDatum or bytearray and into a single column so that I can manipulate them as a single message rather than some merged version of all the types.

I couldn't find a clear way of doing this besides loading the entire file using binaryFile and parsing it.

Is there a better way to do this?",False,0.33,https://www.reddit.com/r/dataengineering/comments/14p9wnm/how_to_read_lots_of_avro_files_as_a_of_mixed/,dataengineering
CI/CD Testing architecture for Spark,14oih18,Vegetable_Home,1688284510.0,,False,True,Help,8,False,24,"Hey all,

We have plenty of Spark jobs, on top of that we are actually going and adjusting spark source code to our needs.

So I would like to design a CI/CD pipeline that runs end to end tests and benchmarking (not unit tests) both on k8s (AWS) and YARN clusters.

My questions/ideas:
1. I should have a YAML config file that will be my single source of truth for all the configuration of the tests (which is version controlled)
2. I should probably use python to for this project.(tried junit and maven, didn't work well due to many JVM heap memory issues)
3. Should I orchastrate that whole suite of tests? Which tool works well with YARN and k8s clusters?



Any others considering/ideas  have I missed?

Also pointers to similar problems that have known solutions/blogs post will be great.

Much appreciated :)",False,0.96,https://www.reddit.com/r/dataengineering/comments/14oih18/cicd_testing_architecture_for_spark/,dataengineering
Is anyone here using Hex or DeepNote?,14ohd98,Comprehensive-Pay530,1688280477.0,,False,True,Discussion,14,False,18,"I'm curious if anyone here is using Hex or DeepNote and if they have any thoughts on these tools. Curious why they might have chosen Hex or DeepNote vs. Google Colab, etc. I'm also curious if there are any downsides to using tools like these over a standard Jupyter Notebook running on my laptop.  


Recently saw Hex also introduced AI features like debugging and writing queries using natural language but like many other tools doing this I believe for complex queries that are generally written the accuracy of these features will be a key challenge has anyone tried it if yes how useful have you found them?",False,0.89,https://www.reddit.com/r/dataengineering/comments/14ohd98/is_anyone_here_using_hex_or_deepnote/,dataengineering
"Created my first Data Engineering Project which integrates F1 data using Prefect, Terraform, dbt, BigQuery and Looker Studio",14nywsi,mortysdad44,1688227988.0,,1688228377.0,True,Personal Project Showcase,27,False,147,"## Overview

The pipeline collects data from the Ergast F1 API and downloads it as CSV files. Then the files are uploaded to Google Cloud Storage which acts as a data lake. From those files, the tables are created into BigQuery, then dbt kicks in and creates the required models which are used to calculate the metrics for every driver and constructor, which at the end are visualised in the dashboard.

[Github](https://github.com/InosRahul/f1-data-pipeline)

Architecture

&#x200B;

https://preview.redd.it/xvffk1orod9b1.png?width=3624&format=png&auto=webp&s=3cd2d18a4939f8b1000f1d572dffe8647fe51133

## Dashboard Demo

  


https://i.redd.it/ukolugwppd9b1.gif

[Dashboard](https://lookerstudio.google.com/reporting/9fd225dd-a9b8-45d9-87dc-7d7dbae0c841)

&#x200B;

## Improvements

* Schedule the pipeline a day after every race, currently it's run manually
* Use prefect deployment for scheduling it.
* Add tests.

[Data Source](https://ergast.com/mrd/)",False,0.98,https://www.reddit.com/r/dataengineering/comments/14nywsi/created_my_first_data_engineering_project_which/,dataengineering
Thinking about a possible career change,14oelqv,Jakeoy,1688270988.0,,False,True,Career,32,False,12," TLDR: I have a few years experience working with the Microsoft power platform/dynamics as a consultant. I want to move to something less client facing and more technical due to stress of consulting and it not matching my personality, I enjoy the technical side of my job most. Could data engineering be a fit?

Hi DE folks. I'm a few years into my career working with Microsoft Dynamics/Power Platform as a consultant and I'm starting to realize I don't want this to be my career long term. My bachelors is in Information Systems but I took 5 or 6 computer science courses in college as well (SQL, Java, data structures/algorithms). I'm trying to think of a possible career switch, something technical and not consulting/super client facing. I don't expect to land a job in my new career anytime soon as I would need to learn whatever it is I choose, so what I'm trying to decide is: what should I spend the next year or two learning so I can jump in when the time is right.

My initial thought was a developer position of some sort (maybe .net) as I already have some development experience at my job/from college and i enjoy writing code. However, after looking at the CSCareerQuestions subreddit it seems like the layoffs have made it near impossible for an entry level/mid level developer to get a job anytime soon.

Things that are important in my decision are

1. work from home, at least to some degree
2. pay. this one is probably most important to me and i want a career where I can continue to level up and not take a big pay hit. I make low six-figures now.
3. Not consulting or a ton of client facing. some is fine, but i don't want that to be a huge part of my job. stresses me out.
4. Want to enter a career path that is growing fast and has some job security with the layoffs going on.
5. I feel like I kind of pigeon-holed myself going into microsoft dynamics specifically. I don't want to do that again and i want to change career paths relatively soon so i don't continue to put myself in a corner.

I have experience building/configuring/extending Power apps/Dynamics crm (mostly model-driven apps built on azure but some canvas apps as well, this is mostly low-code stuff), writing client-side form customizations in javascript, a little bit of backend plugin development in c#. I also have experience managing dev, test, prod environments and moving patches/solutions between them, and controlling access through AD security groups. I've done a small amount of data migration from sql dbs into dynamics, and integrating dynamics with other 3rd party systems via apis. Used Azure DevOps to manage our projects, and control licensing through Azure as well. Also pretty experienced with Power Automate and a little with PowerBI.

With this skillset and the above criteria, could Data Engineering be a good option for me? I like writing code and solving problems, learning new technologies and leveling up in my career.  How steep would the learning curve be for me having some experience with ETL migrations,integrations,SQL? What DE jobs are most in demand and what does the day-to-day look like? Is it mostly technical stuff or would I be getting back into consulting?

If this doesn't sound like the best fit, please recommend some other career paths! Literally any feedback is appreciated.",False,0.87,https://www.reddit.com/r/dataengineering/comments/14oelqv/thinking_about_a_possible_career_change/,dataengineering
Feeling drained and discouraged at my job. Anyone else?,14nshm9,firefighter301,1688209969.0,,False,True,Discussion,34,False,64,"Feel like my current job is slowly draining all my energy and enthusiasm. 

The good parts: decent money, lots of trust and responsibility, good colleagues. 

The bad parts: things break all the time. Debugging is slow, often involves tedious and manual processes. Drowning in bugs and tech debt. Lots of things depend on us, and if a pipeline is on fire, you don't get to go home. Stakeholders come to us with an endless stream of requests, turn quickly impatient. Data producers need us all the time, we have to help them but we also have to educate them and ask them not to screw up the data. Add all the meetings and plannings and SCRUM stuff on top. Then we are expected to build and innovate, but I feel that less than 20% of my time goes into it. Always being dragged down by everything else, spending most of my time on bullshit. In this situation it is becoming very hard to keep up with the industry and experiment with new things and be creative.

Basically the [data engineer burnout report](https://www.businesswire.com/news/home/20211019005858/en/Data-Engineers-Are-Burned-Out-and-Calling-for-DataOps) from Data Kitchen is a reality for me, although I'm not burned out (yet). 

I feel like I don't have enough reference points to judge my situation. My previous job was much better, almost a holiday in comparison, but then again it was boring and nobody cared about data.

How is your experience? How much of this is inherent to the profession vs. my company and situation? Do consultants have it better, since their responsibilities are bounded and they are mostly called to do single projects? Or should I try to get out of DE (which I actually like) and move towards SWE? Is the grass actually greener somewhere?

&#x200B;

&#x200B;",False,0.95,https://www.reddit.com/r/dataengineering/comments/14nshm9/feeling_drained_and_discouraged_at_my_job_anyone/,dataengineering
Data engineering salaries,14p0s2f,Carefull_eater,1688335774.0,,False,True,Discussion,31,False,0,Why are data engineering salaries bad when compared to software engineers for example ? I’m sure we have seen many software engineers with Magen 3 or 5 years of experience making above 200k while data engineers who have experience for 10 or 15 years make 140k or 150k if lucky,False,0.35,https://www.reddit.com/r/dataengineering/comments/14p0s2f/data_engineering_salaries/,dataengineering
How to design data in firebase ?,14og1my,DevMohh,1688275905.0,,False,True,Help,0,False,2,"I am working on fitness app , user can subscribe in app and can go to Gyms,classes, and sessions , gyms and classes have the same attributes in class data model but sessions is different little things and each class has multi sessions , and gym can add session , so my question is how to design these data in firebase ?  make it in one collection or each one in single which is better ? and  should all classes and gyms appears in ui screen",False,0.67,https://www.reddit.com/r/dataengineering/comments/14og1my/how_to_design_data_in_firebase/,dataengineering
Microsoft azure/data engineering certificate ?,14o83pc,ARA-GOD,1688251703.0,,False,True,Career,13,False,5,"Hello guys, i'm a fresh data science masters graduate, i found extreme difficulties finding a job, the market is so competitive.

i thought to enhance my chances to switch to data engineering as i think it's much cooler and has more opportunities than data analysts or other data roles, i will start learning azure with Microsoft  data engineering ecosystem, maybe with power Bi and SQL Server, i have good python skills so learning pyspark won't be hard

i'm thinking of  getting the exam after i finish learning, build some projects just for fun and showcase, build a portfolio and share it alongside with the microsoft certificate + adding to that the masters that i just got

do you think that would be enough ?",False,0.78,https://www.reddit.com/r/dataengineering/comments/14o83pc/microsoft_azuredata_engineering_certificate/,dataengineering
Does the world need a new SQL editor?,14o0hg8,Confident_Reward_387,1688232066.0,,False,True,Help,78,False,14,"Hi everyone,

I am a startup founder and building a AI powered modern SQL editor with following features -

1. Collaborate with your team mates
2. AI powered query generation from plain english
3. AI powered debugging
4. Schedule dashboard/query reruns at a click of a button
5. Integrate and get your data in Slack/Emails

The reason I am building this is I have found most editors like Dbeaver, MySQL workbench are very old fashioned and boring, so the world could use a modern SQL editor.

Let me know your thoughts on this, will anybody buy this product? Will it help increase efficiency of your data team? Would you move to a new editor? If no, then why?

Also would love to know what sort of problems are there with existing editors.

Thanks in Advanced. Go wild.",False,0.6,https://www.reddit.com/r/dataengineering/comments/14o0hg8/does_the_world_need_a_new_sql_editor/,dataengineering
Data Engineer looking to study mechanical engineering,14o8h6h,iGodFather302,1688252684.0,,1688306466.0,True,Discussion,35,False,5,"Hello, I'm a 24 years old with 2 years of data engineering experience in industry (banking, ads companies). Currently, I'm interested in pursuing a mechanical engineering course online, and I would like to know how it is gonna help me and in what ways? I'm not asking as I'm doubting, but more like to have more ideas about what to do with the mechanical course I'm gonna study. I intend to keep working in data industry as I really like it, but I'm also interested in mechanical engineering :D. Also if you have any online course for a full time employee, I would highly appreciate it.

Edit: The reason why I want to study ME is that I intend to work on my personal projects in the near future (designing mechanical equipments) so I thought why not study mechanical engineering to have a good understanding of what I’m about to make and also about choosing the right material and the strength etc.. lot of stuff that go into the making of it

TL;DR I'm a data engineer looking to study mechanical engineer for fun, any course suggestions and advices?",False,0.59,https://www.reddit.com/r/dataengineering/comments/14o8h6h/data_engineer_looking_to_study_mechanical/,dataengineering
This the databricks team? jk Rip.,14o3emp,keeney_arcadia,1688239639.0,,False,False,Meme,2,False,8,,False,0.9,https://www.reddit.com/gallery/14o3emp,dataengineering
Tech blogs about companies running Kafka in production,14nvyxa,dttung2905,1688220307.0,,False,True,Open Source,1,False,17,"Been adminstrating Kafka clusters for a few years now and I absolutely enjoy reading big companies blog on how they manages kafka. Of course, there are resources like Kafka Summit, Current event but I think organising by company ( sorted by year ) will provide a a better idea on how the kafka stack evolves/mature in each company.

Please drop a star if you enjoy the repo and do contribute to it as well !

[https://github.com/dttung2905/kafka-in-production](https://github.com/dttung2905/kafka-in-production)",False,0.96,https://www.reddit.com/r/dataengineering/comments/14nvyxa/tech_blogs_about_companies_running_kafka_in/,dataengineering
Building Feature Pipelines with Apache Flink,14o0wvp,SirOibaf,1688233179.0,,False,False,Blog,0,False,9,,False,1.0,https://www.hopsworks.ai/post/building-feature-pipelines-with-apache-flink,dataengineering
what;s the answer? In my opinion it's either B or E,14ot0aq,No_Conversation_2474,1688316602.0,,False,True,Help,11,False,0,"The code block shown below contains an error. The code block intended to read a parquet at the file path filePath into a DataFrame. Identify the error.  
Code block:  
spark.read.load(filePath, source = ""parquet"")

* A. There is no source parameter to the load() operation – the schema parameter should be used instead.
* B. There is no load() operation – it should be parquet() instead.
* C. The spark.read operation should be followed by parentheses to return a DataFrameReader object.
* D. The filePath argument to the load() operation should be quoted.
* E. There is no source parameter to the load() operation – it can be removed.",False,0.17,https://www.reddit.com/r/dataengineering/comments/14ot0aq/whats_the_answer_in_my_opinion_its_either_b_or_e/,dataengineering
Advice for proper Dashboarding and Visualization workflow,14o0k4n,Emergency_Estate_866,1688232258.0,,False,True,Discussion,8,False,9,"For those DEs who work with dashboarding and visualization tools, I'm looking to shift off of Qlik Sense to Tableau or some other viz tool. From my understanding, the load script in QS is pretty unique such that it's a BI tool that encourages in-tool transformations. On the other hand, I've read that something that should be avoided for Tableau and PBI, etc. For those of you who do heavy transformations before exporting/connecting data to the viz layer, what are some of your methods? 

Do you handle all biz logic in the transform phase, do you create views that are connected/queried by your viz tool? Interested in learning a different approach from my usual ELT->QS load script.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14o0k4n/advice_for_proper_dashboarding_and_visualization/,dataengineering
Monthly General Discussion - Jul 2023,14nylwl,AutoModerator,1688227248.0,,False,True,Discussion,44,False,9,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14nylwl/monthly_general_discussion_jul_2023/,dataengineering
Introducing English as the New Programming Language for Apache Spark,14nl01c,OverratedDataScience,1688184028.0,,False,False,Blog,22,False,77,,False,0.93,https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark,dataengineering
Where can I find the ANSI SQL specification,14o2z19,finlaydotweber,1688238473.0,,False,True,Help,3,False,2,"I googled and the place I found looks like it is being sold

https://webstore.ansi.org/standards/iso/isoiec90752023-2502159?source=blog&_gl=1*el8mb2*_gcl_au*OTE5ODA2ODg0LjE2ODgyMzgxNjg.&_ga=2.257274435.991959738.1688238167-796123166.1688238167

Which I find strange. Anywhere I can find the specifications? I would have thought something like SQL will have an open and free specification available for everyone.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14o2z19/where_can_i_find_the_ansi_sql_specification/,dataengineering
What is record container class?,14o1wyp,Hiddenlevels_,1688235731.0,,False,True,Interview,1,False,1,Can anyone please explain record container class in sqoop?,False,1.0,https://www.reddit.com/r/dataengineering/comments/14o1wyp/what_is_record_container_class/,dataengineering
Analyzing the Data Engineering Discord Server with DuckDB,14ngd5z,Mission-Sector-1696,1688169877.0,,False,False,Blog,8,False,24,,False,0.93,https://xyz.jackbonatakis.com/blog/analyzing-the-data-engineering-discord-server-with-duckdb/,dataengineering
Flink CDC / alternatives,14npfpt,zakpaw,1688199005.0,,False,True,Discussion,11,False,6,"Does anyone have experience with the flink-cdc-connector project? I'm trying to determine whether it's production ready. What are the advantages and disadvantages of using it, compared to deploying Flink and Debezium separately?

I'm considering this solution mainly due to its simplicity. My source database has a total volume of less than 100GB (will probably grow in the future), I need real-time data for fraud detection. Perhaps there are alternative technologies available that I've overlooked?

The datalake is supposed to serve as the main storage, while MySQL and MongoDB are used for latest webapp data only. Any insights or suggestions would be greatly appreciated.

All the assumptions were imposed by the backend team.",False,0.88,https://www.reddit.com/r/dataengineering/comments/14npfpt/flink_cdc_alternatives/,dataengineering
Dumbest requests from end users or code you’ve seen.,14n7vya,Lost_Source824,1688149044.0,,False,True,Discussion,88,False,53,"It’s definitely been a day so I wanted to poll the community, what are some of the dumbest/most frustrating requests you’ve received from end users or dumbest queries etc from coworkers? Can’t wait to hear these responses.",False,0.92,https://www.reddit.com/r/dataengineering/comments/14n7vya/dumbest_requests_from_end_users_or_code_youve_seen/,dataengineering
How valuable is the Databricks Associate Data Engineering certificate in finding jobs?,14nmdx4,traderdrakor,1688188477.0,,False,True,Career,11,False,7,Title,False,0.89,https://www.reddit.com/r/dataengineering/comments/14nmdx4/how_valuable_is_the_databricks_associate_data/,dataengineering
"Using SQL inside Python pipelines with Duckdb, Glaredb (and others?)",14n1xjw,lackbookpro,1688135234.0,,False,True,Discussion,16,False,46,"Most of day to day is working with python scripts (with a smattering of spark, pandas, etc) with pipelines moving data to/from Postgres/SQL server/Snowflake. The team I'm on is very comfortable with python, but we're exploring using duckdb or glaredb in some spots for data transformation, both for performance and how well sql maps to these transformations.

We're still hammering out what exactly this would look like, but I thought we could get some outside opinions on using either of these projects in our pipelines. Concretely, has anyone introduced either of these projects into their pipelines, and how did that go? Any pitfalls?

For reference:

Duckdb: [https://github.com/duckdb/duckdb](https://github.com/duckdb/duckdb) \- seems pretty popular, been keeping an eye on this for close to a year now.

Glaredb: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb) \- just heard about this last week. We played around with hooking directly into snowflake, so that was cool, but I haven't heard of anyone else using it.

Any other projects like this that I'm missing?",False,0.97,https://www.reddit.com/r/dataengineering/comments/14n1xjw/using_sql_inside_python_pipelines_with_duckdb/,dataengineering
Snowflake Summit,14nim8s,vaporandlies,1688176559.0,,False,True,Discussion,2,False,5,"Who was at summit? What do you think was most interesting? What were your favorite sessions? Recordings will be up next week, so I'd love to know what sessions I might have missed in person.",False,0.78,https://www.reddit.com/r/dataengineering/comments/14nim8s/snowflake_summit/,dataengineering
Do you struggle with database user & RBAC management at your org?,14nrhks,agribbon,1688206478.0,,False,True,Discussion,0,False,1,"I often hear that teams struggle with managing users and RBACs, particularly at larger orgs. Does this apply to you or not?

Feel free to add detail in the comments!

[View Poll](https://www.reddit.com/poll/14nrhks)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14nrhks/do_you_struggle_with_database_user_rbac/,dataengineering
My New Job Pays for All My Wanted Certifications. Recommendations?,14n67sd,Emosk8rboi42969,1688145129.0,,False,True,Career,15,False,22,"So I just got my first data engineering position. During the interview I asked about how the company contributes to employees wanting to further their education. They said that they cover all certification exams.

The certs I am thinking of getting:

* Azure DP-203
* An industry recognized Docker/Kubernetes certification
* Maybe a Spark Cert?

What would some of you more experienced DEs do if you were just starting out your career like I am?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14n67sd/my_new_job_pays_for_all_my_wanted_certifications/,dataengineering
How can i improve?,14nc62n,Seyrenz,1688159236.0,,False,True,Career,4,False,9,"I work for a fintech, where my primary responsibilities revolve around data quality, dashboards, and automation.

Given the state of the databases within the company, my dashboards require extensive transformation and data preparation. To address this, I have created a datalake in Google Cloud Platform (GCP) that consolidates all the necessary data sources. Subsequently, I have built a data warehouse using BigQuery to facilitate efficient querying and analysis.

Since the company's database cannot be directly accessed from the Power BI service, BigQuery also serves the purpose of scheduling data refreshes, ensuring up-to-date information for reporting.

To streamline the data pipeline, I leverage Prefect and Python, utilizing automation techniques.

Furthermore, I engage in system development activities, such as creating simple scripts that evaluate whether a client is overdue and sending email alerts accordingly.

I also perform basic analyses, such as cohort analysis and turnover calculation, primarily to present relevant metrics in the reports.

What are your thoughts on my role and responsibilities? How can i improve my portfolio and experience? Do you think my role qualify as a role of a data engineer or would be more of a analytics engineer?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14nc62n/how_can_i_improve/,dataengineering
It do be like that,14n9sfi,udonthave2call,1688153541.0,,False,False,Meme,0,False,9,,False,0.74,https://i.redd.it/a3cwwt9ni79b1.jpg,dataengineering
Clarification on 2NF database normalization?,14np11w,codeyCode,1688197574.0,,False,True,Help,3,False,1," 

I'm newish to database normalization.

I took notes on 1NF, 2NF and 3NF. However my notes for 2NF are confusing.

After re-looking it up, I understand that 2NF means:  


>Each column must pertain to the entire primary key, and not just part of it.

That seems simple enough, however, my notes from years ago seem much more complicated. I wrote something along the lines of:  


>Create  a new table for a column if A) An individual record can have more than  one value for that column or  B) Multiple records can refer to one  particular value in a column.

These  seem like two different rules and I'm wondering what I was thinking  describing the latter one as 2NF, or am I missing something showing they  are the same? Which is the correct 2NF and what does the other actually  refer to?

thanks",False,0.67,https://www.reddit.com/r/dataengineering/comments/14np11w/clarification_on_2nf_database_normalization/,dataengineering
Fivetran vs Estuary.dev,14n8c4y,tomhallett,1688150128.0,,False,True,Discussion,14,False,11,"There was a [discussion post this week about expensive tools](https://www.reddit.com/r/dataengineering/comments/14ltv6p/which_are_the_most_inefficient_ineffective/) where Fivetran came up with some [comments about Estuary being an option](https://www.reddit.com/r/dataengineering/comments/14ltv6p/comment/jpzbbsk/?utm_source=reddit&utm_medium=web2x&context=3).

While it is definitely a newer tool/service, there are a few features which look very interesting: realtime syncs, inline transforms for EtLT (t = estuary, T = dbt) both stateful and non-stateful, cost savings, materializing the same enriched data to multiple places (which starts to function a bit like reverse etl... tbd).

Can anyone speak to [estuary.dev](https://estuary.dev) and their thoughts on it?

Note: I have found estuary's docs to be slightly scattered, but I found these two interviews to be quite good: [DemoHub Youtube interview](https://www.youtube.com/watch?v=ARk7b2_dh_Q),  [data engineering podcast interview](https://www.dataengineeringpodcast.com/estuary-real-time-streaming-data-lake-episode-375), and this [postgres demo video](https://www.youtube.com/watch?v=4VOgmb3Vnts) helpful as well.",False,0.79,https://www.reddit.com/r/dataengineering/comments/14n8c4y/fivetran_vs_estuarydev/,dataengineering
What’s the good hourly bid for freelance data engineer,14nrwmt,Ok_Pick_8431,1688207974.0,,False,True,Career,7,False,0,I have been shortlisted for a freelance data engineer role and going to finalize the rate with HR after the long weekend..what would be the bid to secure this offer?,False,0.33,https://www.reddit.com/r/dataengineering/comments/14nrwmt/whats_the_good_hourly_bid_for_freelance_data/,dataengineering
Now in Snowflake: GROUP BY ALL,14midyu,fhoffa,1688077232.0,,False,False,,84,False,330,,False,0.97,https://i.imgur.com/Kh5unKB.jpg,dataengineering
Is it possible to pull data from Source A to Snowflake using all Snowflake resources?,14nf0en,bay654,1688166283.0,,False,True,Help,14,False,3,"Still a newbie here.

We use Fivetran as a data ingestion tool. But we want to pull data from Source A, which doesn’t have a native connection in Fivetran yet.

Is it possible to schedule a task using snowflake that pulls data from source A’s API, copies it to an internal staging table, create a stream on that table, and use tasks to run that to a production table? Would appreciate the help!",False,0.8,https://www.reddit.com/r/dataengineering/comments/14nf0en/is_it_possible_to_pull_data_from_source_a_to/,dataengineering
How are you organizing data in databricks metastore?,14naf0x,DataDoyle,1688155068.0,,False,True,Discussion,4,False,5,"I am setting up our company's databricks environment and I am wondering how you guys go about naming the catalog.schema.table? 

\- Is catalog by business unit, project, connection? 

\- For bronze/silver/gold is that delineated at the catalog, schema, or table level?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14naf0x/how_are_you_organizing_data_in_databricks/,dataengineering
What's Your Data Strategy? Help us take it to the next level! :),14n38qz,priyasweety1,1688138256.0,,False,True,Discussion,29,False,7,"I'm here seeking your valuable insights and opinions on data strategies, particularly in the context of my organization's current situation. We have made some progress so far, but we're eager to explore what more we can do to enhance our data practices.

Here's a summary of our current data setup: We have sourced data from multiple channels, applied cleansing techniques (such as duplicate removal), and implemented SCD2 (Slowly Changing Dimensions) in our data layer. Our tech stack includes AWS, Glue, DBT, and Redshift, where the final cleansed data resides.

&#x200B;

1. What additional steps can my organization take to improve our data strategy?

&#x200B;

2. How can we maximize the value of our cleansed data in Redshift?

&#x200B;

3. What are open sources available to add more value to the data, infra , DevOps, etc..

&#x200B;

4. Are there any cost-effective solutions or techniques that we should consider implementing to improve data quality, data governance, or data management? I.e Open Source

&#x200B;

5. Data observability, Precise Alerts, and notifications.  


6. AI on Data and analytics. 

I'm good to hear about your experiences, suggestions, and success stories related to data strategies.

&#x200B;",False,0.64,https://www.reddit.com/r/dataengineering/comments/14n38qz/whats_your_data_strategy_help_us_take_it_to_the/,dataengineering
(P.S )these are for a bachelors degree,14nezz0,Carefull_eater,1688166252.0,,False,True,Help,4,False,0,"Guys  which do you advice me to do . (Computer science major + artificial intelligence specialization) or (Data engineering major + artificial intelligence specialization) , plus who makes more a software engineer or data engineer?",False,0.5,https://www.reddit.com/r/dataengineering/comments/14nezz0/ps_these_are_for_a_bachelors_degree/,dataengineering
Our Days Are Numbered ?,14mkrvv,Smart-Weird,1688083312.0,,False,True,Discussion,97,False,61,"What do you guys think ?

https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark

https://github.com/databrickslabs/pyspark-ai",False,0.85,https://www.reddit.com/r/dataengineering/comments/14mkrvv/our_days_are_numbered/,dataengineering
How can I find a mentor?,14mxlvn,Maxtasis,1688123976.0,,False,True,Career,10,False,8,"Hey everyone!

I'm a 27/M data engineer based in Europe, and I could really use some guidance in my career journey. As an immigrant and the only one in my family in the industry, I often find myself unsure of where to turn for advice.

I'm on the lookout for an experienced professional who has successfully navigated the data engineering industry and can lend me some insights to make better decisions.

If any of you have been in a similar position, I'd love to hear your stories. How did you manage to find that mentor figure who provided you with the guidance you needed?",False,0.72,https://www.reddit.com/r/dataengineering/comments/14mxlvn/how_can_i_find_a_mentor/,dataengineering
Replicating Changing Data for Accurate End User Access in a Lake,14n210w,PencilBoy99,1688135465.0,,False,True,Discussion,3,False,3,"What's the best design for providing access to users to the current state of data in a source in a Data Lake architecture, given a source with keyed /entity data that might change.

**Use Case:**

You are replicating table A from a source (maybe it's an API or table or whatever). Table A has an insert/update date. Rows are occasionally deleted. Table A grows over time.

You do an extract into your lake's object storage each day of the prior day's inserted updated data. So you have an entry for each day's worth of data.

So now different versions of a given entity are in different files. **An end user would have to know this and would have to setup their queries in such a way to make sure they got the latest / correct version.**

**Comments**:

My guess would be that you'd have another tier in your lake, where you'd have a process that gets the latest and organizes that in a different way (e.g., insert date, latest version)

With infinite resources, you could do a full extract every day, but that seems overkill, and the source might not support that.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14n210w/replicating_changing_data_for_accurate_end_user/,dataengineering
7 Data Engineering Projects To Put On Your Resume,14mtvvj,Junior-Salary-6859,1688111327.0,,False,False,Blog,0,False,8,,False,1.0,https://medium.com/coriers/7-data-engineering-projects-to-put-on-your-resume-aeccacc704c3,dataengineering
AwsWrangler Athena Iceberg writes empty rows,14n1jrr,mosquitsch,1688134353.0,,False,True,Help,1,False,2,"Hi there,

I am using awswrangler to write two pandas DataFrames to an iceberg table. For some reason both tables have \~900k rows, most of the rows are empty. The dataframes only contains 200k rows.

I also write this to Impala and there it is fine? What is going on? Is this a bug in awswrangler?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14n1jrr/awswrangler_athena_iceberg_writes_empty_rows/,dataengineering
Introducing the SQL Processing Unit (SPU) | NeuroBlade,14n4uz4,RstarPhoneix,1688141947.0,,False,False,Discussion,0,False,0,First time seeing a company making hardware for running SQL,False,0.5,https://www.neuroblade.com/product/,dataengineering
EMR Serverless & Hugging Face,14n3qek,Afraid_Assistance190,1688139353.0,,False,True,Help,0,False,1,"I use a pyspark script pattern to submit jobs to EMR serverless.

Data Science wants me to pull models directly from hugging face, but my script hangs in production on `os.system(f""huggingface-cli login --token {get_secret()}"")`.  Ultimately I get a request timeout, but this works fine in a jupyter notebook.

Google doesn't give me anything with a search of `emr serverless ""huggingface"" -sagemaker`.  Am I going down the wrong path here?

In the mean time we are planning to host the model in s3 as a work around.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14n3qek/emr_serverless_hugging_face/,dataengineering
Langchain : Concept and Getting Started,14n2bgm,de4all,1688136136.0,,False,False,Blog,0,False,1,,False,0.67,https://open.substack.com/pub/decube/p/langchain-intro-and-getting-started?r=25kgne&utm_campaign=post&utm_medium=web,dataengineering
"Cost trend for tools like Snowflake, BigQuery, RedShift, etc.",14mmqew,DarkSolarLamp,1688088835.0,,False,True,Discussion,17,False,11,"In my org, there's a concern about vendor lock and how they could ramp up the costs once we are dependent on them.  My viewpoint is that that's unlikely given the competition between vendors trying to win market share.  More likely is that rising costs will be due to increasing adoption (it's working so well we move more functions here) rather than unit cost increases (your compute unit was X and now it's X\*1.2)

Wondering if anyone has any actual data, either anecdotal or hard data about pricing trends?",False,0.92,https://www.reddit.com/r/dataengineering/comments/14mmqew/cost_trend_for_tools_like_snowflake_bigquery/,dataengineering
Logical model of OLAP multidimensional database,14n0zo0,czg715,1688133048.0,,False,True,Open Source,4,False,1,"I developed an open source OLAP multidimensional database, its name is EuclidOLAP, and its most unique feature is the automatic real-time aggregation capability, if you use it for data analysis then you can focus more on the semantic model that is close to the actual business, and do not need to think about how the detailed data is aggregated upward.

It also has the following features:

* The deployment is very simple to run;
* It can run stand-alone or in a distributed architecture;
* Using SQL-like language MDX, it has more powerful multi-dimensional query capabilities.

This project can be accessed on [Github](https://github.com/EuclidOLAP/EuclidOLAP).

&#x200B;

Next, you can learn about the logical model of a multidimensional database through the following.

Multidimensional databases organize and present data in the structure of hyper-cubes, and you can imagine that these cubes exist in a multidimensional space, and each cube can be associated with several dimensions. Dimensions are similar to axes, while a dimension represents a specific business perspective, such as the date dimension and the region dimension. The interior of a cube is filled with some quantifiable data, which is called measure.

# Dimension

Dimensions are similar to axes, and a dimension also represents a business perspective. For example, an airline can analyze its turnover data through three business perspectives: classes of service dimension, aircraft models dimension and date dimension.

[dimension](https://preview.redd.it/i0eu7owrs59b1.png?width=793&format=png&auto=webp&s=78d70e645430bc7e4b94df89335fdc369521bc85)

# Member

A member represents a definite value under the business perspective that a dimension represents. For example, there are members such as economy plus, business suite and first class suite under the classes of service dimension, A380 and Boeing 787 members under the aircraft models dimension, and members such as 2022, the first quarter of 2022, and January 2022 under the date dimension. Similar to thinking of dimensions as axes, you can think of members as scales on axes. Members under a dimension have parent-child relationships with each other, and they form a tree-like structure, the tree has a default root member.

[member](https://preview.redd.it/o1thzn3vs59b1.png?width=767&format=png&auto=webp&s=3db9e8a1660ae24bcaa52ce50623041b69a39620)

# Cube

Cubes model represent data marts oriented to business topics, which are associated with several dimensions that describe some business perspectives. You can think of an 1D cube and a 2D cube as a line segment and a plane, and a 3D cube as a Rubik's cube in a Cartesian coordinate system. For cubes associated with more dimensions, they are multidimensional structures that exist in higher dimensional space, but you don't need to imagine what they look like, they are just described from more business perspectives, they have exactly the same characters as 3D cubes, as long as you understand 3D cubes you can understand higher dimensional cubes.

Some data called measure is populated inside cubes, such as revenue and cost.

Here's a cube about an airline, it has two measures revenue and cost, and associates three dimensions classes of service, aircraft models, and date.

[cube](https://preview.redd.it/p17p2glys59b1.png?width=532&format=png&auto=webp&s=6e03dc28046a23477a2e78813410da24ead08bca)

# Measure

For a cube, dimensions representing business perspectives are descriptive information, and measures represent values that can be quantified.

Just as selecting a scale on each axis of a coordinate system determines a point, selecting a member on all dimensions of a cube determines a measure position that represents one or more measures.

In the cube shown in the figure below, the marked measure represents the revenue and cost of business suite service on the A380 aircraft in 2022.

[measure](https://preview.redd.it/s64hogh1t59b1.png?width=810&format=png&auto=webp&s=527460b7deae0340a48f72b8b0b9aa640c11f974)

# Level

Levels represent that members of the same level in the dimension tree structure, for example, the date dimension has four levels, one default root level represents the root member, and the other three levels represent the year, quarter, and month.

[level](https://preview.redd.it/9pmuz3l4t59b1.png?width=953&format=png&auto=webp&s=30648a2aa719729d44f0884da90c66bcb71d5e80)

# Hierarchy

A hierarchy is a tree structure composed of dimension members, and a dimension will have a default hierarchy, and its name is generally the same as the dimension name.

A dimension may have multiple hierarchies, such as the aircraft models dimension, which has two hierarchies, one that categorizes aircraft models by aircraft manufacturer and one that categorizes aircraft models by aircraft size.

[hierarchy](https://preview.redd.it/x559iv88t59b1.png?width=1057&format=png&auto=webp&s=e7e6dce6a0fd2aec786edf5da4f02d3b3c2490e0)

It should be noted that members with the same name under different hierarchies are not the same member, such as \[Airbus\]. \[A320\] and \[Medium\]. \[A320\] is two different members, and a non-default hierarchy member is generally associated with a default hierarchy member.

Date dimension also have two hierarchies, one is the calendar hierarchy and the other is the financial hierarchy. Because some companies' fiscal years do not coincide with calendar years, for example, fiscal year 2022 starts in April 2022 and ends in March 2023, a separate date dimension hierarchy suitable for financial analysis needs to be designed.

The date dimension financial hierarchy is three months behind the calendar hierarchy.

[hierarchy of date dimension](https://preview.redd.it/6le8slzbt59b1.png?width=846&format=png&auto=webp&s=82f1d88b2478c5d9d721a06c6fe020dc1808b31a)

# Dimension Role

An airline builds a cube for analyzing turnover data, which not only correlates the three dimensions of classes of service, aircraft models, and date, but also needs to analyze it from the perspective of flight starting point and flight ending point. Flight starting point and ending point are describing the region, they can be represented by the region dimension, this cube needs to be associated with the region dimension twice, so the concept of dimension role is introduced in EuclidOLAP, and the region dimension plays two dimension roles on this cube, flight starting point and flight ending point.

[dimension role](https://preview.redd.it/2fvvh39ht59b1.png?width=863&format=png&auto=webp&s=2f570a0a3bbd7e7f986dc9bc9fcd46b77a8ecfe1)

For a dimension itself, it does not have a role attribute, and only when the dimension is associated by a cube, the association relationship is a dimensional role.

If a dimension is associated with a cube, it plays at least one dimension role, and if it plays multiple dimension roles, we need to indicate the specific dimension role when analyzing.

Since the hierarchy, level and member models all belong to a specific dimension, when this dimension is associated with a cube to form a dimensional role, these models under the dimensions will also form corresponding role information, which are hierarchy role, level role and member role.

[dimension role 2](https://preview.redd.it/j6x09c0kt59b1.png?width=1316&format=png&auto=webp&s=dd3d2214c072fd8d8b294a47763046fbd82b891d)

# Tuple

A tuple consists of several dimension members associated with a cube, and the following are two tuples:

* (\[Date\].\[2022\].\[Q1\].\[M2\], \[Measures\].\[Revenue\])
* (\[Classes of Service\].\[Economy Plus\], \[Aircraft Models\].\[Airbus\].\[A380\])

A dimension member can also be treated as a tuple, for example: \[Measures\].\[Cost\] can be equated with (\[Measures\].\[Cost\]).

When a tuple contains members of all dimensions to which the cube is associated, it is a complete tuple, otherwise it is a tuple fragment.

(\[Classes of Service\].\[Economy Plus\], \[Aircraft Models\].\[Boeing\], \[Date\].\[2022\], \[Measures\].\[Cost\]) is a complete tuple.

(\[Aircraft Models\].\[Boeing\], \[Date\].\[2022\]) is a tuple fragment.

When performing multidimensional queries, a tuple fragment can be equated to a complete tuple in combination with context information.

If a complete tuple is composed of all leaf members, then it represents a specific detail measure of a cube, and if a complete tuple contains aggregate members, it corresponds to a aggregate measure, which is automatically summarized from some detail measures.

[tuple](https://preview.redd.it/auia6wiot59b1.png?width=1019&format=png&auto=webp&s=e871e122c2132c5bc35ffc89772b97086e9144db)

# Set

A Set consists of several tuples, and here are two sets:

* { Tuple\_1, Tuple\_2 }
* { Tuple\_1, Tuple\_2, Tuple\_3, Tuple\_4 }

A single tuple can also be thought of as a set, for example: Tuple\_1 equivalent to { Tuple\_1 }.

Similarly, since a single dimension member can be thought of as a tuple, a single member can also be thought of as a set, \[Date\].\[2022\].\[Q1\] is equivalent to { ( \[Date\].\[2022\].\[Q1\] ) }.

Set is generally used to define the display of a multidimensional query result, such as the following multidimensional query expression:

    select
    {
        [Date]. [2020],
        [Date]. [2021]
    } on rows,
    {
        [Measures]. [Revenue], [Measures]. [Cost]
    } on columns
    from [Airline A];

It defines two sets, { \[Date\].\[ 2020\], \[Date\]. \[2021\] } will be displayed on row position, { \[Measures\].\[ Revenue\], \[Measures\]. \[Cost\] } will be displayed on column position.

Execute this MDX(Multidimensional Expressions), and you will see the query result similar to the following table:

&#x200B;

|| Revenue |Cost|
|:-|:-|:-|
|2020| 494849380 | 415181710 |
|2021| 590103250 | 497682690 |

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14n0zo0/logical_model_of_olap_multidimensional_database/,dataengineering
New Cloud Question GCP -> AWS,14mqvsc,Emergency_Estate_866,1688101362.0,,False,True,Help,5,False,5,"Leaving my more established company to be the sole DE for a small start-up. My current workflow in GCP is to get data into GCS -> Orchestrate in Composer (Airflow) -> Create and delete staging tables in BQ using the DAG operations + Biz Logic -> Finalized table. 

I'm pretty comfortable with this workflow and I was wondering how best to replicate it using AWS. I know they have S3 and Airflow, but I'm not sure if Redshift is just as flexible. I saw a thread where someone said they used Athena to mimic this workflow. Don't think the data is big enough for Glue or even Redshift but I want to use some sort of DW.

Also as I'm building from the ground up, feedback on this architecture is encouraged, and I would to know if what I'm trying to replicate can be improved on. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mqvsc/new_cloud_question_gcp_aws/,dataengineering
List secret ML repositories to contribute?,14msy8b,trafalgar28,1688108260.0,,False,True,Help,1,False,4," I believe that open-source contributions will significantly impact candidate selection processes. Although I'm relatively new to contributing to ML models and data science or even something related to data engineering, I'm eager to get involved. It would greatly benefit me if individuals could share their bookmarked projects that I could contribute to. Thank you!! ",False,0.83,https://www.reddit.com/r/dataengineering/comments/14msy8b/list_secret_ml_repositories_to_contribute/,dataengineering
Considering a Change: Computer Engineering to Computer Science For Data Engineering,14myr1o,Disastrous-Review547,1688127222.0,,False,True,Career,0,False,1,"I am a student studying computer engineering at a well-known university in Turkey, which is currently ranked 333rd. I am contemplating changing my degree to Computer Science (CS) and transferring to a more flexible university. The reason behind this consideration is that my current university requires intensive study, and I believe that transferring to a different institution would allow me to take advantage of the flexibility it offers. Additionally, during this transition period, I plan to pursue certifications from Google and DataCamp in the fields of data and cloud. I am unsure whether I should proceed with this plan or stay at my current university, which has a strong reputation.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14myr1o/considering_a_change_computer_engineering_to/,dataengineering
How to go about testing data pipeline output?,14mpveu,DataEngineerA,1688098141.0,,False,True,Help,2,False,4,"I have a data pipeline to runs daily, transporting data from Area A to Area B. 

Once the data arrives in Area B, it is immediately consumed to deliver a notification.

I have created business & logic checks to ensure that the data produced is of quality but I'd also like to test the actual numbers that are produced.

A typical week looks like this:

* Monday: 350 
* Tuesday: 57,000
* Wednesday: 24,000
* Thursday: 20,000
* Friday: 48,000
* Saturday: 33,000
* Sunday: 1,100

These numbers will fluctuate but their proportion relative to each other will stay the same. 

Using only SQL, how would you go about testing this output? 

&#x200B;

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mpveu/how_to_go_about_testing_data_pipeline_output/,dataengineering
Why is Hive Metastore everywhere? (Especially Iceberg),14mrhhw,Mysterious_Act_3652,1688103317.0,,False,True,Discussion,5,False,4,"I have been looking at Iceberg and Hudi more lately, and learnt that they both require Hive metastore, but I don't understand why we have this dependency.  

Hive metastore is a pain to setup.  It has a dependency on Hadoop and is configured with XML.  It also has a dependency on a relational database.  So to query my Iceberg table I need Hive, Hadoop and Postgres running.  

I thought Iceberg and Hudi were self describing?  It seems in the case of Iceberg that the only thing the Metastore is used for is to point the table name at the Metadata file.  

Is there not something much more lightweight?  It's just a matter of recording schemas.  Why does something as legacy as Metastore persist for so long?  ",False,0.83,https://www.reddit.com/r/dataengineering/comments/14mrhhw/why_is_hive_metastore_everywhere_especially/,dataengineering
Bringing value as a DE in a startup,14mp34n,it_wasnt_me_who_said,1688095761.0,,False,True,Discussion,10,False,4,"I'm currently confused on this and I'd appreciate some advice/suggestion.

Few ways I've identified

Making their pipeline incremental

Moving the modelling away from the Viz tool

Documentation 
    1) the current dashboard, the tables powering it 
     2) having a place where a search can be done on all the tables (have the feature of adding comments/business meaning to columns/tables)

Adopting an orchestrator and adding proper alert
     currently using the schedule feature of the tool which requires one to go and check for status.

I was talking to a higher up and I was asked what next after all these.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mp34n/bringing_value_as_a_de_in_a_startup/,dataengineering
pyspark-ai: English SDK for Apache Spark,14m8ze6,mjgcfb,1688054926.0,,False,False,Open Source,11,False,30,,False,0.92,https://github.com/databrickslabs/pyspark-ai/,dataengineering
"Apache Airflow, Spark, and Kubernetes",14mc2m9,PhysicalTomorrow2098,1688062286.0,,False,True,Blog,1,False,20,"Hello guys,  
As i didn't find any resources describing the whole chain allowing to test deploying Airflow and Spark within a Kubernetes cluster.  
I have wrote a description of the whole process to get it work in local.  
[https://medium.com/p/869c6b48a026](https://medium.com/p/869c6b48a026)

I hope this could help any begginer stucking on running Airflow and Apache Spark in a local machine.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mc2m9/apache_airflow_spark_and_kubernetes/,dataengineering
Suggestions/Advice on Dataform,14mwa32,bha159,1688119740.0,,False,True,Discussion,0,False,1,"Hi all,

I come seeking your valuable insights and suggestions on utilizing Dataform for dependency management and automating table creation in a data pipeline.

My team and I have recently started exploring Dataform as a solution for managing dependencies between our data tables in BigQuery. We believe that Dataform can help us eliminate redundancies in queries and facilitate smoother data pipeline management.

Here are a few specific things we have trouble understanding:

1. **Dependency Management**: We would like to understand how to effectively use Dataform to manage dependencies between different tables. How can we set up these dependencies so that when one table is created after executing a query, the next layer of dependent tables is automatically generated, is that done automatically or do we have to setup something here(like workflow configurations).
2. **Partial Updates Handling**: If in some cases, only a few source tables are updated, while others remain unchanged. We are curious to know how Dataform handles partial updates and ensures that our data pipeline stays consistent even when some source tables change.
3. **Managing Automation**: We want to explore automation possibilities with Dataform. How can we ensure that once a query is executed and a table is created, the subsequent layers of dependent tables are automatically generated without manual intervention?

I would greatly appreciate any advice, best practices, or architectural suggestions from those who have experience with Dataform or a similar dependency management tool. We want to optimize our data pipeline and streamline the process as much as possible.

Thank you in advance for your time and expertise. Looking forward to your valuable responses!",False,0.67,https://www.reddit.com/r/dataengineering/comments/14mwa32/suggestionsadvice_on_dataform/,dataengineering
Event-driven architecture best practices for databases and files,14m7hqv,itty-bitty-birdy-tb,1688051403.0,,False,False,Blog,14,False,30,,False,0.9,https://tbrd.co/event-driven-rd,dataengineering
DE to Backend SWE,14metqr,maraskooknah,1688068833.0,,False,True,Career,3,False,11,"I completed an interview loop with a position that was titled DE, but it's really a backend engineer who might do some light pipeline work. I am familiar with the typical DE tech stack including Python, SQL, AWS services, Airflow, Spark, etc. They are moving data from source to Snowflake using Kafka, protobuf/gRPC while using languages like Go, some Python, and sometimes have to do some front end work using Typescript and React. It seemed like a lot of stuff I don't have familiarity with, but also sounded exciting.

I've kind of always wanted to make the transition to SWE. Has anyone done this? How difficult was it? I don't want to join a place and then fail. During my technical rounds, I was expecting to have to do SQL and light python, but ended up having to do leetcode and another python round. I asked based on my different background if the hiring manager saw any issues, and he said my background seemed great.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14metqr/de_to_backend_swe/,dataengineering
2.5k parque files.. merge them or create data warehouse for AWS project?,14mudip,pussydestroyerSPY,1688112988.0,,False,True,Help,10,False,0,"Hi, i have 2,500 parquet files of historical data of the stock market. Around 50 GB. I’m working on my thesis project which is to create a deep learning model for forecasting the stock prices by taking in account multiple variables. Also, I am planning to create a site where you can filter the data, set date ranges, etc.
MY QUESTION IS: should i merge all the files into a big single file? Or should I create a data warehouse where i can access this data? .. any other suggestion? I’m also planning to implement AWS during this project, so i would like to know the best approach. Thanks!",False,0.5,https://www.reddit.com/r/dataengineering/comments/14mudip/25k_parque_files_merge_them_or_create_data/,dataengineering
How to Prepare for 2nd Round Technical Interview?,14mgc3s,SonOfaSaracen,1688072349.0,,False,True,Interview,5,False,8,"I just scored a 2nd interview at a company for the role of Data Engineer, half of the interview will be behavioral half will be technical python/ SQL coding. While my python/ SQL skills are good, they're a little bit rusty atm. I have DS Masters from GA Tech

Is there any website out there where I can just drill python related questions back to back for the next week to work out any of the rust?

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mgc3s/how_to_prepare_for_2nd_round_technical_interview/,dataengineering
How to do column projection (filtering) server-side with Azure Blob Storage (Python Client Library)?,14mt29y,Plenty-Button8465,1688108602.0,,False,True,Help,10,False,1,"As stated in the title, I'm learning how to download a parquet file from Azure Blob Storage with the Python Client Library. Yesterday I was able to implement the code but I was wondering if I could filter only the desidered columns before actualing downloading the file from Azure in order to limite the resources and the time spent on the I/O networking. Is there a solution?

My code so far:

    class BlobStorageAsync:
        def __init__(self, connection_string, container_name, logging_enable):
            self.connection_string = connection_string
            self.container_name = container_name
            container_client = ContainerClient.from_connection_string(
                conn_str=connection_string,
                container_name=container_name,
                # This client will log detailed information about its HTTP sessions, at DEBUG level
                logging_enable=logging_enable
            )
            self.container_client = container_client
    
        async def list_blobs_in_container_async(self, name_starts_with):
            blobs_list = []
            async for blob in self.container_client.list_blobs(name_starts_with=name_starts_with):
                blobs_list.append(blob)
            return blobs_list
    
        async def download_blob_async(self, blob_name):
            try:
                blob_client = self.container_client.get_blob_client(blob=blob_name)
                async with blob_client:
                    stream = await blob_client.download_blob()
                    data = await stream.readall() # data returned as bytes-like object
                # return data as bytes (in-memory binary stream)
                return BytesIO(data)
            except ResourceNotFoundError:
                logging.warning(f'The file {blob_name} was not found')
                return None
    
        async def download_blobs_async(self, blobs_list):
            tasks = []
            async with asyncio.TaskGroup() as tg:
                for blob_name in blobs_list:
                    task = tg.create_task(self.download_blob_async(blob_name))
                    tasks.append(task)
            return tasks

&#x200B;",False,0.67,https://www.reddit.com/r/dataengineering/comments/14mt29y/how_to_do_column_projection_filtering_serverside/,dataengineering
What do you call yourself if you do both front end Dev and DE?,14m6t8f,Swimming_Cry_6841,1688049782.0,,False,True,Career,20,False,12,"I’ve been at my current job for 7 years. It’s a small team (3 devs) maintaining. SaaS CRM for 1000 users at a $7billion company.

My development work consists of creating new screens in the web app with handlebars / JQuery / JavaScript, writing Rest API calls for other teams and passing them the postman file or working on Azure Synapse (spark) pipelines, the Azure SQL DB (stored procedures, views, etc)

I pretty much feel like it’s a Jack of all
Trades situation.  Anything that comes up no matter l, what gets thrown at us devs.  If I update my Linked In to say Data Engineer it goes dark as I suspect not many companies use Azure SQL and Synapse and when I update it to Full Stack Developer or similar I get a lot of hits from people wanting me to work in the same CRM(D365) for them.  

If I’m trying to target a new DE role somewhere should I drop all the front end Dev work stuff off my linked in and just write about the data work?  Any advice on how to parlay my Synapse / Azure SQL experience into something else? My goal for a next position would just be pure DE so I’m not also tweaking CSS and writing handlebar templates half the time.  
Thanks",False,0.93,https://www.reddit.com/r/dataengineering/comments/14m6t8f/what_do_you_call_yourself_if_you_do_both_front/,dataengineering
"Which are the most inefficient, ineffective, expensive tools in your data stack?",14ltv6p,drc1728,1688009399.0,,False,True,Discussion,222,False,85,"With all of the buzz around the high costs of various platforms and tools used for building data pipelines, including data collection, data warehousing, data processing and transformation, extracting insights out of the data - 

Which are the most inefficient, ineffective, expensive products that you have experienced?

Top 5 or 10 products listicles in various categories are just paid marketing campaigns and provide biased information.

What is the tribal wisdom about the worst offenders in data tools and platforms that you would recommend staying away from and why?

Share away and help the budding data engineers out.",False,0.91,https://www.reddit.com/r/dataengineering/comments/14ltv6p/which_are_the_most_inefficient_ineffective/,dataengineering
Any good resources for average cloud bill benchmarking for different company sizes/ industries?,14mc7wg,Dull_Lettuce_4622,1688062634.0,,False,True,Discussion,1,False,6,"I'd like to make a business case to my bosses that we can save a ton of money on human labor by beefing up some of our cloud spending. Anything from something as trivial as giving each engineer on my team a dedicated VM (as opposed to sharing them and running out of RAM), to spending more on our cloud compute for auto scaling so we don't need to worry about spending months designing our systems to be more fault tolerant of the days when users decide once per month to hit our system all at once.

My issue is other than my boss the CTO, who also favors automation, none of the rest of the C-Suite understands what a typical cloud bill in our industry is and may look at my ask and say ""we could hire 3 FTEs to do xyz in other departments with the money your asking for"", which I can counter with the hypothetical that hiring 2 software engineers to do this stuff is more costly. 

 Compared to past companies I've worked for which support a similar number of customers, our current cloud bill is fractions of what we spent in the past for similar problems. Obviously I can't go sharing that out loud with specific numbers since it's non public information, but I'm curious if there are publicly available well respected benchmarks I can reference?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mc7wg/any_good_resources_for_average_cloud_bill/,dataengineering
Data mart for semi-structured JSON data,14m4bmf,thebemusedmuse,1688043350.0,,False,True,Help,31,False,11,"Would very much appreciate some advice.

We have a ton of historic semi-structured data which is stored in JSON. I say semi-structured because the structure has been expanded over the years, so the schema is not fixed over time, even if the major dimensions have not changed, the key figures have expanded.

We have probably records rows a day of JSON for the last 5 years, so 2m records. Each JSON record could be as much as a few kb. Maybe a few GB total, at the very most.

Is there a product (ideally cloud) which is well suited to really easily importing this data and enabling us to do some basic time series analysis? I need some quick and dirty analysis and want to avoid a big data warehouse project. Doesn't matter if it's a commercial or open source product.

Back in the day I used MongoDB to do things like this, but maybe there are better options now?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14m4bmf/data_mart_for_semistructured_json_data/,dataengineering
Azure data lake - Data Share,14mbj7d,soujoshi,1688061021.0,,False,True,Help,9,False,3,We have a requirement to share data securely that lies on ADLS with a team outside of our organization. What are the options we have to get this done?,False,0.81,https://www.reddit.com/r/dataengineering/comments/14mbj7d/azure_data_lake_data_share/,dataengineering
Data Engineering and Analytics Day,14m951l,fmaina1,1688055311.0,,False,False,Blog,0,False,4,,False,1.0,https://cloudonair.withgoogle.com/events/data-engineering-and-analytics-day,dataengineering
How can I schedule python ETL code?,14lu0dk,Rawvik,1688009808.0,,False,True,Help,52,False,41,So I am still a newbie. I have a python project where I am scraping data from some log files on a daily basis and cleaning it then pushing it to db. I have scheduled this project to run daily on windows task scheduler. Is there any other(better) way it can be scheduled on a windows system?,False,0.9,https://www.reddit.com/r/dataengineering/comments/14lu0dk/how_can_i_schedule_python_etl_code/,dataengineering
First DE gig at large scale,14lwjf0,ElephantCorrect2132,1688017626.0,,False,True,Career,6,False,23,"These are all boiler plate concerns and just seeking an open dialogue on advice. 

I have worked at multiple startups as a data engineer and run successful data warehouses. I write and follow standards, procedures, and best practices. Typically ive worked solo or with one other person. I am proud of the data platforms I have created. 

I am starting a new project as a consultant next week with a well known billion dollar company that I cant disclose. Im a little nervous because my past projects have been significantly less data than in the past. I work with snowflake. What do I need to be prepared for when working at scale. I.e I previously have worked with fact tables 500k-1M rows, now will be working with MM and billion row tables. 

I consider myself good at optimizing queries and I hope the power of larger snowflake warehouses will be of assistance. I am the lead engineer on the project so want to be as ready as possible for the transition from a small warehouse to a large warehouse. 

Anything helps! Thanks!",False,0.97,https://www.reddit.com/r/dataengineering/comments/14lwjf0/first_de_gig_at_large_scale/,dataengineering
Transitioning from Bioinformatics to Data Engineering: Feasibility and Career Opportunities?,14m4j6x,greengecko7,1688043907.0,,False,True,Career,14,False,7,"Hello, I am currently on the verge of completing my Ph.D. in Bioinformatics and I am exploring potential career opportunities outside of academia. I have been considering a transition to the field of Data Engineering and I'm curious to know if anyone here has made a similar leap or has insights to share.

To provide some background, I thoroughly enjoy working with data, especially when it comes to organizing, curating, and cleaning it. While I also find joy in analyzing data and making predictions, I feel a stronger affinity towards the data management aspects of the process. With proficiency in Python, SQL, and R, I have a solid foundation in data-related programming languages.

During my research journey, I extensively employed ML algorithms, particularly focusing on clustering, within the domain of Genomic Data Science. This experience allowed me to gain valuable skills in data manipulation, preprocessing, and extracting meaningful insights from complex biological datasets.

Moreover, to further enhance my skills in data management and engineering, I completed the Data Engineering career path on DataCamp. This course deepened my understanding of various data storage and processing technologies, such as Apache Spark, Hadoop, and ETL pipelines.

Given this background, I am curious to know if it is possible to transition from Bioinformatics to Data Engineering. I understand that there might be differences in the domains, but I believe that the skills I have acquired can be transferrable.

I would greatly appreciate any insights, experiences, or advice from those who have successfully transitioned from Bioinformatics or a related field to Data Engineering. Are there any specific skills or knowledge areas that I should focus on to increase my chances of making a smooth transition? Are there any potential hurdles or challenges that I should be aware of?

Thank you in advance for your time and valuable input. I am eager to hear your thoughts and experiences on this matter.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14m4j6x/transitioning_from_bioinformatics_to_data/,dataengineering
Putting a Filesystem on Top of an Object Store is a Bad Idea. Here is why.,14mav25,swodtke,1688059430.0,,False,False,Blog,2,False,2,,False,1.0,https://blog.min.io/filesystem-on-object-store-is-a-bad-idea/,dataengineering
Experts wanted: exploring the gap between AI hype and real-world decision making,14mg6hm,Mister_Ragusa,1688071971.0,,False,True,Help,0,False,0,"Hello everyone, I'm writing my master's thesis and I would like to conduct 1:1 interviews with data scientists, BI practitioners, and executives to understand their perspective on the topic.

&#x200B;

In the business context, we often hear statements like ""AI enhances decision-making,"" ""AI can tell executives what to do"" and ""data-driven decision-making is easy"". However, I saw a considerable amount of misinformation, snake oil solutions, references to Gartner, and the same hypothetical use cases over and over.

&#x200B;

Essentially, I am conducting a reality check on the capabilities and limitations of AI technologies (ML, expert systems, fuzzy logic, etc.) in assisting analytical decision-making within a business context. I aim to determine when it is reasonable to adopt these solutions. If AI proves to be unattainable for most companies, I want to provide managers with practical advices on what they should focus instead (e.g. data culture, centralization and availability of data, real problem identification).

Lastly, I am also interested in the role of human intution in decision making, because it plays a role in ""managing the things you can't measure"" or those situations that are not practical to model.

&#x200B;

If you are a data scientist, BI practitioner, or executive, I would greatly appreciate your insights and expertise in this field. 

Please send me a DM to schedule a 30-minute interview at your convenience. Your participation will have a significant impact on my research, and your insights will help shape practical recommendations for businesses navigating the AI landscape. Together, we can uncover the truth behind AI's potential and empower decision-makers with accurate information.

&#x200B;

Thank you in advance for your time and contributions.

&#x200B;

Note: All information provided will be anonymized",False,0.5,https://www.reddit.com/r/dataengineering/comments/14mg6hm/experts_wanted_exploring_the_gap_between_ai_hype/,dataengineering
Taking one hit after the other || Findig my sweet spot,14mewfw,binchentso,1688068994.0,,False,True,Career,5,False,1,"I am a bit confused these days. Having a business background it always felt difficult to move into data engineering or analytics engineering. 

My company sponsors me a DE course but ultimately does not give me the chance to work on projects related to it. I feel i am still suck. 

So i went on to push for a BI role / analytics engineering position. Here they do not hire atm. 

So I asked for a data analytics position. Here the feedback is that i am missing statistical knowledge and A/B testing experience. 

In my current role i am way stepping out side my role doing engineering work - actually building stuff that is in production without the team double checking it. They now rely on several of my scripts and tools. It seems that they like my work but when I want something in return i get a no. I do coding in python and javascript and automate several highly important processes. 

I feel frustrated that i am not getting a chance. Yes, i am no CS major but i am very eager to learn on the job. Doesn't that count anymore? I am working for a FAANG like company in central Europe. I was eager to develop myself within the company but do not see any chance to move sideways. 

I am considerig leaving the company to apply what i learn hard every day (paid by them) and diving more into tooling and setups. 

Does someone has a recommendation or was / is maybe ina similar situation? My aim would be to work in either analytics engineering or data analytics but more technical - buiding ETLs. I am just frustrated and want to stop running after things and start working on something tangible.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14mewfw/taking_one_hit_after_the_other_findig_my_sweet/,dataengineering
What is a Data Product? An In-depth Introduction,14m8os9,Dkreig,1688054242.0,,False,True,Blog,0,False,2,How did I do with this in-depth intro? [https://www.dataops.live/what-are-data-products](https://www.dataops.live/what-are-data-products),False,0.67,https://www.reddit.com/r/dataengineering/comments/14m8os9/what_is_a_data_product_an_indepth_introduction/,dataengineering
Delta Lake 3.0,14lfqfu,rexicusmaximus,1687973614.0,,False,True,Discussion,80,False,104,"Just announced, Delta Lake 3.0 now compatible with Hudi and Iceberg.

My life just got more interesting.",False,0.96,https://www.reddit.com/r/dataengineering/comments/14lfqfu/delta_lake_30/,dataengineering
OpenSearch ELK stack for analytics?,14m44y9,VersatileGuru,1688042829.0,,False,True,Discussion,5,False,2,"I've been struggling to get some decent client facing analytic tools up and running because our legacy on premise setup is basically two extremes of ""a couple poorly deployed COTS front end COTS apps only connected to 20% of our data"" or ""spark clusters and jupyter notebook data science"". The latter works for myself and other tech literate analysts or users.

Been reading more about OpenSearch since the current open-source ELK stack is super restricted to basically useless (you can't even print a dashboard or viz to PDF) and it's starting to look very appealing to get some datamarts hooked up to dashboards and viz, but I don't really come across discussion around OpenSearch and ELK stacks beyond search engines or as part of another app. No one at my workplace seems to have experience because everything for search engines deployed are all Solr setups.

How difficult are some of the AD integration for authn and authz to setup or the ABAC permission configs that the docs explain? Or setting up logstash / beats data connectors that listen in on an API?

It's all very outside my mostly SQL world wheelhouse but I experimented with it and kibana on a small ad hoc dataset and I really liked it as a kind of data exploration, EDA and search tool.

But is it something you would look to create indexes on a couple datamarts with some dashboards as a general purpose analytics tool or is the effort and esoteric configs just not worth it?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14m44y9/opensearch_elk_stack_for_analytics/,dataengineering
Practical Applications of ML for analysis or ETL?,14m3oxa,VersatileGuru,1688041586.0,,False,True,Discussion,1,False,2,"I'm a data analyst, and I'm not on the development side really I mainly run BI style analysis on our spark clusters with notebooks.

I've done lots of reading and tutorials and the like on different ML applications in python, but all of them seem like they are quite 'heavy' in terms of investment, and really are best done as part of a concerted effort to actually build in a model onto some sort of product (i.e. recommendation engine on your app).

But, aside from that, are there any applications of ML or NLP that are more for making some of the more mundane tasks easier when it comes to analysis or ELT that is *actually* less work than writing out specific functions for transformation or dedupe or data enrichment?

For example, are there any interesting techniques people have somehow applied to help automate metadata tagging or data normalization?

 I don't mean like some magical one button non existent AI bullshit. I just mean practical day to day techniques that are repeatable and actually worth the time spent doing (as opposed to just tackling it manually) like when looking at a new data source for ETL ""you could use NLP to flag columns that likely match your existing schema"".

Closest example I'm currently looking to play with is a few NLP based libraries for identifying and flagging PII or GDPR data that needs to be masked / purged.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14m3oxa/practical_applications_of_ml_for_analysis_or_etl/,dataengineering
Book recommendation for Data virtualization,14m7djl,x_butnocigar,1688051115.0,,False,True,Help,1,False,1,"Hope I'm not breaking any rules of this sub. 

I'm looking for some book (or any other resource tbh) recommendations to learn more about data virtualization. My company is now moving towards using Denodo and I'd like read up on this over the weekend. I probably don't want something that goes into the very minute detail - something that scratches the surface should suffice for the first pass.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14m7djl/book_recommendation_for_data_virtualization/,dataengineering
Data Meet Security for Location-based Access Control,14m2ch5,Permit_io,1688037537.0,,False,True,Blog,0,False,2,"A nice article shows how to utilize ipinfo database to create location-based access control in a full-stack app. Good to see how those new data cloud products can use to create much better security for every app in no time.

[https://io.permit.io/location-based-access](https://io.permit.io/location-based-access) ",False,0.75,https://www.reddit.com/r/dataengineering/comments/14m2ch5/data_meet_security_for_locationbased_access/,dataengineering
Databricks Data + AI Summit,14lw0op,Known-Delay7227,1688015974.0,,False,True,Discussion,0,False,6,Anyone else here? What are your thoughts so far?,False,0.71,https://www.reddit.com/r/dataengineering/comments/14lw0op/databricks_data_ai_summit/,dataengineering
What is it you do with Python in your Job?,14lhzv9,Cstadler25,1687978940.0,,False,True,Discussion,30,False,37,"I want to switch jobs and trying to determine what skills I would need to learn to try and get some interviews. My current job skills are heavy database, and coding in c#.  Therefore I think one of the skills I apparently need is Python, but I am not sure in want context I would need to learn/use it.

So my question is how are you using it? Is it simply to import files, run transform calc and export file? Are you exporting results to Databases/ pulling from databases? What editor do you use for your programs/scripts? I just feel lost at the idea of the generic ""learn python"", like I do with most other skills I am going to have to learn.",False,0.94,https://www.reddit.com/r/dataengineering/comments/14lhzv9/what_is_it_you_do_with_python_in_your_job/,dataengineering
Spark explain(): how does Spark decide on the value of column identifiers?,14m6etv,haskathon,1688048787.0,,False,True,Help,2,False,1,"# Background

* I’ve been playing around with `explain` out of curiosity
* I’m trying to understand how column-level lineages can be built from `explain(""formatted"")` (although this isn’t that relevant to my question)
* This question’s scope is limited to query plans for queries of the form `spark.table(""schema.table"").explain(""formatted"")`

# Question

* When running the above query on separate tables, why is it that the column identifiers start at different values for different tables (see example below)?

# Examples

* Disclaimers:
   * The columns and identifiers are separated from each other and stored in a dictionary as a result of a function I wrote to process the `explain(""formatted"")` output
   * I’ve renamed the columns since they’re from actual tables used in my company

**Table A**

    {'a_1': '0L',
     'a_2': '1L',
     'a_3': '2',
     'a_4': '3',
     'a_5': '4',
     'a_6': '5',
     'a_7': '6',
     'a_8': '7',
     'a_9': '8L',
     'a_10': '9',
     'a_11': '10',
     'a_12': '11',
     'a_13': '12',
     'a_14': '13',
     'a_15': '14',
     'a_16': '15',
     'a_17': '16'}

**Table B**

    {'b_1': '1844L',
     'b_2': '1845',
     'b_3': '1846',
     'b_4': '1847L',
     'b_5': '1848',
     'b_6': '1849',
     'b_7': '1850',
     'b_8': '1851',
     'b_9': '1852',
     'b_10': '1853'}",False,1.0,https://www.reddit.com/r/dataengineering/comments/14m6etv/spark_explain_how_does_spark_decide_on_the_value/,dataengineering
Do you as an engineer care how much efficiency/cost reductions you're bringing in?,14lhlo6,OptimistCherry,1687978013.0,,False,True,Help,35,False,34,"Most people in their resume mention, they improved processing speeds by 20%, 30% etc, reduced costs. by x, y etc, . Do most developers know these kind of impacts in general? as an interviewer, do you care to ask them to elaborate? and people who mentioned stats like this, do you see improved calls for interviews? Also, at what years of experience people will care about this? ",False,0.96,https://www.reddit.com/r/dataengineering/comments/14lhlo6/do_you_as_an_engineer_care_how_much/,dataengineering
Databricks releases official SDK for Python,14leo16,serge_databricks,1687971083.0,,False,False,Open Source,8,False,34,,False,0.88,https://github.com/databricks/databricks-sdk-py,dataengineering
"Delta merge is writing data, even though there are no inserts or updates?",14lnprw,Fredbull,1687992523.0,,False,True,Help,12,False,7,"Hi everyone, I have recently started using Delta tables with Pyspark and AWS S3 storage.

I am doing a merge into my Delta table, with conditions for update and insert.

I have some test data that does not change between runs, so I was expecting the merge to be idempotent, meaning: two subsequent merges should not write any data to S3.

&#x200B;

However, when I run the merge statement, most of the delta table is re-written.

Even stranger,  when I check the Delta table's history, I verify that 0 rows were inserted, 0 rows were updated, and 0 rows were deleted - but also that 70 files were removed and 70 fliles were added (corresponding to most of my dataset).

Any ideas why this might be the case? Thank you!",False,0.9,https://www.reddit.com/r/dataengineering/comments/14lnprw/delta_merge_is_writing_data_even_though_there_are/,dataengineering
"External tables, storage credentials, and Unity Catalog.",14lqcqk,Doyale_royale,1687999390.0,,False,True,Discussion,1,False,5,"We are working on setting up our databricks environment (I’m sure you’ve seen my other posts) I am wondering how you guys use these 

- do you have one to many storage credentials to external locations or one to one? 
- do you specify external locations at the top most level of a bucket or to a folder hierarchy in the bucket? 
- how do external tables work with unity catalog? If I make a catalog/ schema with an external location, where is that catalog and schema?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14lqcqk/external_tables_storage_credentials_and_unity/,dataengineering
LakehouseIQ: Your new AI overlord,14lcl25,mjgcfb,1687966222.0,,False,False,Blog,5,False,17,,False,0.73,https://www.databricks.com/blog/introducing-lakehouseiq-ai-powered-engine-uniquely-understands-your-business,dataengineering
Resources for Weather/Geospatial data?,14lcyxr,gbromley,1687967137.0,,False,True,Discussion,18,False,15,"I am currently a postdoc at a university, but I would like to move into a private industry career that acquires and analyzes weather/climate/geospatial data. For example, there are a ton of mapping apps (OnX, Gaia, etc) that I think would benefit from including more weather data and derived products. 

&nbsp;

**Are there any resources (websites, books) that deal explicitly with the DE challenges unique to these data?** For example, I have almost never had to use SQL in my work and most things I work with are gridded (i.e. raster). I know there will be overlap with things like AWS, Spark, etc, so I am focusing on those right now. 

&nbsp;

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14lcyxr/resources_for_weathergeospatial_data/,dataengineering
https://rockset.com/blog/real-time-clinical-trial-monitoring-at-clinical-ink/,14lx2u7,ssb61,1688019412.0,,False,True,Blog,0,False,0,"Clinical Ink does real-time clinical trial monitoring using DynamoDB with an external index for real-time search, aggregations and joins",False,0.4,https://www.reddit.com/r/dataengineering/comments/14lx2u7/httpsrocksetcomblogrealtimeclinicaltrialmonitoring/,dataengineering
Dagster Cloud Pricing,14lk206,KindaRoot,1687983809.0,,False,True,Discussion,5,False,6,"What do you guys think of dagsters new pricing model? I am pretty new to dagster and the pricing model just changed during trial.
With the Team Subscription, you have 10000 Credits available, which means 330 asset matrializations a day, which seems pretty low when you have a lot of small assets you are materializing daily. Materializing one asset every 10min gets you 150 already. Do I have a  wrong understanding of this?",False,0.88,https://www.reddit.com/r/dataengineering/comments/14lk206/dagster_cloud_pricing/,dataengineering
Facebook data transfers declared illegal,14l4vx6,nulovyk,1687944975.0,,False,False,Blog,2,False,20,,False,0.92,https://www.simpleanalytics.com/blog/meta-hit-with-record-breaking-1-3-billion-fine-over-facebook-data-transfers-to-the-us,dataengineering
Exploring Graphs in Rust. Yikes.,14lcciw,DarkClear3881,1687965666.0,,False,False,Blog,0,False,6,,False,0.75,https://www.confessionsofadataguy.com/exploring-graphs-in-rust-yikes/,dataengineering
How complex is the coding in data engineering?,14l2se5,jimharbaughthrowaway,1687937371.0,,False,True,Help,47,False,20,"I ask as someone who is completely unfamiliar with the field and only knows it as a subset of software engineering — my title is software engineer and I’m part of a new grad rotational program and the rotation that I was just assigned to is basically a data engineering role. 

I’ve been told I’ll be using Python, GCP, and some APIs/ETL tools but I have really no concept of what that looks like in terms of how they interact with each other or how technical it will be. I’ve read on this sub that it can vary from solutions where you don’t write a ton of code and that most of the heavy lifting is done by some tool or low-code platform, to just being Python scripts gathering things from different sources, to really complex brain surgery code. I know that every case is different, but what is the most common scenario (especially given the technologies I mentioned)?",False,0.82,https://www.reddit.com/r/dataengineering/comments/14l2se5/how_complex_is_the_coding_in_data_engineering/,dataengineering
👉 New Awesome Polars release! What's new in the world of Polars in June 2023 ? Let's find out! 🚀,14la4y2,damiendotta,1687960389.0,,False,False,Blog,3,False,7,,False,0.77,https://github.com/ddotta/awesome-polars/releases/tag/2023-06-28,dataengineering
Are these terms irrelevant in the industry anymore?,14kra4i,Bloodylime,1687903985.0,,1687953754.0,True,Interview,116,False,105,"I am having interviews to hire someone who will work for me. I interviewed two people so far. Neither of them answered on questions:

&#x200B;

1. OLAP and OLTP systems
2. Star Schema vs. Cube
3. ETL vs. ELT
4. Window function SQL question

&#x200B;

It is a position for 3+ years in data analytics, business intelligence, or a related field and I didn't expect to get the full extent of complete answers. Am I asking too difficult questions? or am I becoming out of touch and those aren't relevant anymore?

&#x200B;

Edit: I didn't really make it clear what the role is for. The role is BI Engineer, but the candidates that the head hunter sent to our HR manager happened to have a data analyst background. ",False,0.92,https://www.reddit.com/r/dataengineering/comments/14kra4i/are_these_terms_irrelevant_in_the_industry_anymore/,dataengineering
Java Data Engineering and Data Science Libraries,14licjh,magu01,1687979786.0,,False,True,Help,3,False,2,"What are my choices for working with data i.e. Data Engineering and ML/AI task on JAVA? 

&#x200B;

I know in Python there are many libraries/Frameworks choices Pandas(/numpy)/Polars/Dask/PySpark/Databricks depending on the size of data, type of system, on prem/cloud, project (webapp/edge device app) ect. For ML AI task again there are many options Scikit Learn, SparkML, Keras, ect. 

&#x200B;

After some googling and chatGPT prompts I didn't exactly find great results. I saw that many JAVA Frameworks that are used for ML task accept Tablesaw (Dataframes) i.e.DJL (Deep Java Library)\\DeepLearning4j\\Flink . However Tablesaw hasn't been updated since April 2022. I started looking at Spark on Java but had some trouble finding resources in Java. The resources I kept finding for spark ended up being for either Scala or Python. 

&#x200B;

Are there any good resources for data engineering and data science using JAVA. ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14licjh/java_data_engineering_and_data_science_libraries/,dataengineering
ADF parameters in SQL query -Urgent,14letcn,ProcedureScared2970,1687971436.0,,False,True,Help,1,False,3,"Hi Guys,

I am writing a query in dataflow lookup with schema name as a parameter.
I have already created a schema_name parameter with $parameter_name with string default value.
So how should i write a below query in lookup.
Select * from $parameter_name.tablename

I have tried-
Concat(‘Select * from ’, $parameter_name,
‘.tablename’)

This is working, it is not able read the value of parameter.

Can anyone help me with the solution.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14letcn/adf_parameters_in_sql_query_urgent/,dataengineering
Basic Matillion question,14lms2a,el_pedrodude,1687990197.0,,False,True,Help,3,False,1,"So my background is in MS SQL Server and SSIS mainly and I've never used Matillion before (not even seen it).  But I need to help someone else figure out what I think is an elementary problem.  Since they're not familiar with ETL at *all*, I've been promoted to the rank of Local Expert (*in the land of the blind*... and all that).

He* literally needs to do 2 things:

* Append some new hardcoded and derived columns to the dataflow
* Flatten some json (to map keys to destination table columns)

Basic stuff, i feel, but could someone point my nose in the right direction?  Bonus points if you can translate it into SQL Server terms.

In SSIS land I'd chuck a derived column and a 3rd-party tool/c# script at the problem and get on with my life, but with Matillion I'm not sure what the equivalent is.  I've also heard a nasty rumour that Matillion ETL is conceptually more like ELT anyway so I realise there may not be direct equivalents of SSIS patterns.

I'm sure I could find out the answer with enough googling but it would save us a bit of project time if we could crack this when he takes me through what he's trying to do tomorrow.

*very aware this sounds like I'm ""asking for a friend"" but I really am... *as well as* asking for myself...",False,1.0,https://www.reddit.com/r/dataengineering/comments/14lms2a/basic_matillion_question/,dataengineering
How-to guide for spinning up a Jupyter lab instance in the cloud,14ledla,dask-jeeves,1687970421.0,,False,True,Blog,0,False,2,"How-to guide for a convenient way of spinning up a Jupyter lab instance with hundreds of GBs of memory, a fancy GPU, or ARM-based instance. \`coiled notebook start\` will replicate your local environment on any machine in the cloud. [https://medium.com/coiled-hq/coiled-notebooks-d4577596ff4a](https://medium.com/coiled-hq/coiled-notebooks-d4577596ff4a)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ledla/howto_guide_for_spinning_up_a_jupyter_lab/,dataengineering
Snowflake or Databicks,14l5wfj,rolledthrough7578,1687948330.0,,False,True,Discussion,6,False,6,"Currently a senior analyst/junior engineer.

I’ve built one pipeline using snowflake as a storage solution.

Should I continue learning snowflake? Or should I learn databricks. (I’m also looking to get certified in one or the other).",False,0.75,https://www.reddit.com/r/dataengineering/comments/14l5wfj/snowflake_or_databicks/,dataengineering
How to get a job in this market??,14kyy12,Test_Known,1687924578.0,,False,True,Career,33,False,18,"I am a data engineer with 3+ years of full time work experience in India and 1 year of experience in a full time internship in US.

I have good skills in SQL, Python, ETL, Data warehousing, GCP, Snowflake and databricks. 

I have been applying for jobs for past 3 months now, but still not getting any calls. I have tried applying to jobs via LinkedIn/indeed, connecting with my ex managers and employers, cold emailing Hiring Managers etc.

What am I doing wrong. What else can I do to get a job in this market",False,0.85,https://www.reddit.com/r/dataengineering/comments/14kyy12/how_to_get_a_job_in_this_market/,dataengineering
Anyone else really frustrated with the additional responsibilities?,14l93wo,PeacefullyFighting,1687957762.0,,1687958109.0,True,Discussion,4,False,3,"When getting my career started (information technology BS degree with an emphasis on data/data systems). I got in with a smaller, somewhat new startup and had the opportunity of my life. I got to design a data warehouse from the ground up based on the source data, nothing more and no opinions,  using a very solid ERP system as my source. 

I created a very nice kimball model that fit the specs to a T (I still get reminded how good it is whenever I have to let my old boss know he's being used as a reference). 

After that I knew what my skill was and I moved on to keep building instead of creating reports/dashboards from my own design. The probablem is no one gives a F about scalability anyone. Sure it's always something talked about but as soon as you try to say something such as ""We are talking a lot about dataset C that is generated from dataset A and dataset B but as the engineer can someone start by defining dataset A Or B?

...crickets. And then I ask, as someone new I have to ask, do you know what the error tolerance of our reports are?",False,0.71,https://www.reddit.com/r/dataengineering/comments/14l93wo/anyone_else_really_frustrated_with_the_additional/,dataengineering
Reltio and SQL for my first job ever.,14lb7i0,TophKatara,1687962987.0,,False,True,Help,5,False,2,"Good evening Gentleman and Ladies.

I have an Internship that starts July 10th, this will be my first job ever.

I want to get a head start.

* Internship will involve **Reltio a master data maangement platform.**
* I will be **learning to manage records**
* With s**ql I will be managing/creating burn down list**
* **Writing querys to target certain records.**

My questions are:

1. What SQL Commands/Functions should I be really trying to Master to do this task?
2. I don't know much about IT too be honest, what are some basic things I should know about to not look clueless.
3. How can I get good at using Reltio?

To those who took the time to answer, thank you.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14lb7i0/reltio_and_sql_for_my_first_job_ever/,dataengineering
What is going on with Ubuntu 22.04,14lgpfn,Rokuzov,1687975907.0,,False,True,Help,0,False,0,"I've been trying to connect to a mssql table, using FREETDS, unixODBC and can't get to connect...

Wondering in forums, many People is having the same issue with 22.04, is anyone else experiencing shitty issues connecting from Ubuntu 22.04 to mssql?",False,0.5,https://www.reddit.com/r/dataengineering/comments/14lgpfn/what_is_going_on_with_ubuntu_2204/,dataengineering
Hevo Data: integration over public internet,14lev25,Exciting-Resident130,1687971552.0,,False,True,Help,2,False,1,"Does anyone have any experience using Hevo over public internet? We are considering an ETL/ELT pipeline solution that integrates a data source on Gov Cloud with a data source on Commercial Cloud. But the only way one could connect is via IP whitelisting since VPC peering or site-to-site VPN connections are not possible. What kind of security provisions do we need for such data transfers (I understand we need TLS 2.0 and data level encryption at the minimum but does Hevo support SAML 2.0, OAuth2.0 and/or OIDC based authentication?)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14lev25/hevo_data_integration_over_public_internet/,dataengineering
Fivetran and Sisense: I am annoyed,14kx9l2,Batspocky,1687919702.0,,1687947241.0,True,Discussion,29,False,12,"As the title says, I am annoyed.

Four or five years ago, we signed up with Periscope Data for BI. Their package included some basic ELT & warehousing infrastructure using Fivetran and Redshift. It was convenient (but not necessarily cost effective) to have our entire data stack managed by a single vendor, for a single price.

Fast forward a year and Periscope gets bought by Sisense. Sisense agreed to continue managing our whole stack, but each time we renewed our contract, they seemed more and more confused by the relationship with the other vendors, didn’t know how Fivetran was supposed to get paid, stuff like that.

So this year, I see that Fivetran is now offering a free plan for customers who use less than 500K monthly active rows (MAR). We were just over that amount, and I saw an opportunity to lower our costs by reducing our usage. I checked in with Sisense and Fivetran to make sure I was thinking correctly - get under 500K MAR, and drop down to the free tier. Cut out the Fivetran portion of our spend.

Now, just a couple days before we have to renew our contract with Sisense, they’re telling me that *even though I have optimized our ELT process to get into Fivetran’s free tier*, our bill to Sisense is *not going to decrease at all* and will actually go up the usual 5%.

What a joke. In what world is a business - especially a small biz like ours (<50 employees) going to pay *more* for the vendor to do *less*? If I can optimize our ELT, I’ll bet I can optimize our BI as well, and Sisense can go pound sand.

Is this the norm for vendors in the analytics space? Does anyone have any suggestions for more cost-effective BI solutions? I was playing around with Looker Studio today…query times were pretty long for a dataset that only has about 200,000 records…I have a call with Mode next week. Less than $5,000 a month would be good…less than $2,500 a month would be even better.

*Edit - 500K MAR, not 500. Sorry. Shouldn’t post when tired.*",False,0.88,https://www.reddit.com/r/dataengineering/comments/14kx9l2/fivetran_and_sisense_i_am_annoyed/,dataengineering
Data Platform Adoption and FAANG Companies?,14ksgkm,jduran9987,1687906844.0,,False,True,Discussion,19,False,24,"I've been thinking about the future of data engineering and what kind of learning path I would suggest to someone breaking into the industry.

Databricks + Snowflake seems to be providing a ton of value by giving us a platform to focus on data engineering (instead of configuration, connecting tools, etc.). 

However, I don't see adoption at the holy-grail big data FAANG companies. From looking at their job descriptions, these companies are still using tools within the Hadoop ecosystem. 

This makes me wonder if it is the small number of ""big data"" companies keeping the Hadoop ecosystem alive as the industry tries to move forward and simplify. Or why haven't they also adopted platforms like Databricks or Snowflake. I get these tools would be costly given the vast amounts of data, but I would imagine that there would also be a ton of savings on engineer hours. 

Is the future of DE platforms like Databricks? Do we only care about the Hadoop-ecosystem because of a few large companies who haven't figured out how to adopt these platforms in a cost-effective way?

&#x200B;",False,0.96,https://www.reddit.com/r/dataengineering/comments/14ksgkm/data_platform_adoption_and_faang_companies/,dataengineering
Would a data science masters degree be useful for DE?,14lc985,prillo7991,1687965441.0,,False,True,Career,18,False,0,"I've been studying and working toward a career transition in data engineering.  I'm not ready yet, but I have passed the Microsoft DE exam (and the DBA one), and I've been working in Azure to create demos.  

My mentor has described me as ""internship ready"" though I don't have access to internships, as all (so far) have required that only current CS-related college students apply.  I completed my bachelors years ago in social sciences, and I had lots of sporadic IT skills/training, but little of it is formal classes.

Next step is to accelerate/focus the process, which means some formal training.  My best options are these:

\- an offer of a data science masters degree (FREE) at an honors college, inc paid internship  
\- a bootcamp style 6 month graduate program ($10K) remote through a university/Springboard  


My question: If my goal remains DE, would be it normal (or not) to see a data science master's degree on a job application for a data engineer?",False,0.5,https://www.reddit.com/r/dataengineering/comments/14lc985/would_a_data_science_masters_degree_be_useful_for/,dataengineering
How do you implement the medallion pattern in databricks unity catalog,14ku3ae,Doyale_royale,1687910930.0,,False,True,Discussion,17,False,16,"How are you guys implementing the medallion architecture in unity catalog. 

We currently have a dev and prod workspace with bronze, silver, gold catalogs in each. We also have external tables, and externally managed tables. 

I am the one architecting our databricks environment but have not found good documentation on the medallion architecture within unity catalog. Any advice is much appreciated!",False,0.94,https://www.reddit.com/r/dataengineering/comments/14ku3ae/how_do_you_implement_the_medallion_pattern_in/,dataengineering
What Have We Learned From Using Pandas?,14lb1yh,AmphibianInfamous574,1687962625.0,,False,False,Blog,0,False,0,,False,0.4,https://medium.com/gooddata-developers/what-have-we-learned-from-using-pandas-78b513cd58e0,dataengineering
Snowflake and NVIDIA partner to let businesses use their own data to build custom AIs,14khz2i,fhoffa,1687882530.0,,False,False,Blog,0,False,45,,False,0.83,https://siliconangle.com/2023/06/26/snowflake-nvidia-partner-let-businesses-use-data-build-custom-ais/,dataengineering
Agile transformations using the “The Spotify Model”,14l84mw,asc2450,1687955106.0,,False,False,Blog,0,False,0,,False,0.33,https://youtu.be/wtmW89I941I?list=PLEx5khR4g7PKbZ_dYIeEKwjGTwLbB5SOM,dataengineering
"historical data and ""point in time"" data modeling techniques, advice.",14l6e0c,thesnappingdog,1687949925.0,,False,True,Help,6,False,1,"any experiences with this topic?

things to consider, approaches to modeling the data once it's extracted etc.

Things that you've tried or seen working.

A generic description from Chatty G on what im referring to:

&#x200B;

>In a data engineering context, ""point in time"" refers to a specific moment or snapshot of data. It represents a specific timestamp or a specific version of a dataset. It is often used to indicate when a particular event or action occurred or to capture the state of data at a specific instance.  
>  
>In data engineering, managing data at different points in time is crucial for various purposes, such as tracking changes, auditing, analysis, and maintaining data integrity. By capturing data at different points in time, data engineers can analyze historical trends, perform comparisons, and ensure data consistency across different systems and processes.

&#x200B;

bonus points if you've worked on these topics using dbt, that's my main data transform engine.

full transparency: am looking into productizing this type of data modeling exercise, making it easier to achieve without making the queries super inefficient/costly. looking to understand the topic better.

&#x200B;

thanks in advance y'all 🙏

&#x200B;

https://preview.redd.it/fcayybrmpq8b1.jpg?width=500&format=pjpg&auto=webp&s=382e4984d12cd8568072c24e4e7292bbdc52be7a",False,1.0,https://www.reddit.com/r/dataengineering/comments/14l6e0c/historical_data_and_point_in_time_data_modeling/,dataengineering
Migrate Databricks Data across Tenant,14l2esp,Extra_Blacksmith_567,1687935987.0,,False,True,Help,1,False,2,"Hello All,
Can anyone help me in understanding whether migrating the delta lake tables as well as the storage account mounted to it will migrate all the data from the databricks or we need to also migrate the object storage which it creates for itself needs to be migrated as well ?
Any help is highly appreciated.",False,0.75,https://www.reddit.com/r/dataengineering/comments/14l2esp/migrate_databricks_data_across_tenant/,dataengineering
Jacobson's Rank,14l4o9f,qunatrimonix,1687944201.0,,False,False,Blog,0,False,1,,False,1.0,https://denvaar.github.io/posts/jacobsons_rank.html,dataengineering
Do you study outside of work?,14kdb23,gintokiredditbr,1687871225.0,,False,True,Career,36,False,31,"




Hi guys, how is your study routine when you are already employed? do you study on the weekend or after working hours? or just during work? and if it's during work, do you try to research and implement the new concepts in some project at work or do you really study by taking a course, etc? thanks!",False,0.93,https://www.reddit.com/r/dataengineering/comments/14kdb23/do_you_study_outside_of_work/,dataengineering
How does your company manage feature stores?,14kl55h,fithrowaway379,1687889724.0,,False,True,Discussion,4,False,8,"For those of you that work on or maintain a feature store, I'm curious how its managed.

&#x200B;

Assuming you have centralized infrastructure for the store, do you have one core team that is developing and contributing features? Or do you allow the domain experts to create the feature pipelines and contribute the data to the store?

&#x200B;

For a large feature store, I am thinking it makes the most sense to have a central team manage the infrastructure of the store and act as gatekeepers to determine what does and doesn't get into the feature store. Then, develop contracts with the business teams who can create the features and contribute them into the store following the contract.

&#x200B;

How does your company manage it?",False,0.91,https://www.reddit.com/r/dataengineering/comments/14kl55h/how_does_your_company_manage_feature_stores/,dataengineering
Databricks and Snowflake Summit Performers,14ktxs1,nerdistheword88,1687910539.0,,False,True,Discussion,2,False,3,"I'm attending the Databricks Data & AI Summit in SF this year and I was shocked to find out that Salt n Pepa are performing at the big Data Party at Chase Party.

Just curious, does anyone know if the Snowflake Summit in Vegas also have a similar performance? Looking to see who had the better accommodations/performers haha",False,0.71,https://www.reddit.com/r/dataengineering/comments/14ktxs1/databricks_and_snowflake_summit_performers/,dataengineering
Truncating Delta Tables - is that wise? (Ignorant Q),14kte7b,cdigioia,1687909158.0,,False,True,Help,7,False,3,"We're switching from on-prem to Synapse. 

Our current build stored procs, are getting switched over to Spark SQL notebooks.  Some of these build procs are truncate and inserts. 

Delta retains the history though -it's a soft delete, until vacuum is run anyway.  If say, a Delta file is getting truncate and insert every night:

  * Is that an OK practice?
  * What does that look like in the Delta file?  Say a 1,000 row table on Day 1, is then truncated and inserted, and no data changed. Is that now a 2,000 row table, with 1,000 'active' rows and 1,000 soft deleted ones?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14kte7b/truncating_delta_tables_is_that_wise_ignorant_q/,dataengineering
Learning full stack development as a DE,14k9h4v,wtfzambo,1687860367.0,,False,True,Discussion,18,False,25,"I'm a data engineer with about 4 years of experience, and I'd like to learn full stack development to be able to build my own shit.

I sometimes find stumble upon opportunities where I could make a few extra bucks but have to decline due to lack of expertise.

I am proficient in python, SQL and Typescript, I'm junior level with frontend development in React, know some DevOps and have almost zero knowledge about backend development and web servers.

What would you recommend?",False,0.94,https://www.reddit.com/r/dataengineering/comments/14k9h4v/learning_full_stack_development_as_a_de/,dataengineering
How crucial is data reliability?,14ky566,hatchikyu,1687922146.0,,False,True,Discussion,3,False,1,"Hi, dropping in to ask a quick question about your thoughts on how important data reliability is as a problem. 

I work with teams in the software reliability realm and have started hearing some mention ""data reliability"" as an issue. 

Is it an actual problem worth solving or is it an artefact of being in an echo chamber?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ky566/how_crucial_is_data_reliability/,dataengineering
When to validate data / apply contracts in a data lake pipeline?,14knfzj,EarthEmbarrassed4301,1687895247.0,,False,True,Discussion,3,False,3,"I've seen two scenarios of this topic.

1. Ephemeral Landing Zone -> append in Bronze as parquet -> **Validate new/modified records** \-> Merge in Silver
2. Ephemeral Landing Zone -> **Validate new/modified records** \-> append in bronze as parquet-> merge in silver

The argument for method #1 is that we just care about getting data into bronze as fast as possible and we don't care about the quality, structure, or validity of the data when appending to the bronze table. Only when we want to merge those new/changed records in silver, we should apply data contract validation.

I understand that reasoning, but I feel like method #2 is better, no? why not validate with the contracts BEFORE appending to bronze? This way I am only appending validated data into bronze, which can just be merged into silver with no worries.

What is your approach to data validation & quality checks in your pipeline?

As always, thanks! :)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14knfzj/when_to_validate_data_apply_contracts_in_a_data/,dataengineering
Which AWS Associate level certification is most suited for a career in data engineering ?,14ke6rb,lancelot_of_camelot,1687873474.0,,False,True,Discussion,12,False,9,"Hello Redditors,

I recently passed the AWS CCP certification, and I am studying Kafka and different other technologies, as well as building projects with those. I want to get a more advanced cloud certification in the next two months to prove my skills to a potential employer.

&#x200B;

 AWS Associate level has three options: Solutions Architect, Developer and SysOps, and I would like to know which one is the most suitable for starting a career in DE. I know that certs are not the most essential part of a good DE portfolio, but I believe they can help with getting my resume noticed and get some hands-on experience with AWS.

&#x200B;

Thank You !

&#x200B;",False,0.81,https://www.reddit.com/r/dataengineering/comments/14ke6rb/which_aws_associate_level_certification_is_most/,dataengineering
Interview resources,14kvt7x,StockSea5996,1687915617.0,,False,True,Interview,1,False,0,"Hello everyone, 

I recently started looking for new opportunities for data engineer (Mid-senior level) position. I am mostly doing leetcode sql and python. Learning about data modeling and stuff from youtube.  What other resources do you guys recommend?",False,0.5,https://www.reddit.com/r/dataengineering/comments/14kvt7x/interview_resources/,dataengineering
Where does computation occur with Spark?,14kplh3,JustAnotherMortalMan,1687900072.0,,1687900292.0,True,Discussion,5,False,2,"I'm second guessing my understanding here: if pyspark is used to read a table from a remote DB (with something like J/ODBC connections) and some transformation is done to the data, is the compute used to perform that transformation sourced from the database servers storing the data or from the executor nodes?  Does the answer change if it is a local spark session instead of a cluster, or if the transformation is provided purely as a sql query?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14kplh3/where_does_computation_occur_with_spark/,dataengineering
Recommendations for User-Friendly data visualization tool,14kogbl,chas_jk,1687897546.0,,False,True,Help,9,False,2,"Hi all, I've been facing a challenge at my job recently and could use a product recommendation. I'm the only data engineer at a small startup, and I spend a lot of my time preparing and formatting reports for our executives. Our execs aren't technical, so they can only view data in the form of a filterable spreadsheet. We use Airtable for this, and it works decently, but has serious issues. Firstly, it has a max table size of 100k records, so some requests that exceed that are impossible for us to send in one report. It also doesn't have live querying capabilities for quick iteration if the original spec wasn't 100%.

I'd love to hear recommendations for data visualization tools that are flexible to use, can connect to and query from standard data warehouses (we use BigQuery mostly), and most importantly can produce reports that are filterable by the end user. Most of what we want to display is aggregate, filterable stats and rollups on user activity, where we have \~20 million activity data points across \~500k users.   


So far we've looked into Mode and Tableau, but with mode being acquired (yesterday) we're skeptical, and Tableau at least at first doesn't look very user-friendly to non-technical users. Open to considering both of these though.",False,0.75,https://www.reddit.com/r/dataengineering/comments/14kogbl/recommendations_for_userfriendly_data/,dataengineering
Hashing Phone Numbers For 2-Factor Authentication,14ksgey,small_hole_energy,1687906834.0,,False,False,Blog,0,False,1,,False,0.66,https://theabbie.github.io/blog/2FA-phone-number-hashing,dataengineering
[Help] Marc Lamberti's Apache Airflow: The Hands-On Guide vs. The Complete Hands-On Introduction to Apache Airflow,14kes2t,coldstare_warmheart,1687874945.0,,False,True,Help,1,False,5,"I am planning to enroll to one of the Udemy courses offered by Marc Lamberti. I am not sure which one I should choose as I am still starting my journey in data engineering. For context, I am a backend engineer with <1 year of experience and have worked extensively with Docker, Python and SQL for several school projects of mine during my CS undergrad. I also already have completed this [2 hour course by coder2j on YouTube](https://www.youtube.com/watch?v=K9AnJ9_ZAXE&list=PLwFJcsJ61oujFW8pTo9S8_b6wujg5NgGW&ab_channel=coder2j).   
I already tried searching for a comparison of the two courses I mentioned but got no luck finding one. I would like to know which course will benefit me the most given my current skills.   
Thank you in advance!",False,0.86,https://www.reddit.com/r/dataengineering/comments/14kes2t/help_marc_lambertis_apache_airflow_the_handson/,dataengineering
SSIS in GitHub,14krvjr,Fraiz24,1687905404.0,,False,True,Career,5,False,1,"I am doing an SSIS project and am curious if it’s worth putting in my GitHub.  I would like to, to see how my progress grows, but would this be beneficial for future job prospects? Are companies likely to check out your GitHub for these type of projects? I have a summer goal to build my skills so I  can have something to show for when I get offered an interviewed, just wondering if this is something they look into.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14krvjr/ssis_in_github/,dataengineering
Suggestions for research data?,14klgt0,nobblessobligee,1687890459.0,,False,True,Help,0,False,2,"I am working in a small research team. I am thinking about how to streamline and modernize our data  processes. 

Right now, we have various people collecting research data on various paper forms. These are all then scanned and entered into a database by an admin. This isn't the worst part - the 'database' was designed by an in house IT team a few years ago. It's terrible, it's not even a research database but rather some crude business analytics tool called Diver. (I asked IT last week if I could analyse and compare two very obvious groups in the database, and they said no, it wasn't built that way (!) but I could export two different Excel spreadsheets if I wanted and merge them manually. I nearly cried.)

I am meeting someone from IT tomorrow to discuss options. Basically I have an opportunity to drive us to streamline and modernize the whole setup. 

I want something that will allow front-end entry of data into specially designed forms (perhaps on tablets carried by frontline staff). This will then automatically update one central repository of data. On the other end I want us to be able to slice, dice, analyse and extract the data in an easy fashion. I also want the ability to dynamically build dashboards that update with team progress automatically. (As it stands one admin spends a week with Diver to generate a quarterly report on team progress). 

I am just trying to get a feel for my various options. 

Option 1: The university I'm in uses Redcap. This is the default for academic research, and it's pretty good. I'll investigate but I am unsure if it has all of the functionality I'm looking for. 

Option 2: The university I'm in also subscribes to O365, which means they have a good working relationship with their local Microsoft Partner. I'm wondering if it would be worth asking this vendor to design a database solution from scratch for us, e.g. using Microsoft Forms for direct data entry into a central data repository, which we can then layer analytics tools on top of. For example I presume we could have PowerBI dashboards which update automatically with our progress, if we have good data going in? There would also presumably be the opportunity to automate other processes we haven't even thought about. 

Thanks for answers in advance. The data we're generating is golden but it's all siloed, and very difficult to extract/analyse, so in essence it's useless in its current form. I need to try and build a solid data pipeline that will pay dividends for the next 10 years+, and I'd love any advice/comments/suggestions. 

&#x200B;

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14klgt0/suggestions_for_research_data/,dataengineering
"Big Tech Digest #2: Building Airbnb’s Next Generation Data Management Platform, Building Etsy’s Search by Image Feature, GPT-4 + Streaming Data = Real-Time Generative AI, and more!",14ki254,av818,1687882728.0,,False,False,Blog,0,False,2,,False,0.75,https://bigtechdigest.substack.com/p/big-tech-digest-2,dataengineering
Using behavioral customer data,14kgls3,Euphoric-Let-8960,1687879285.0,,False,True,Discussion,2,False,2,"Doing some research about the challenges of creating and using behavioral customer data. Wanted to ask the group what are some of the challenges you're finding collecting and using behavioral customer data? What are your preferred tools, sources, targets for this type of data?

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14kgls3/using_behavioral_customer_data/,dataengineering
How do I highlight my skills outside my work?,14kl0ut,what_duck,1687889463.0,,False,True,Help,1,False,1,"I'm taking a data engineering course and have been leveling up on DE skills and knowledge. My goal is to move into a dedicated data engineering role. My current job has some DE components, but I'm unsure how to highlight my additional skills? Would you recommend adding a section in my resume or mentioning these skills in the cover letter?",False,1.0,https://www.reddit.com/r/dataengineering/comments/14kl0ut/how_do_i_highlight_my_skills_outside_my_work/,dataengineering
Parquet File Format: Everything You Need to Know,14k1a5f,Junior-Salary-6859,1687834082.0,,False,False,Blog,1,False,16,,False,0.91,https://towardsdatascience.com/parquet-file-format-everything-you-need-to-know-4eed5c0019e7,dataengineering
"Side Project - productionalizing life stuff, would people find it interesting?",14kjtfi,BoysenberryLanky6112,1687886728.0,,False,True,Discussion,1,False,0,"So I currently have a good number of excels where I track things like my spending, nutrition, workouts, etc. I have some jupyter notebooks to view some hacked together views. I've now worked as a data engineer for about a year and I had the idea to basically do what I do with my job with my personal stuff. I'd want to move it all to the cloud (I use gcp for my job so would likely use that), automate any pulls from various apis (financial apis, Fitbit api, etc.), Build a data entry webapp for things that can't be pulled via API to completely eliminate excels, and build out a webapp I'd use for tracking. I'd also want to leverage GitHub and GitHub actions for deployments, set up a testing suite, and basically treat it like it's a production application that was being developed in a professional setting.


So the question is, would this be something people would be interested in reading about? Has anyone else done something similar and written about it? What would be a good medium for posting my work?

Thanks!",False,0.5,https://www.reddit.com/r/dataengineering/comments/14kjtfi/side_project_productionalizing_life_stuff_would/,dataengineering
Analyze PostgreSQL Data using DuckDB and MotherDuck,14kijde,jekapats,1687883835.0,,False,False,Blog,0,False,1,,False,0.6,https://youtu.be/5BRBbm60j-Y,dataengineering
Programming Internship summer,14kigtb,Human-Blackberry3325,1687883672.0,,False,True,Help,2,False,1,"I need help. I'm currently in 2 programming internships (data science and data engineering) and I'm in the final round for 2 more(both data science).  All of these are summer internships. The question I have is if I do get all of these internships, I would basically come out with 4 programming internships this summer. Would this impress companies or would they think I just half-assed all of them? ",False,1.0,https://www.reddit.com/r/dataengineering/comments/14kigtb/programming_internship_summer/,dataengineering
Is it me or are there a bunch products/tools out there that do the exact same thing?,14ju1vu,jbrune,1687815303.0,,False,True,Discussion,17,False,24,"I'm fairly new to the DE world and I'm trying to learn all the names and the players.  It's occurred to me that a lot of the DE world is like Facebook/MySpace/tiktok/YouTube/etc.  You can do the exact same thing on multiple platforms.  What makes people chose one over the other?  I get price and usability factor in, but are there folks that say, ""I like working with AWS (or MS or Apache, etc) stuff, so I'm just going to stick with what they have""?",False,0.94,https://www.reddit.com/r/dataengineering/comments/14ju1vu/is_it_me_or_are_there_a_bunch_productstools_out/,dataengineering
Need Advice on Optimising a long running job,14ke0ao,Worth-Lie-3432,1687873019.0,,False,True,Help,1,False,1,"I’m trying to optimise a job which is currently running for 2 hours. It is using rdd’s and converting that into DF’s make it even more worse. Here is what I did:

I have a very big fact table where a column is an array of structs. I can only join with the dimension table when that field is exploded. 

Once I join with dimension table, I need to aggregate based on certain dimension field obtained from dim table and collect all the structs associated with that that field and convert back into array of structs. 

Also, I cannot braodcast the dim table because it’s above 10 GB and the limit is 8 GB. So, instead I built a hash Map and did sc.broadcast to avoid shuffle.

Currently I’m using explode, hashmap lookups and collect_list functions, but however all the executors are getting killed.

I feel explode and collect_list are also expensive operations. 

So, I’d greatly appreciate if you could suggest me any ways I can optimise it or suggest any alternative and make it run less than an hour.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14ke0ao/need_advice_on_optimising_a_long_running_job/,dataengineering
How can a pipeline ELT be tested?,14ju4bn,CauliflowerJolly4599,1687815454.0,,False,True,Discussion,19,False,18,"I mainly create pipelines and always try to tune and group activities in ADF like a template and refactor the blocks.

Mostly, when I create new pipelines that transfer data from source to DWH, I always check some quality data control (if on source I've 50 applications, on DWH I must have 50 applications).

My question is, how can I automate this kind of  tests? And in which part of Azure? Which tool or component?",False,0.95,https://www.reddit.com/r/dataengineering/comments/14ju4bn/how_can_a_pipeline_elt_be_tested/,dataengineering
Let's install the databricks vs code extension,14k0199,-Maestro-,1687830515.0,,False,False,Meme,14,False,8,,False,0.58,https://i.redd.it/yzmop2clug8b1.jpg,dataengineering
"MDM Tools - CluedIn, etc.",14k8khz,sc_santy,1687857363.0,,False,True,Help,2,False,2,"Information: -
Data pipeline on Azure, gets data as csv files from different souces (SAP, SAGE, KingDee, Dynamics) in ADLS storage. Data Factory does the initial clean up, type casting, schema mapping, etc. and pushes to DB. DB takes care of all harmonisation, currency conversion, unit conversion, etc.

I was interested in the Azure Purview tool to help Data Governance. Unfortunately it has every aspect of Data Governance covered except Data Quality or MDM tools. For MDM Azure heavily certifies CluedIn, however on reseaeching CluedIn come up to be 135k Euros/year + taxes + Azure infrastructure costs for more than 200k data points. Which is exorbitant because our expense on the whole data platform is 10k - 14k a year.

What are you advises on MDM tools, our focus is mostly to fix Data Quality issues.",False,1.0,https://www.reddit.com/r/dataengineering/comments/14k8khz/mdm_tools_cluedin_etc/,dataengineering
"Building a Modern Data Pipeline: A Deep Dive into Terraform, AWS Lambda and S3, Snowflake, DBT, Mage AI, and Dash",14jm3gi,Jealous_Ad6059,1687797237.0,,False,True,Blog,4,False,39," 

https://preview.redd.it/1zgo3vnl3e8b1.png?width=2000&format=png&auto=webp&s=cb912643a1faef8dbf412eb58e6caddc9fb95376

This blog post will provide a detailed walkthrough of creating a modern data pipeline using a combination of Terraform, AWS Lambda and S3, Snowflake, DBT, Mage AI, and Dash.

[https://medium.com/@stefentaime\_10958/building-a-modern-data-pipeline-a-deep-dive-into-terraform-aws-lambda-and-s3-snowflake-dbt-cac6816f2100](https://medium.com/@stefentaime_10958/building-a-modern-data-pipeline-a-deep-dive-into-terraform-aws-lambda-and-s3-snowflake-dbt-cac6816f2100)",False,0.9,https://www.reddit.com/r/dataengineering/comments/14jm3gi/building_a_modern_data_pipeline_a_deep_dive_into/,dataengineering
Seeking Feedback on 'Data Engineering 101' eBook!,14joypx,Anishekkamal,1687803770.0,,1687825224.0,True,Career,189,False,26,"Hi All,

I have mentored more than 200+ students and working professionals in the past 2 years. I've just released my latest ebook, **""Data Engineering 101: A Comprehensive Guide for Beginners and Career Transitioners.""**

Whether you're a beginner or transitioning careers, this guide covers all the essentials of data engineering. I'd love to hear your feedback and suggestions to make it even better. Please direct message me to receive a copy.

Description Of the ebook:

""Data Engineering 101"" is the ultimate resource for anyone interested in exploring the world of data engineering. Authored after having 200+ mentoring sessions and by a seasoned data engineering expert, this guide offers a structured and practical approach to mastering the essentials of data engineering.

Whether you are a beginner aiming to start a career in data engineering or a professional looking to transition into this field, this guide has been meticulously crafted to cater to your needs. It covers everything from the core concepts and responsibilities of a data engineer to the key distinctions between data engineering and other data roles. Additionally, it provides valuable insights into the crucial role of data engineering in today's data-driven organizations.

One of the standout features of this guide is its comprehensive framework, which breaks down data engineering into six pillars. Each pillar is explored in detail, providing you with a solid foundation and a clear understanding of the subject matter. To further enhance your learning journey, the guide includes a curated list of recommended resources for expanding your knowledge and skill set.

&#x200B;

Thank you in advance for your support and participation!

&#x200B;",False,0.63,https://www.reddit.com/r/dataengineering/comments/14joypx/seeking_feedback_on_data_engineering_101_ebook/,dataengineering
People of Data Engineering,14jg3xi,sspaeti,1687782815.0,,1687805268.0,True,Discussion,19,False,56,"A curated list^(1) of the people in Data Engineering:

* **Dipankar Mazumdar**: Dremio - Apache Iceberg
* **Maxime Beauchemin**: Father of Data Engineering
* **Mehdi Ouazza**: Awesome written content, also now on YouTube. Creator of Data Creators Club.
* **Benjamin Rogojan**: Seattle Data Guy
* **Ananth Packkildurai**: Functional Data Engineering, Creator of dataengineeringweekly.com.
* **Zach Wilson**: Data Engineering Challenges at Hyperscale
* **Marc Lamberti**: Airflow
* **Wes McKinney**: Pandas / Arrow
* Andy Grove: Apache Arrow PMC. Creator of DataFusion & Ballista (Arrow) query engines)
* ThePrimeagen: Rust, Netflix, programming, Neovim
* **Nick Schrock**: Dagster, data orchestration
* **Denny Lee**: Delta Lake, Rust, OSS
* **Simon Whiteley**: Databricks, Data Engineering), popular YouTube
* **Matt Turck**: Creator of MAD landscape
* **Jacek Laskowski**: **ApacheSpark** DeltaLake Databricks ApacheKafka KafkaStreams ksqlDB
* **Matt Housley**: Creator of Fundamentals of data engineering
* **Joe Reis**: Creator of Fundamentals of data engineering
* **Erik Bernhardsson**: Building a simple version of Kubernetes Modal
* **Matei Zaharia**: Chief Technologist at Databricks
* **Adi Polak**: Author of Scaling Machine Learning with Spark
* **Andy Petrella**: Writing on Data Observability
* **Peter Marshall**: Druid Advocate
* Alexey Grigorev: Manages DataTalksClub which has a blog, zoom camps and GitHub tutorials
* **Joseph Machado**: Lots of great how-tos and projects on Start Data Engineering
* Chris Riccomini: Essays on tech, data, and streaming
* Christophe Blefari: A combination of aggregate newsletters and one-off articles on data engineering
* **Itai Yaffe**: Druid use cases: Streaming with delta lake and Druid
* **Wayne Eckerson**: Author, keynote speaker, and consultant Eckerson Group
* **Andreas Kretz**: Creator of The Data Engineering Cookbook
* **Tobias Macey**: Data engineering podcast
* **Darshil Parmar**: Popular youtube
* **Michael Kahan**: Popular YouTube and Content on DE
* **Matt Weingarten**: Data Engineer at Disney Streaming Services. Previously at Facebook and Nielsen.
* **Robert Sahlin**: Data Platform with Google Cloud
* **Jérémy Ravenel**: Naas, Jupyter Notebooks into powerful automation, analytical, and AI
* Chad Sanderson: Data products, contracts, and captivating articles
* Sarah Krasnik: Great for infra and solutions insights
* Daniel Beach: Broad range of data engineering topics
* Benn Stancil: Prolific writer on his blogs, and they usually start with it Friday, let's fight...
* Barr Moses: Great articles on Data Observability.
* **Thalia Barrera**: Excellent post on date engineering
* **Stephen Bailey**: Exploring the world of data and its adjacencies at Data People Etc.
* **Shane Gibson**: Data modeling, in data for 30 years. Not technical, but about agile and data modeling.
* **Petr Janda**: Awesome blogs on petr@substack now working on Synq
* **Jonathan Neo**: Creator of Data Engineering Bootcamp
* Sandy Ryza: Dagster and passionate about Partitioning and Backfill.
* **Xinran Waibel**: Personalization Data Engineering at Netflix
* Simon Späti: Lots of open-source data engineering

Who is missing?  


^(1)[^(https://www.ssp.sh/brain/people-of-data-engineering/)](https://www.ssp.sh/brain/people-of-data-engineering/)^(.)",False,0.8,https://www.reddit.com/r/dataengineering/comments/14jg3xi/people_of_data_engineering/,dataengineering
A platform for building Gen-AI applications on Spark,14k0amt,several27,1687831252.0,,False,True,Open Source,0,False,4,"Hey, I'm Maciej, one of the co-founders of Prophecy ([https://prophecy.io](https://prophecy.io/)) - low code for Data Transformations and Generative-AI. [Blog here.](https://www.prophecy.io/blog/announcing-prophecy-3-0-low-code-sql-transformations)

We've just released our product's latest version, making it easier for teams to build data pipelines for LLM-based applications. As part of the release, we're also **open-sourcing the toolbox for Spark** ([https://github.com/prophecy-io/spark-ai](https://github.com/prophecy-io/spark-ai)) that contains useful connectors, transformations, and agents for building Gen-AI applications.

Hopefully, this allows some of you to build your apps easier and make them more robust and scalable from day one. Quick getting started example available here: [https://github.com/prophecy-samples/gen-ai-chatbot-template](https://github.com/prophecy-samples/gen-ai-chatbot-template) (all code is open-source too).

While our product is already mature, the toolbox shared above is still pretty early but will be completely OSS.

Any thoughts on the concept and ideas for what's essential to the community would be super appreciated!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14k0amt/a_platform_for_building_genai_applications_on/,dataengineering
Data Engineering VS Data Science,14kk7v9,chrisgarzon19,1687887648.0,,False,True,Blog,2,False,0,[https://dataengineeracademy.com/blog/data-science-vs-data-engineering/](https://dataengineeracademy.com/blog/data-science-vs-data-engineering/),False,0.18,https://www.reddit.com/r/dataengineering/comments/14kk7v9/data_engineering_vs_data_science/,dataengineering
"IS IT NECESSARY TO LEARN SSIS, SSRS AND SSAS",14jpoj6,Able_Truth6207,1687805412.0,,False,True,Career,42,False,9,"So I am on data engineering learning track, and i have learnt SQL, and Python, but I wanted to ask if its important or needed to learn SSIS, or should i just go straight to pyspark or cloud data warehouses. I already did a course on udemy that took me through the fundamentals of Datawarehousing though.  


N.B I must say, i have learnt a lot from the discourse on this channel. Thank you all so much",False,0.68,https://www.reddit.com/r/dataengineering/comments/14jpoj6/is_it_necessary_to_learn_ssis_ssrs_and_ssas/,dataengineering
What do you use to piece together sessions when analyzing user activity?,14k53xw,JanethL,1687845691.0,,False,True,Blog,0,False,1,"Do you use Scripts or Spreadsheets?

Then how do you make sense of the details within each session?

[I wrote a blog about how we do this at Teradata](https://medium.com/p/d1022847fce9) with sessionize, and nPath functions, that efficiently take care of the pre-processing, allowing us to concentrate on analysis and delivering valuable insights to the business.

[https://medium.com/p/d1022847fce9](https://medium.com/p/d1022847fce9)

Let me know what you think about this approach. And if you found my first blog helpful. :)",False,0.6,https://www.reddit.com/r/dataengineering/comments/14k53xw/what_do_you_use_to_piece_together_sessions_when/,dataengineering
how can i pitch the case for a data warehouse to leadership?,14jsvux,djaycat,1687812681.0,,1687813497.0,True,Help,10,False,5,"Greetings all. i am a data analyst, not engineer so i am not super knowledgeable on data pipelines, but i know a little. i work at a small startup and we are looking to get our data to the next level. We are currently pumping all of our production data into an s3 bucket and using amazon athena to query it. We use looker studio to visualize our data, but have not been able to find a good way to create scheduled sql jobs in athena so our dashboards can run on those specific tables.

i want to get us to a place where all of our business logic for analytics is stored in a specific github repo that the analytics team has access to, can collaborate on & edit, and can run the our jobs. i have been advocating that we switch to a data warehouse and start using something like airflow or dbt to manage our data models (since this is what i have done in the past at other orgs) but some folks higher up maintain that athena is superior to redshift and we shouldnt switch to a warehouse. my question: what is the case for a data warehouse and how can i pitch it effectively? also, if we do switch to redshift or something, is it necessary to also move to a dbt or airflow to create scheduled queries for our data models? apologies for any vagueness, can clarify anything unclear.

&#x200B;

edit: we do not have any data engineers currently, which i have also been advocating for",False,1.0,https://www.reddit.com/r/dataengineering/comments/14jsvux/how_can_i_pitch_the_case_for_a_data_warehouse_to/,dataengineering
Need advice on what tools to use on AWS / best way to do the following task,14k3dnm,anasp1,1687840253.0,,False,True,Help,5,False,1,"CONTEXT: I've currently got a lambda running with a cron scheduler to execute every 10 hours which basically grabs a csv file from the web and updates any records / replaces an existing csv file in a S3 bucket with the newer more recent data.

MY GOAL: Now I need to get this csv file in an actual tabular data format (i'm thinking a postgres database?). For starting I don't have too many rows of actual data, around 700 - 800. But I'm planning on adding more pipelines that will automatically collect more data and add to this database (so think few thousands to even 100k+ rows over the coming few months / year). 

**MY MAIN QUESTION: My question is, what do I use to perform the simple data engineering required to actually take this raw csv file in my S3 bucket, perform any data cleaning such as replacing nan values, lowercasing, removing any random characters / dropping columns that i may not need, and then uploading all this new cleaned data into a actual postgres database? What AWS tools are required and how do I plan an actual workflow for this type of things?**

What is the industry standard way of doing it? (fyi, I don't want to be spending too much, of course I know I'll have to spend $$ for this, but ideally what is a cost efficient way to do this)",False,1.0,https://www.reddit.com/r/dataengineering/comments/14k3dnm/need_advice_on_what_tools_to_use_on_aws_best_way/,dataengineering
GA4 clickstream data as a data stream,14k1vv8,gautiexe,1687835807.0,,False,True,Discussion,0,False,1,How can we get a realtime data stream of the GA4 clickstream?,False,1.0,https://www.reddit.com/r/dataengineering/comments/14k1vv8/ga4_clickstream_data_as_a_data_stream/,dataengineering
Replacing RDBMS with EMR (S3 & Presto),14jwtj1,Contango_4eva,1687821938.0,,False,True,Discussion,4,False,2,Anyone have experience and thoughts of using EMR to replace their DB?  Saw an article about Netflix switching to this.,False,1.0,https://www.reddit.com/r/dataengineering/comments/14jwtj1/replacing_rdbms_with_emr_s3_presto/,dataengineering
Alternatives to Jupyter notebook,14jftr6,Busy_Elderberry8650,1687782036.0,,1687794096.0,True,Discussion,19,False,15,"Suppose you work on a database and you run queries through a client without python. How would you organize a presentation? I mean you can use of course excel but i would like something that pretty easily shows a query and its relative results and that is also easy to send across your company.

Are there alternatives not depending on python?",False,0.89,https://www.reddit.com/r/dataengineering/comments/14jftr6/alternatives_to_jupyter_notebook/,dataengineering
Using rust for DE activities?,14jhco2,Ok_Raspberry5383,1687786093.0,,False,True,Discussion,22,False,12,"Is anyone using rust for any DE activities, solutions etc? My old manager was really keen on learning it and had the opinion it would be the next big thing in the data/big data space. 

I personally don't see it for every day DE - I a lot of value in things like Photon (in cpp but could easily be rust) that Databricks have built to take advantage of SIMD in Apache Spark but this is all under the hood - not something you'd interact with in even critical low latency streaming pipelines necessarily?

Wondered if anyone is using it? If so:
- what are you using it for?
- what were you using previously and why did you switch?
- how is hiring for this skillet in DE (UK perspective would be great)?
- would you recommend?
- any final comments?

Thanks!",False,1.0,https://www.reddit.com/r/dataengineering/comments/14jhco2/using_rust_for_de_activities/,dataengineering
"When using hive with spark to manage our data, what exactly does hive contribute?",14jcte4,Important_Row_3772,1687772811.0,,False,True,Discussion,17,False,22,Spark is the computation software. Is hive  just a metastore?,False,0.96,https://www.reddit.com/r/dataengineering/comments/14jcte4/when_using_hive_with_spark_to_manage_our_data/,dataengineering
Introducing `mask-json-field` Single Message Transform for Kafka Connect,14jnlp2,Affectionate-Fuel521,1687800613.0,,False,True,Open Source,0,False,4,"Hi! All,

I wrote a Single Message Transform for Kafka Connect. It operates on messages that are JSON. It's purpose is to remove fields that have sensitive data, like PII, Financial etc.

Here is the blog post introducing it:

[mask-json-field SMT for Kafka Connect](https://ferozedaud.blogspot.com/2023/06/mask-json-field-transform-kafka-connect.html)

And here is the source code:

[GitHub: ferozed/mask-json-field-transform](https://github.com/ferozed/mask-json-field-transform)

&#x200B;",False,1.0,https://www.reddit.com/r/dataengineering/comments/14jnlp2/introducing_maskjsonfield_single_message/,dataengineering
How I Got Into Data Engineering | Finding Success in Data Engineering Without the Textbook Route,14jqy4t,Luxi36,1687808290.0,,False,False,Blog,1,False,0,,False,0.5,https://medium.com/data-engineer-things/how-i-got-into-data-engineering-740392fbd6f5,dataengineering
